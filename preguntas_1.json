{
    "0": {
        "q": "Question 1\n\nWhich of the following are best practice recommendations that should be considered when loading data into Snowflake",
        "opt": {
            "0": "Remove all dates and timestamps.",
            "1": "Avoid embedded characters, such as commas for numeric data types.",
            "2": "Load files that are approximately 25MB or smaller.",
            "3": "Load files approximately 100-250 MB (or larger) in size compressed",
            "4": "Remove semi-structured data types."
        },
        "ans": [
            1,
            3
        ],
        "exp": "\n\nPreparing delimited text files\n\nConsider the following guidelines when preparing your delimited text (CSV) files for loading:\n\nUTF-8 is the default character set, however, additional encodings are supported. Use the ENCODING file format option to specify the character set for the data files.\n\nFields that contain delimiter characters should be enclosed in quotes (single or double). If the data contains single or double quotes, then those quotes must be escaped.\n\nFields that contain carriage returns should also be enclosed in quotes (single or double). Carriage returns are commonly introduced on Windows systems in conjunction with a line feed character to mark the end of a line (\\r \\n).\n\nThe number of columns in each row should be consistent.\n\n\n\nNumeric data guidelines\n\nAvoid embedded characters, such as commas (e.g. 123,456).\n\nIf a number includes a fractional component, it should be separated from the whole number portion by a decimal point (e.g. 123456.789).\n"
    },
    "1": {
        "q": "Question 2\n\nWhat are the steps to load data from an archival cloud storage class (for example, Amazon S3 Glacier or Microsoft Azure Archive Storage) into Snowflake",
        "opt": {
            "0": "Same steps as loading data from a non-archival cloud storage class.",
            "1": "It\u2019s not possible to do it directly on Snowflake.",
            "2": "You should create an ARCHIVAL stage, then you can perform the same steps as loading data from a non-archival cloud storage class."
        },
        "ans": [
            1
        ],
        "exp": "\n\nSupported file locations\n\nSnowflake refers to the location of data files in cloud storage as a stage. The COPY INTO <table> command used for both bulk and continuous data loads (i.e. Snowpipe) supports cloud storage accounts managed by your business entity (i.e. external stages) as well as cloud storage contained in your Snowflake account (i.e. internal stages).\n\n\n\nExternal stages\n\nLoading data from any of the following cloud storage services is supported regardless of the cloud platform that hosts your Snowflake account:\n\nAmazon S3\n\nGoogle Cloud Storage\n\nMicrosoft Azure\n\n\nYou cannot access data held in archival cloud storage classes that requires restoration before it can be retrieved. These archival storage classes include 3 Glacier Flexible Retrieval or Glacier Deep Archive storage class, or Microsoft Azure Archive Storage.\n"
    },
    "2": {
        "q": "Question 3\n\nWhat are supported file formats for unloading data from Snowflake",
        "opt": {
            "0": "TXT",
            "1": "Parquet",
            "2": "Delimited (CSV, TSV)",
            "3": "ORC",
            "4": "Avro",
            "5": "XML",
            "6": "JSON"
        },
        "ans": [
            1,
            2,
            6
        ],
        "exp": "\n\nWhen Unloading data from a table (or query) into one or more files, you can choose one of the following locations:\n\nNamed internal stage (or table/user stage). The files can then be downloaded with GET.\n\nNamed external stage that references an external location (S3, Google, Azure).\n\nExternal location (Amazon S3, Google Cloud Storage, or Microsoft Azure).\n\n\n\nThe file formats supported includes:\n\nCSV (comma-separated, text based)\n\nJSON (semi-structured, text based)\n\nParquet (semi-structured / binary-based)\n"
    },
    "3": {
        "q": "Question 4\n\nIn terms of Warehouses, what is recommended when loading and querying large data sets",
        "opt": {
            "0": "Use larger warehouses for querying",
            "1": "Use the same warehouses for loading and querying large data sets",
            "2": "Use different warehouses for loading and querying"
        },
        "ans": [
            2
        ],
        "exp": "\n\nPlanning a data load\n\nDedicating separate warehouses to load and query operations\n\nLoading large data sets can affect query performance. We recommend dedicating separate warehouses for loading and querying operations to optimize performance for each.\n\nThe number of data files that can be processed in parallel is determined by the amount of compute resources in a warehouse.\n"
    },
    "4": {
        "q": "Question 5\n\nWhat is the main difference between the LOAD_HISTORY and COPY_HISTORY views",
        "opt": {
            "0": "They both do the same function.",
            "1": "LOAD_HISTORY returns the history of data loaded using Snowpipe, and COPY_HISTORY the data loaded using the COPY_INTO command.",
            "2": "LOAD_HISTORY returns the history of data loaded using the COPY_INTO command, and COPY_HISTORY the data loaded using both Snowpipe and the COPY_INTO command.",
            "3": "LOAD_HISTORY returns the history of data loaded using the COPY_INTO command, and COPY_HISTORY the data loaded using Snowpipe."
        },
        "ans": [
            2
        ],
        "exp": "\n\nTroubleshooting bulk data loads\n\nTo troubleshoot bulk data failures, follow this two steps:\n\nView the COPY HISTORY for the table.\n\nValidate the data load.\n\n\n\nViewing the COPY history for a table:\n\n\n\nThe COPY_HISTORY view can be used to query Snowflake data loading history along various dimensions within the last 14 days. The function returns load activity for both Snowpipe and COPY INTO <table>. The table function avoids the 10,000 row limitation of the LOAD_HISTORY View. The results can be filtered using SQL predicates. You can also view data loading details in Snowsight.\n\nThe LOAD_HISTORY view enables you to retrieve the history of data loaded into tables using the COPY INTO <table> cmd within the last 14 days, displaying one row for each file loaded. However, this view does not return the history of data loaded using Snowpipe. For this historical information, query the COPY_HISTORY table function instead.\n\n\n\nResources"
    },
    "5": {
        "q": "Question 6\n\nWhat two options can we specify in the FILE_FORMAT option of the COPY INTO command",
        "opt": {
            "0": "TYPE",
            "1": "FORMAT_TYPE",
            "2": "FORMAT_VALIDATION",
            "3": "FORMAT_NAME"
        },
        "ans": [
            0,
            3
        ],
        "exp": "\n\nCOPY INTO Command Syntax\n\n/* Standard data load */\nCOPY INTO [<namespace>.]<table_name>\n     FROM { internalStage | externalStage | externalLocation }\n \n/* Data load with transformation */\nCOPY INTO [<namespace>.]<table_name> [ ( <col_name> [ , <col_name> ... ] ) ]\n     FROM ( \nselect [<alias>.]$<file_col_num>[.<element>] \n\t [ , [<alias>.]$<file_col_num>[.<element>] ... ]\nFROM { internalStage | externalStage } \n\t\t)\n \n[ FILES = ( '<file_name>' [ , '<file_name>' ] [ , ... ] ) ]\n[ PATTERN = '<regex_pattern>' ]\n[ FILE_FORMAT = ( { \n\tFORMAT_NAME = '[<namespace>.]<file_format_name>' |\n\tTYPE = { CSV | JSON | AVRO | ORC | PARQUET | XML } [formatTypeOptions] })]\n[ copyOptions ]\n[ VALIDATION_MODE = RETURN_<n>_ROWS | RETURN_ERRORS | RETURN_ALL_ERRORS ]\nNote that the FORMAT_NAME (existing named file format) and TYPE (CSV, JSON, Avro, etc.) options are mutually exclusive.\n"
    },
    "6": {
        "q": "Question 7\n\nWho can provide the compute resource used for data loading on Snowflake",
        "opt": {
            "0": "Snowflake-managed serverless compute",
            "1": "Hardware provisioned by user directly from cloud providers",
            "2": "User-managed virtual warehouse"
        },
        "ans": [
            0,
            2
        ],
        "exp": "\n\nBulk Loading vs Snowpipe\n\nhttps://www.snowflake.com/blog/best-practices-for-data-ingestion/\n\n\n\nBulk loading is the process of loading large amounts of data into Snowflake all at once. Bulk loading is a good option for loading large amounts of data that do not need to be available immediately. Continuous loading is the process of loading data into Snowflake in small batches over time. This can be done using Snowpipe.\n\n\nCOPY provides file-level transaction granularity as partial data from a file will not be loaded by default ON_ERROR semantics. Snowpipe does not give such an assurance as Snowpipe may commit a file in micro-batch chunks for improved latency and availability of data. When you are loading data continuously, a file is just a chunking factor and is not seen as a transaction boundary determinant.\n\n\n\nSnowpipe is designed for continuous ingestion and is built on COPY, though there are some differences in detailed semantics listed in the documentation. In distinction to COPY, Snowpipe runs a serverless service, meaning that there are no virtual warehouses to manage, with Snowflake-managed resources instead that automatically scale to changing workloads. This frees you from the burden of managing your warehouse, scaling it for a variable load, and optimizing it for the best cost-performance balance. It also shifts the burden of monitoring file loading from the customer to Snowflake.\n\n\n\nCOPY command\n\nCustomer managed warehouse\n\nFile level Granularity. Loads are always performed in a single transaction.\n\nLatency is deterministic\n\nSNOWPIPE\n\nSnow managed / serverless compute\n\nMicro-batch chunks Granularity. Loads are combined or split into a single or multiple transactions\n\nLatency is under 1 minute\n\nResources"
    },
    "7": {
        "q": "Question 8\n\nWhich is the slowest option when using COPY INTO command for data loading",
        "opt": {
            "0": "Specifying a list of specific files to load",
            "1": "Loading by path (internal stages) /prefix",
            "2": "Using the Snowpipe REST API",
            "3": "Using pattern matching"
        },
        "ans": [
            3
        ],
        "exp": "\n\nLoading data\n\nBulk Load Options\n\nBy path (internal stages) / prefix (Amazon S3 bucket). See Organizing data by path.\n\nSpecifying a list of specific files to load.\n\nUsing pattern matching to identify specific files. See Pattern Matching in COPY INTO.\n\nOf the three options for identifying/specifying data files to load from a stage, providing a discrete list of files is generally the fastest; however, the FILES parameter supports a maximum of 1,000 files, meaning a COPY command executed with the FILES parameter can only load up to 1,000 files. Using pattern matching, on the other hand, is not recommended for best performance - try to avoid applying patterns that filter on a large number of files.\n"
    },
    "8": {
        "q": "Question 9\n\nWhat are the default values of ON_ERROR property when loading a file using COPY command and Snowpipe respectively",
        "opt": {
            "0": "COPY command \u2192 ABORT_STATEMENT, Snowpipe \u2192 ABORT_STATEMENT",
            "1": "COPY command \u2192 SKIP_FILE; Snowpipe \u2192 SKIP_FILE",
            "2": "COPY command \u2192 ABORT_STATEMENT; Snowpipe \u2192 SKIP_FILE",
            "3": "COPY command \u2192 CONTINUE; Snowpipe \u2192 SKIP_FILE"
        },
        "ans": [
            2
        ],
        "exp": "\n\nCOPY INTO copyOptions\n\n\nON_ERROR =\n\nSKIP_FILE | SKIP_FILE_num | 'SKIP_FILE_num%' | ABORT_STATEMENT | CONTINUE\n\nFor Data loading only, String (constant) that specifies the error handling for the load operation.\n\n\n\nSKIP_FILE (Default for SnowPipe)\n\nSkip a file when an error is found. Note that the SKIP_FILE action buffers an entire file whether errors are found or not. For this reason, SKIP_FILE is slower than either CONTINUE or ABORT_STATEMENT. Skipping large files due to a small number of errors could result in delays and wasted credits. When loading large numbers of records from files that have no logical delineation (e.g. the files were generated automatically at rough intervals), consider specifying CONTINUE instead.\n\nAdditional patterns includes SKIP_FILE_num (e.g. SKIP_FILE_10) that skip a file when the number of error rows found in the file is equal to or exceeds the specified number, and 'SKIP_FILE_num%' (e.g. 'SKIP_FILE_10%') that skip a file when the percentage of error rows found in the file exceeds the specified percentage.\n\nABORT_STATEMENT (Default for Bulk loading using COPY)\n\nAbort the load operation if any error is found in a data file.\n\nNote that the load operation is not aborted if the data file cannot be found (e.g. because it does not exist or cannot be accessed), except when data files explicitly specified in the FILES parameter cannot be found.\n\n\n\nCONTINUE\n\nContinue to load the file if errors are found. The COPY statement returns an error message for a maximum of one error found per data file.\n\n\n"
    },
    "9": {
        "q": "Question 10\n\nWhat consideration should be made when loading data into Snowflake",
        "opt": {
            "0": "create small data files (~250MB) to optimize data loading.",
            "1": "The number of load operations that run in parallel can exceed the number of files to be loaded.",
            "2": "The number of data files that can be processed in parallel is determined by the warehouse."
        },
        "ans": [
            0,
            2
        ],
        "exp": "\n\nPreparing your data files\n\nGeneral file sizing Recommendations\n\nThe number of load operations that run in parallel cannot exceed the number of data files to be loaded. To optimize the number of parallel operations for a load, we recommend aiming to produce data files roughly 100-250 MB (or larger) in size compressed.\n\n\n\nPlanning a data load\n\nDedicating separate warehouses to load and query operations\n\nLoading large data sets can affect query performance. We recommend dedicating separate warehouses for loading and querying operations to optimize performance for each.\n\nThe number of data files that can be processed in parallel is determined by the amount of compute resources in a warehouse. If you follow the file sizing guidelines described in Preparing your data files, a data load requires minimal resources. Splitting larger data files allows the load to scale linearly.\n\n\n"
    },
    "10": {
        "q": "Question 11\n\nWhich stream type can be used for tracking the records in external tables",
        "opt": {
            "0": "insert-only",
            "1": "Standard",
            "2": "External",
            "3": "Append-only"
        },
        "ans": [
            0
        ],
        "exp": "\n\nStreams\n\nStreams are used for CDC (Change Data Capture) purposes, by recording DML changes (insert, updates and deletes) made to tables and views, as well as metadata about each change.\n\n\n\nTypes of Streams\n\nThe following stream types are available based on the metadata recorded by each:\n\nStandard: Tracks DML changes to the source table, including inserts, updates, and deletes\n\nAppend-only: Tracks row inserts only\n\ninsert-only: Tracks row inserts only. Supported on external tables only.\n"
    },
    "11": {
        "q": "Question 12\n\nWhat is generally the FASTEST way to bulk load data files from a stage",
        "opt": {
            "0": "Using pattern matching",
            "1": "Using the Snowpipe REST API",
            "2": "Specifying a list of specific files to load",
            "3": "Loading by path (internal stages) /prefix"
        },
        "ans": [
            2
        ],
        "exp": "\n\nLoading data\n\nBulk Load Options\n\nBy path (internal stages) / prefix (Amazon S3 bucket). See Organizing data by path.\n\nSpecifying a list of specific files to load.\n\nUsing pattern matching to identify specific files. See Pattern Matching in COPY INTO.\n\nOf the three options for identifying/specifying data files to load from a stage, providing a discrete list of files is generally the fastest; however, the FILES parameter supports a maximum of 1,000 files, meaning a COPY command executed with the FILES parameter can only load up to 1,000 files. Using pattern matching, on the other hand, is not recommended for best performance - try to avoid applying patterns that filter on a large number of files.\n"
    },
    "12": {
        "q": "Question 13\n\nWhile loading data through the COPY command, you can transform the data.\n\nWhich of the below transformations is NOT allowed",
        "opt": {
            "0": "Reorder columns",
            "1": "Filters",
            "2": "Cast",
            "3": "Omit columns",
            "4": "Truncate columns"
        },
        "ans": [
            1
        ],
        "exp": "\n\nCOPY INTO Transformation parameters\n\nThe COPY command supports:\n\nColumn reordering and column omission using a 'select' statement. There is no requirement for your data files to have the same ordering of columns as your target table.\n\nData types conversion (Cast). For example, convert strings as binary values, decimals, or timestamps using the TO_DECIMAL, TO_NUMBER, and TO_TIMESTAMP .\n\nThe ENFORCE_LENGTH | TRUNCATECOLUMNS option, which can truncate text strings that exceed the target column length.\n\nResources"
    },
    "13": {
        "q": "Question 13\n\nWhile loading data through the COPY command, you can transform the data.\n\nWhich of the below transformations is NOT allowed",
        "opt": {
            "0": "Reorder columns",
            "1": "Filters",
            "2": "Cast",
            "3": "Omit columns",
            "4": "Truncate columns"
        },
        "ans": [
            1
        ],
        "exp": "\n\nCOPY INTO Transformation parameters\n\nThe COPY command supports:\n\nColumn reordering and column omission using a 'select' statement. There is no requirement for your data files to have the same ordering of columns as your target table.\n\nData types conversion (Cast). For example, convert strings as binary values, decimals, or timestamps using the TO_DECIMAL, TO_NUMBER, and TO_TIMESTAMP .\n\nThe ENFORCE_LENGTH | TRUNCATECOLUMNS option, which can truncate text strings that exceed the target column length.\n\nResources"
    },
    "14": {
        "q": "Question 15\n\nWhich SQL command will consume a stream and advance the stream offset",
        "opt": {
            "0": "insert into table select from stream",
            "1": "alter table",
            "2": "select FROM STREAM",
            "3": "BEGIN COMMIT"
        },
        "ans": [
            0
        ],
        "exp": "\n\nStreams\n\nStreams are used for CDC (Change Data Capture) purposes, by recording DML changes (insert, updates and deletes) made to tables and views, as well as metadata about each change.\n\n\n\nStreams Offset Storage\n\nWhen created, a stream logically takes an initial snapshot of every row in the source object (e.g. table, external table, or the underlying tables for a view) by initializing a point in time (called an offset) as the current transactional version of the object.\n\nNote that a stream itself does not contain any table data. A stream only stores an offset for the source object and returns CDC records by leveraging the versioning history for the source object.\n\nWhen the first stream for a table is created, several hidden columns are added to the source table and begin storing change tracking metadata. These columns consume a small amount of storage.\n\n\n\nStreams Data Flow\n\nWhen created, a stream logically takes an initial snapshot of every row in the source object by initializing a point in time (called an offset) as the current transactional version of the object. The change tracking system utilized by the stream then records information about the DML changes after this snapshot was taken. Change records provide the state of a row before and after the change. Change information mirrors the column structure of the tracked source object and includes additional metadata columns that describe each change event.\n\nWhenever a DML statement consumes the stream contents, the stream position advances to track the next set of DML changes to the table (i.e. the changes in a table version).\n"
    },
    "15": {
        "q": "Question 16\n\nWhat is the recommended file size for the best load performance and to avoid size limitations during data load ",
        "opt": {
            "0": "100 MB to 250 MB, compressed",
            "1": "10 MB to 100 MB, uncompressed",
            "2": "10 GB to 100 GB, compressed",
            "3": "100 MB to 250 MB, uncompressed"
        },
        "ans": [
            0
        ],
        "exp": "\n\nPreparing your data files\n\nGeneral file sizing Recommendations\n\nThe number of load operations that run in parallel cannot exceed the number of data files to be loaded. To optimize the number of parallel operations for a load, we recommend aiming to produce data files roughly 100-250 MB (or larger) in size compressed. Loading very large files (e.g. 100 GB or larger) is not recommended. Aggregate smaller files to minimize the processing overhead for each file. Split larger files into a greater number of smaller files to distribute the load among the compute resources in an active warehouse. We recommend splitting large files by line to avoid records that span chunks.\n"
    },
    "16": {
        "q": "Question 17\n\nStreams support what level of isolation",
        "opt": {
            "0": "None",
            "1": "Repeatable read",
            "2": "Read committed",
            "3": "Read uncommitted"
        },
        "ans": [
            1
        ],
        "exp": "\n\nStreams\n\nStreams are used for CDC (Change Data Capture) purposes, by recording DML changes (insert, updates and deletes) made to tables and views, as well as metadata about each change.\n\n\n\nStreams Offset Storage\n\nWhen created, a stream logically takes an initial snapshot of every row in the source object (e.g. table, external table, or the underlying tables for a view) by initializing a point in time (called an offset) as the current transactional version of the object.\n\nNote that a stream itself does not contain any table data. A stream only stores an offset for the source object and returns CDC records by leveraging the versioning history for the source object.\n\nWhen the first stream for a table is created, several hidden columns are added to the source table and begin storing change tracking metadata. These columns consume a small amount of storage.\n\n\n\nStream Repeatable Read Isolation\n\nStreams support repeatable read isolation. In repeatable read mode, multiple SQL statements within a transaction see the same set of records in a stream. This differs from the read committed mode supported for tables, in which statements see any changes made by previous statements executed within the same transaction, even though those changes are not yet committed.\n\nThe delta records returned by streams in a transaction is the range from the current position of the stream until the transaction start time. The stream position advances to the transaction start time if the transaction commits; otherwise it stays at the same position.\n"
    },
    "17": {
        "q": "Question 18\n\nThe following COPY INTO command is run:\n\n>: COPY INTO TABLE VALIDATION_MODE= 'RETURN_ERRORS';\n\nAfter the execution, we realize that no data has been copied. Why is this happening",
        "opt": {
            "0": "The command should be VALIDATION_MODE = RETURN_ALL_ERRORS.",
            "1": "VALIDATION_MODE doesn\u2019t load data; it just validates the data.",
            "2": "Data in the stage contains errors."
        },
        "ans": [
            1
        ],
        "exp": "\n\nCOPY INTO Optional parameters\n\nVALIDATION_MODE = RETURN_n_ROWS | RETURN_ERRORS | RETURN_ALL_ERRORS\n\nString that instructs the COPY command to validate the data files instead of loading them into the specified table; i.e. the COPY command tests the files for errors but does not load them.\n\nThe command validates the data to be loaded and returns results based on the option specified:\n\nRETURN_n_ROWS (e.g. RETURN_10_ROWS): Validates the specified number of rows, if no errors are encountered; otherwise, fails at the first error encountered in the rows.\n\nRETURN_ERRORS: Returns all errors (parsing, conversion, etc.) across all files specified in the COPY statement.\n\nRETURN_ALL_ERRORS: Returns all errors across all files specified in the COPY statement, including files with errors that were partially loaded during an earlier load because the ON_ERROR copy option was set to CONTINUE.\n\n\nNote:\n\nVALIDATION_MODE does not support COPY statements that transform data during a load. If the parameter is specified, the COPY statement returns an error.\n\nUse the VALIDATE table function to view all errors encountered during a previous load. Note that this function also does not support COPY that transform data during a load.\n"
    },
    "18": {
        "q": "Question 19\n\nYou can unload data to which of the following",
        "opt": {
            "0": "External stage",
            "1": "All of the above",
            "2": "Table stage",
            "3": "Internal named stage"
        },
        "ans": [
            1
        ],
        "exp": "\n\nWhen Unloading data from a table (or query) into one or more files, you can choose one of the following locations:\n\nNamed internal stage (or table/user stage). The files can then be downloaded with GET.\n\nNamed external stage that references an external location (S3, Google, Azure).\n\nExternal location (Amazon S3, Google Cloud Storage, or Microsoft Azure).\n"
    },
    "19": {
        "q": "Question 20\n\nYou need to load staged data from an external stage. Which practices will provide the MOST efficient load performance",
        "opt": {
            "0": "Limit file names to under 30 characters.",
            "1": "Organize files into logical paths that reflect a scheduling pattern.",
            "2": "Store the files on the external stage to ensure caching is maintained.",
            "3": "PUT all files in a single directory."
        },
        "ans": [
            1
        ],
        "exp": "\n\nStaging data\n\nOrganizing data by path\n\nBoth internal (i.e. Snowflake) and external (S3, Google, Azure) stage references can include a path (or prefix in AWS terminology). When staging regular data sets, we recommend partitioning, organizing the data into logical paths that include identifying details such as geographical location or other source identifiers, along with a scheduling pattern, the date when the data was written.\n\nFor example, if you were storing data for an SOUTH AMERICAN company by geographical location, you might include identifiers such as continent, country, and city in paths along with data write dates:\n\nBrazil/Recife/2016/07/10/05/\n\nArgentina/BuenosAires/2016/06/01/11/\n\nUruguay/Montevideo/2016/12/21/03/\n\nOrganizing your data files by path lets you copy any fraction of the partitioned data into Snowflake with a single command. This allows you to execute concurrent COPY statements that match a subset of files, taking advantage of parallel operations.\n"
    },
    "20": {
        "q": "Question 21\n\nWhat are the requirements to allow a data consumer to create a stream on a data share table",
        "opt": {
            "0": "Store the files on the external stage to ensure caching is maintained.",
            "1": "Execute alter table ... set CHANGE_TRACKING = TRUE on the shared table",
            "2": "Set the data retention period for the shared table to > 0",
            "3": "create a secured view on the shared table to be used by the stream and add it to the data share",
            "4": "create a view on the shared table to be used by the stream and add it to the data share"
        },
        "ans": [
            1,
            2
        ],
        "exp": "\n\nStreams on shared objects\n\nData consumers can create streams in their own databases that record data manipulation language (DML) changes made to the shared source tables or views.\n\nYou can allow consumers to create streams on shared tables or secure views. This requires to:\n\nExtend the data retention period for the tables. A stream on a shared table does not extend the data retention period for the table, so you manually need to specify a longer data retention period with the DATA_RETENTION_TIME_IN_DAYS parameter.\n\nEnable change tracking on the shared tables or the underlying tables for a shared view. You set the CHANGE_TRACKING parameter.\n\nThe operations are not supported:\n\nCreating append-only streams on shares of secondary source objects is not supported.\n\nModifying a stream in another account is not supported.\n"
    },
    "21": {
        "q": "Question 22\n\nYou have been asked to automate an ETL process to be executed daily when new data arrives in a table. What is the best way to automate it in Snowflake",
        "opt": {
            "0": "Use a service provided by your cloud provider (such as AWS Lambda)",
            "1": "create a Snowflake Task",
            "2": "create a Stored Procedure"
        },
        "ans": [
            1
        ],
        "exp": "\n\nSnowflake Tasks\n\nA task can execute any one of the following types of SQL code:\n\nSingle SQL statement\n\nCall to a stored procedure\n\nProcedural logic using Snowflake Scripting\n\nTasks can be combined with table streams for continuous ELT workflows to process recently changed table rows. Streams ensure exactly once semantics for new or changed data in a table.\n\nTasks can also be used independently to generate periodic reports by inserting or merging rows into a report table or perform other periodic work. To automate a task that requires multiple SQL statements and is performed frequently, you might want to use a stored procedure within the task.\n"
    },
    "22": {
        "q": "Question 23\n\nAs a Data Engineer, you upload files into a stage containing new customer data. The ingest operation completes with no errors, using the following command:\nCOPY INTO my_table FROM @my_stage;\nA week later, you add more files to the stage so that now the stage contains a mixture of new customer data and updates to the previous data; original files were not removed.\n\n\nWhat will happen if you execute the same COPY INTO command ",
        "opt": {
            "0": "All data from only the newly-added files will be appended to the table.",
            "1": "All data from all the files will be appended.",
            "2": "The operation will fail with the error \u201cUNCERTAIN FILES IN STAGE\u201d.",
            "3": "Only data about new customers from the new files will be appended to the table."
        },
        "ans": [
            0
        ],
        "exp": "\n\nIDEMPOTENT\n\nIf you specify the name of the stage instead of the file in the FROM line of the COPY INTO statement, It will attempt to load all the files from the stage. However, the COPY INTO command knows which files it already loaded and it doesn't load the same file twice - Snowflake is designed like this to have a process that doesn't double-load files. In other words, it automatically helps you keep your processes IDEMPOTENT.\n\nYou could add a FORCE=TRUE; as the last line of your COPY INTO statement and then you would double the number of rows in your table.\n"
    },
    "23": {
        "q": "Question 24\n\nWhat different types of streams exist in Snowflake",
        "opt": {
            "0": "Merge",
            "1": "Append and Delete",
            "2": "Update-only",
            "3": "Append-only",
            "4": "Standard",
            "5": "insert-only"
        },
        "ans": [
            3,
            4,
            5
        ],
        "exp": "\n\nStreams\n\nStreams are used for CDC (Change Data Capture) purposes, by recording DML changes (insert, updates and deletes) made to tables and views, as well as metadata about each change.\n\n\n\nTypes of Streams\n\nThe following stream types are available based on the metadata recorded by each:\n\nStandard: Tracks DML changes to the source table, including inserts, updates, and deletes\n\nAppend-only: Tracks row inserts only\n\ninsert-only: Tracks row inserts only. Supported on external tables only.\n"
    },
    "24": {
        "q": "Question 25\n\nWhen unloading the data for file format type specified (TYPE = 'CSV'), SQL NULL can be converted to string \u2018null\u2019 using which file format option",
        "opt": {
            "0": "EMPTY_FIELD_AS_NULL",
            "1": "NULL_IF",
            "2": "ESCAPE_ENCLOSED"
        },
        "ans": [
            1
        ],
        "exp": "\n\nFile Format options - Empty strings and NULL values\n\nAn empty string is a string with zero length or no characters, whereas NULL values represent an absence of data. In CSV files, a NULL value is typically represented by two successive delimiters (e.g. ,,) to indicate that the field contains no data; however, you can use string values to denote NULL (e.g. null) or any unique string. An empty string is typically represented by a quoted empty string (e.g. '') to indicate that the string contains zero characters.\n\nThe following file format options enable you to differentiate between empty strings and NULL values when unloading or loading data.\n\n- FIELD_OPTIONALLY_ENCLOSED_BY = 'character' | NONE :\n\nUse this option to enclose strings in the specified character: single quote ('), double quote (\"), or NONE.\n\n-EMPTY_FIELD_AS_NULL = TRUE | FALSE:\n\nWhen unloading empty string data from tables, you can choose to Enclose strings in quotes to be treated as null values.\n\n-NULL_IF = ( 'string1' [ , 'string2' ... ] ):\n\nWhen unloading data from tables: Snowflake converts SQL NULL values to the first value in the list. Make sure to specify a value that will be interpreted as NULL by that system.\n"
    },
    "25": {
        "q": "Question 26\n\nWhich statements about Snowflake tasks are true",
        "opt": {
            "0": "A task can execute a single SQL statement",
            "1": "A task can execute multiple SQL statement",
            "2": "A task can execute a store procedure",
            "3": "A task can execute a function"
        },
        "ans": [
            0,
            2
        ],
        "exp": "\n\nSnowflake Tasks\n\nA task can execute any one of the following types of SQL code:\n\nSingle SQL statement\n\nCall to a stored procedure\n\nProcedural logic using Snowflake Scripting\n\nTasks can be combined with table streams for continuous ELT workflows to process recently changed table rows. Streams ensure exactly once semantics for new or changed data in a table.\n\nTasks can also be used independently to generate periodic reports by inserting or merging rows into a report table or perform other periodic work. To automate a task that requires multiple SQL statements and is performed frequently, you might want to use a stored procedure within the task.\n"
    },
    "26": {
        "q": "Question 27\n\nWhat types of services created and managed by the Cloud Providers does an event-driven pipeline in Snowflake depend on",
        "opt": {
            "0": "A Pub/Sub Notification service (SNS, Azure Event Hub, Clou pub/sub)",
            "1": "Messaging Queuing Service (SQS, Storage Queues, Cloud Tasks)",
            "2": "All of the above",
            "3": "A Storage Service (S3, GCS buckets, Blob)"
        },
        "ans": [
            2
        ],
        "exp": "\n\nSnowpipe\n\nSnowpipe is a fully Serverless feature that enables loading data from files as soon as they\u2019re available in a stage \u2013 you do not need to specify a warehouse. This means you can load data from files in micro-batches (small volume of frequent data), making it available to users within minutes, rather than manually executing COPY statements on a schedule to load larger batches.\n\nHow does Snowpipe work?\n\nDifferent mechanisms for detecting the staged files are available.\n\n\n\nAutomating Snowpipe using cloud messaging: Automated data loads leverage event notifications for cloud storage to inform Snowpipe of the arrival of new data files to load. Snowpipe polls the event notifications from a queue. By using the metadata in the queue, Snowpipe loads the new data files into the target table in a continuous, serverless fashion based on the parameters defined in the pipe object.\n\nCalling Snowpipe REST endpoints: Your client application calls a public REST endpoint with the name of a pipe object and a list of data filenames. If new data files matching the list are discovered in the stage referenced by the pipe object, they are queued for loading. Snowflake-provided compute resources load data from the queue into a table based on parameters defined in the pipe.\n\n\n\nFor example, when using Cloud Messaging on Amazon, the create PIPE statement subscribes the Amazon Simple Queue Service (SQS) queue to the specified SNS topic. The pipe copies files to the ingest queue triggered by event notifications via the SNS topic.\n\n\n"
    },
    "27": {
        "q": "Question 28\n\nA user needs to ingest 1 GB of data that is available in an external stage using a COPY INTO command. How can this be done with MAXIMUM performance and the LEAST cost",
        "opt": {
            "0": "Ingest in an uncompressed format as a single file",
            "1": "Split into smaller files (100-250MB) compressed",
            "2": "Ingest in a compressed format as a single file",
            "3": "Split into smaller files (100-250MB) uncompressed"
        },
        "ans": [
            1
        ],
        "exp": "\n\nPreparing your data files\n\nGeneral file sizing Recommendations\n\nThe number of load operations that run in parallel cannot exceed the number of data files to be loaded. To optimize the number of parallel operations for a load, we recommend aiming to produce data files roughly 100-250 MB (or larger) in size compressed. Loading very large files (e.g. 100 GB or larger) is not recommended. Aggregate smaller files to minimize the processing overhead for each file. Split larger files into a greater number of smaller files to distribute the load among the compute resources in an active warehouse. We recommend splitting large files by line to avoid records that span chunks.\n"
    },
    "28": {
        "q": "Question 29\n\nWhat will happen if we specify the name of the stage instead of the file in the FROM line of the COPY INTO statement",
        "opt": {
            "0": "It will attempt to load the last file from the stage",
            "1": "It will trigger an error; no files will be loaded",
            "2": "It will attempt to load the first file from the stage",
            "3": "It will attempt to load all the files from the stage"
        },
        "ans": [
            3
        ],
        "exp": "\n\nIDEMPOTENT\n\nIf you specify the name of the stage instead of the file in the FROM line of the COPY INTO statement, It will attempt to load all the files from the stage. However, the COPY INTO command knows which files it already loaded and it doesn't load the same file twice - Snowflake is designed like this to have a process that doesn't double-load files. In other words, it automatically helps you keep your processes IDEMPOTENT.\n\nYou could add a FORCE=TRUE; as the last line of your COPY INTO statement and then you would double the number of rows in your table.\n"
    },
    "29": {
        "q": "Question 30\n\nWhen executing a COPY INTO command, performance can be negatively affected by using which optional parameter on a large number of files",
        "opt": {
            "0": "FILES",
            "1": "VALIDATION_MODE",
            "2": "PATTERN"
        },
        "ans": [
            2
        ],
        "exp": "\n\nLoading data\n\nBulk Load Options\n\nBy path (internal stages) / prefix (Amazon S3 bucket). See Organizing data by path.\n\nSpecifying a list of specific files to load.\n\nUsing pattern matching to identify specific files. See Pattern Matching in COPY INTO.\n\nOf the three options for identifying/specifying data files to load from a stage, providing a discrete list of files is generally the fastest; however, the FILES parameter supports a maximum of 1,000 files, meaning a COPY command executed with the FILES parameter can only load up to 1,000 files. Using pattern matching, on the other hand, is not recommended for best performance - try to avoid applying patterns that filter on a large number of files.\n"
    },
    "30": {
        "q": "Question 31\n\nWhat are the steps to unload data from Snowflake to a cloud storage location",
        "opt": {
            "0": "Download the files using LS if is a Snowflake stage.",
            "1": "Download the files using AWS tools if is an external stage in Amazon S3.",
            "2": "Copy table data to stage files using GET.",
            "3": "Download the files using Snowflake GET command for all external stages.",
            "4": "Copy table data to stage files using the COPY INTO <location> command.",
            "5": "Download the files using GET if is a Snowflake stage.",
            "6": "Download the files using COPY INTO <location> command."
        },
        "ans": [
            1,
            4,
            5
        ],
        "exp": "\n\nThis set of topics describes how to use the COPY command to unload data from a table into an internal stage. You can then download the unloaded data files to your local file system.\n\nUnloading data to a local file system is performed in two, separate steps:\n\nCopy the data from table into a stage.\n\nDownload the file from the stage.\n\n\n\nStep 1. Use the COPY INTO <location> command to copy the data from the Snowflake database table into one or more files in a Snowflake stage. In the SQL statement, you specify the stage (named stage or table/user stage) where the files are written.\n\nRegardless of the stage you use, this step requires a running, current virtual warehouse for the session if you execute the command manually or within a script. The warehouse provides the compute resources to write rows from the table.\n\nStep 2. Download the file from the stage:\n\nFrom a Snowflake stage, use the GET command to download the data file(s).\n\nFrom S3, use the interfaces/tools provided by Amazon S3 to get the data file(s).\n\nFrom Azure, use the interfaces/tools provided by Microsoft Azure to get the data file(s).\n"
    },
    "31": {
        "q": "Question 32\n\nWhat is the maximum number supported by the FILES parameter in the COPY INTO <table> command",
        "opt": {
            "0": "200",
            "1": "500",
            "2": "1000",
            "3": "100",
            "4": "10000"
        },
        "ans": [
            2
        ],
        "exp": "\n\nThe COPY command supports several options for loading data files from a stage:\n\nBy path (internal stages) / prefix (Amazon S3 bucket). See Organizing data by path.\n\nSpecifying a list of specific files to load.\n\nUsing pattern matching to identify specific files by pattern.\n\nOf the three options for identifying/specifying data files to load from a stage, providing a discrete list of files is generally the fastest; however, the FILES parameter supports a maximum of 1,000 files, meaning a COPY command executed with the FILES parameter can only load up to 1,000 files. Using pattern matching, on the other hand, is not recommended for best performance - try to avoid applying patterns that filter on a large number of files.\n\n\n\nIf we specify the name of the stage instead of the file (or list of files) name(s), the COPY command will attempt to load all the files from the stage.\n"
    },
    "32": {
        "q": "Question 33\n\nWhat operations can be performed while loading a simple CSV file into a Snowflake table using the COPY INTO command",
        "opt": {
            "0": "Renaming columns",
            "1": "Reordering the columns",
            "2": "Converting data types",
            "3": "Grouping by operations",
            "4": "Perform aggregate calculations",
            "5": "selecting the first rows"
        },
        "ans": [
            1,
            2
        ],
        "exp": "\n\nCOPY INTO Transformation parameters\n\nThe COPY command supports:\n\nColumn reordering and column omission using a select statement. There is no requirement for your data files to have the same ordering of columns as your target table.\n\nData types conversion (Cast). For example, convert strings as binary values, decimals, or timestamps using the TO_DECIMAL, TO_NUMBER, and TO_TIMESTAMP .\n\nThe ENFORCE_LENGTH | TRUNCATECOLUMNS option, which can truncate text strings that exceed the target column length.\n"
    },
    "33": {
        "q": "Question 34\n\nWhat is the recommendation from Snowflake regarding splitting large files for data load",
        "opt": {
            "0": "By size",
            "1": "By line",
            "2": "By column"
        },
        "ans": [
            1
        ],
        "exp": "\n\nPreparing your data files\n\nGeneral file sizing Recommendations\n\nThe number of load operations that run in parallel cannot exceed the number of data files to be loaded. To optimize the number of parallel operations for a load, we recommend aiming to produce data files roughly 100-250 MB (or larger) in size compressed. Loading very large files (e.g. 100 GB or larger) is not recommended. Aggregate smaller files to minimize the processing overhead for each file. Split larger files into a greater number of smaller files to distribute the load among the compute resources in an active warehouse. We recommend splitting large files by line to avoid records that span chunks.\n"
    },
    "34": {
        "q": "Question 35\n\nAn internal stage was created using the following command:\n\n  :> create or replace stage my_stage file_format = my_file_format;\n\nWhich COPY command can be used to load files from the stage my_stage into the table CUSTOMERS ",
        "opt": {
            "0": "copy into CUSTOMERS from my_stage file_format = (type = csv);",
            "1": "copy into CUSTOMERS from @my_stage file_format = (type = my_file_format);",
            "2": "copy into CUSTOMER from @my_stage file_format = my_file_format;",
            "3": "copy into CUSTOMERS from @my_stage file_format = (format_name = my_file_format);"
        },
        "ans": [
            3
        ],
        "exp": "\n\nThe correct file format option of the COPY INTO command, when specifying the file format name, is \u201cfile_format = (format_name = xxx)\u201d.\n"
    },
    "35": {
        "q": "Question 1\n\nWhich cache stores object definitions and statistics",
        "opt": {
            "0": "Warehouse Cache",
            "1": "Metadata Cache",
            "2": "Result Cache"
        },
        "ans": [
            1
        ],
        "exp": "\n\nMetadata Cache\n\nThis cache stores information about database, schemas, tables, views and other database objects in memory for fast access. When a user queries a table or view, Snowflake checks if the metadata for the object is already in the Metadata Cache. If it is, Snowflake returns the cached metadata instead of retrieving it again from the database.\n\nThis cache has no time dependency and is not invalidated by changes to table data, but is invalidated if the schema or object definitions change. Table row count, table size in bytes, min and max values, number of distinct values and null count are stored in the Metadata Cache.\n"
    },
    "36": {
        "q": "Question 2\n\nWhich of the following significantly improves the performance of selective point lookup queries on a table",
        "opt": {
            "0": "Materialized Views",
            "1": "Query Acceleration Service",
            "2": "Search Optimization Service",
            "3": "Clustering a table"
        },
        "ans": [
            2
        ],
        "exp": "\n\nSearch Optimization Service (Enterprise Edition or higher)\n\nThe search optimization service aims to significantly improve the performance of certain Predicate Types of queries on tables, including:\n\nselective point lookup queries on tables. A point lookup query returns only one or a small number of distinct rows (c1=v1 | c1 IN (v1, v2) ). Use case examples include:\n\nThe need for fast response times for critical dashboards with highly selective filters.\n\nData scientists that explore large data volumes and looks for specific subsets of data.\n\nApplications retrieving a small set of results based on large set of filtering predicates.\n\nQueries using conjunctions (AND) and disjunctions (OR).\n\nSubstring and regular expression searches (e.g. [ NOT ] LIKE, [ NOT ] ILIKE, [ NOT ] RLIKE, etc.).\n\nQueries on fields in VARIANT, OBJECT, and ARRAY (semi-structured) columns that use the following types of predicates:\n\nEquality predicates (src:TEXT = 'true').\n\nIN predicates.\n\nPredicates that use ARRAY_CONTAINS and ARRAYS_OVERLAP.\n\nSubstring and regular expression predicates.\n\nPredicates that check for NULL values.\n\nQueries that use selected geospatial functions with GEOGRAPHY values.\n"
    },
    "37": {
        "q": "Question 3\n\nWhich function is used to profile warehouse credit usage",
        "opt": {
            "0": "WAREHOUSE_LOAD_HISTORY",
            "1": "WAREHOUSE_CREDIT_HISTORY",
            "2": "WAREHOUSE_METERING_HISTORY"
        },
        "ans": [
            2
        ],
        "exp": "\n\nWarehouse credit usage views and functions\n\n\n\nWAREHOUSE_METERING_HISTORY View\n\nThis Account Usage view can be used to return the hourly credit usage for a single warehouse (or all the warehouses in your account) within the last 365 days (1 year).\n\n\n\nWAREHOUSE_METERING_HISTORY function\n\nThis table function can be used in queries to return the hourly credit usage for a single warehouse (or all the warehouses in your account) within a specified date range.\n\n\n\nSnowsight Cost Management tab\n\nAll compute resources (virtual warehouses, serverless, cloud services) consume Snowflake credits. Users can use Snowsight to view the overall cost of compute usage for any given day, week, or month. To explore compute cost, (1) Sign in to Snowsight. Switch to the ACCOUNTADMIN role. If you are not the account administrator, switch to a role with access to cost and usage data, (2) Navigate to Admin \u00bb Cost Management.\n"
    },
    "38": {
        "q": "Question 4\n\nWhat information is included in the display in the Query Profile",
        "opt": {
            "0": "Credit usage for the account",
            "1": "Graphical representation of processing plan",
            "2": "Clustering key information",
            "3": "Index hints used on the query",
            "4": "Details and statistics for the overall query",
            "5": "Credit usage for each query"
        },
        "ans": [
            1,
            4
        ],
        "exp": "\n\nQuery Profile provides execution details and statistics for a query. For the selected query, it provides a graphical representation of the main components of the processing plan for the query, with statistics for each component, along with details and statistics for the overall query. It is designed to help you spot typical mistakes in SQL query expressions to identify potential performance bottlenecks and improvement opportunities.\n"
    },
    "39": {
        "q": "Question 5\n\nWhat does the \u201cpercentage scanned from cache\u201d represent in the Query Profile",
        "opt": {
            "0": "Data scanned from remote disk cache",
            "1": "Data scanned from query cache",
            "2": "Data scanned from local disk cache",
            "3": "Data scanned from result cache"
        },
        "ans": [
            2
        ],
        "exp": "\n\nQuery Profile Statistics\n\nA major source of information provided in the detail pane is the various statistics, grouped in the following sections:\n\nIO \u2014 information about the input-output operations performed during the query:\n\nScan progress \u2014 the percentage of data scanned for a given table so far.\n\nBytes scanned \u2014 the number of bytes scanned so far.\n\nPercentage scanned from cache \u2014 the percentage of data scanned from the local disk cache.\n\nBytes written \u2014 bytes written (e.g. when loading into a table).\n\nBytes written to result \u2014 bytes written to the result object. For example, select * from . . . would produce a set of results in tabular format representing each field in the selection. In general, the results object represents whatever is produced as a result of the query, and Bytes written to result represents the size of the returned result.\n\nBytes read from result \u2014 bytes read from the result object.\n\nExternal bytes scanned \u2014 bytes read from an external object, e.g. a stage.\n\nSpilling \u2014 information about disk usage for operations where intermediate results do not fit in memory:\n\nBytes spilled to local storage \u2014 volume of data spilled to local disk.\n\nBytes spilled to remote storage \u2014 volume of data spilled to remote disk.\n\nDML \u2014 statistics for Data Manipulation Language (DML) queries:\n\nNumber of rows inserted \u2014 number of rows inserted into a table (or tables).\n\nNumber of rows updated \u2014 number of rows updated in a table.\n\nNumber of rows deleted \u2014 number of rows deleted from a table.\n\nNumber of rows unloaded \u2014 number of rows unloaded during data export.\n\nPruning \u2014 information on the effects of table pruning:\n\nPartitions scanned \u2014 number of partitions scanned so far.\n\nPartitions total \u2014 total number of partitions in a given table.\n\nNetwork \u2014 network communication:\n\nBytes sent over the network \u2014 amount of data sent over the network.\n\nExternal Functions \u2014 information about calls to external functions:\n\nExtension Functions \u2014 information about calls to extension functions:\n"
    },
    "40": {
        "q": "Question 6\n\nSearch optimization works best to improve the performance of a query in which at least one of the columns accessed through the query filter operation of how many distinct values",
        "opt": {
            "0": "1,000 or more",
            "1": "100,000 or more",
            "2": "10,000 or more"
        },
        "ans": [
            1
        ],
        "exp": "\n\nSearch Optimization Service (Requires Enterprise Edition or higher)\n\nThe search optimization service aims to significantly improve the performance of certain Predicate Types of queries on tables.\n\nOnce identified you can enable search optimization for the columns and tables.\n\nThe search optimization service is generally transparent to users. Queries work the same as they do without search optimization; some are just faster. However, search optimization does have effects on certain other table operations.\n\nTo improve performance of search queries, the search optimization service creates and maintains a persistent data structure called a search access path, that keeps track of which values of the table\u2019s columns might be found in each of its micro-partitions, allowing some micro-partitions to be  when scanning the table. Search access path maintenance is transparent - you don\u2019t need to create a virtual warehouse for running the maintenance service.\n\nHowever, there is a cost for both storage and compute resources of maintenance.\n\n\n\nIdentifying Queries that can benefit from Search Optimization\n\nSearch optimization works best to improve the performance of the following queries:\n\nThe query involves a column or columns other than the primary cluster key.\n\nThe query typically runs for a few seconds or longer (before applying search optimization). Search optimization will not substantially improve the performance of a query that has a sub-second run time.\n\nAt least one of the columns accessed by the query filter operation has on the order of 100,000 distinct values or more.\n"
    },
    "41": {
        "q": "Question 7\n\nSnowflake compute costs depend on which of the following",
        "opt": {
            "0": "The number of warehouses in the account.",
            "1": "The sizes of running warehouses.",
            "2": "The number of rows returned in queries.",
            "3": "The time warehouses have run."
        },
        "ans": [
            1,
            3
        ],
        "exp": "\n\nVirtual warehouse credit usage\n\nA virtual warehouse is one or more clusters of compute resources that enable executing queries, loading data, and performing other DML operations. The web interface and other features use warehouses, such as Cross-Cloud Auto-Fulfillment or display information in dashboards.\n\nSnowflake credits are used to pay for the processing time used by each virtual warehouse.\n\n\n\nCompute Costs (Snowflake credits) are charged based on:\n\nThe number of virtual warehouses you use,\n\nHow long they run, and\n\nThe size of the warehouse.\n"
    },
    "42": {
        "q": "Question 8\n\nWhat does the orange bar on an operator represent when reviewing the Query Profile",
        "opt": {
            "0": "The fraction of time the operator consumed within the step",
            "1": "The cost of the operator in terms of CPU",
            "2": "A measure of progress of the operator\u2019s execution",
            "3": "The fraction of data scanned from cache vs disk"
        },
        "ans": [
            0
        ],
        "exp": "\n\nQuery Profile\n\nOperator Nodes by Execution Time\n\nA collapsible panel in the operator tree pane lists nodes by execution time in descending order, enabling users to quickly locate the costliest operator nodes in terms of execution time. The panel lists all nodes that lasted 1% or longer of the total execution time of the query (or the execution time for the displayed query step, if the query was executed in multiple processing steps).\n\n\n\n\nWhen a component is selected by clicking on the node, the panel shows information for the component. The detail pane is divided into 3 sections:\n\nExecution Time: Provides information about which processing tasks consumed query time (described in Query/Operator Details below). Additionally, for step-level information, it shows the state of the given step, and its execution time (orange bar).\n\nStatistics: Provides detailed information about various statistics (described in Query/Operator Details below).\n\nAttributes: Provides component-specific information (described in Operator Types below).\n"
    },
    "43": {
        "q": "Question 9\n\nWhich use case will always cause an exploding join in Snowflake",
        "opt": {
            "0": "A query with at least 10 left outer joins",
            "1": "A query that has not specified a join condition",
            "2": "A query that request too many columns",
            "3": "A query using a UNION without ALL"
        },
        "ans": [
            1
        ],
        "exp": "\n\nQuery Profile\n\nQuery Profile is a powerful tool for understanding the mechanics of queries. It can be used to know more about the performance or behavior of a particular query.\n\nThe common query problems identified by Query Profile includes:\n\nExploding joins: A common SQL mistake is joining tables without providing a join condition (resulting in a \u201cCartesian product\u201d), or providing a condition where records from one table match multiple records from another table.\n\nUNION without ALL: The difference between them is that UNION ALL simply concatenates inputs, while UNION does the same, but also performs duplicate elimination.\n\nThese queries show in Query Profile as a UnionAll operator with an extra Aggregate operator on top (which performs duplicate elimination).\n\nQueries too large to fit in memory / Data Spilling: Warehouse memory is not sufficient to hold intermediate results. The engine will start spilling the data to local disk - If the local disk space is not sufficient, the spilled data is then saved to remote disks.\n\nInefficient pruning (most often evidenced by large table scans): Observed by comparing Partitions scanned and Partitions total statistics in the TableScan operators.\n"
    },
    "44": {
        "q": "Question 10\n\nA warehouse ran for 70 seconds and gets suspended. After some time, it ran for another 12 seconds and get suspended. For how many seconds will you be billed",
        "opt": {
            "0": "72 seconds",
            "1": "82 seconds",
            "2": "180 seconds",
            "3": "130 seconds"
        },
        "ans": [
            3
        ],
        "exp": "\n\nVirtual warehouse credit usage\n\nA virtual warehouse is one or more clusters of compute resources that enable executing queries, loading data, and performing other DML operations. The web interface and other features use warehouses, such as Cross-Cloud Auto-Fulfillment or display information in dashboards.\n\nSnowflake credits are used to pay for the processing time used by each virtual warehouse.\n\nCompute Costs (Snowflake credits) are charged based on:\n\nThe number of virtual warehouses you use,\n\nHow long they run, and\n\nThe size of the warehouse.\n\nThe credit numbers shown above are for a full hour of usage; however,\n\ncredits are billed per-second, with a 60-second (i.e. 1-minute) minimum:\n\nEach time a warehouse is started or resumed, the warehouse is billed for 1 minute\u2019s worth of usage based on the hourly rate shown above.\n\nEach time a warehouse is resized to a larger size, the warehouse is billed for 1 minute\u2019s worth of usage; however, the number of credits billed are only for the additional compute resources that are provisioned. For ex, resizing from S (2 credits/hr) to M (4 credits/hr) results in billing charges for 1 minute\u2019s worth of 2 additional credits.\n\nAfter 1 minute, all subsequent billing is per-second as long as the warehouse runs continuously.\n\nSuspending and then resuming a warehouse within the first minute results in multiple charges because the 1-minute minimum starts over each time a warehouse is resumed.\n\nResizing a warehouse from 5X-Large or 6X-Large to 4X-Large (or smaller) results in a brief period during which the warehouse is billed for both the new compute resources and the old resources while the old resources are quiesced.\n\n\n\nIn this ## question, you will be billed for 130 seconds (70 + 60 s) because warehouses are billed for a minimum of one min. The price would be different if the warehouse wasn't suspended before executing the 2nd query. For example, if we had only run a query, and it had only run for 70 sec, you would be billed for these 70 sec. If it had only run for 20 sec, you would've been billed for 60 sec.\n"
    },
    "45": {
        "q": "Question 10\n\nA warehouse ran for 70 seconds and gets suspended. After some time, it ran for another 12 seconds and get suspended. For how many seconds will you be billed",
        "opt": {
            "0": "72 seconds",
            "1": "82 seconds",
            "2": "180 seconds",
            "3": "130 seconds"
        },
        "ans": [
            3
        ],
        "exp": "\n\nVirtual warehouse credit usage\n\nA virtual warehouse is one or more clusters of compute resources that enable executing queries, loading data, and performing other DML operations. The web interface and other features use warehouses, such as Cross-Cloud Auto-Fulfillment or display information in dashboards.\n\nSnowflake credits are used to pay for the processing time used by each virtual warehouse.\n\nCompute Costs (Snowflake credits) are charged based on:\n\nThe number of virtual warehouses you use,\n\nHow long they run, and\n\nThe size of the warehouse.\n\nThe credit numbers shown above are for a full hour of usage; however,\n\ncredits are billed per-second, with a 60-second (i.e. 1-minute) minimum:\n\nEach time a warehouse is started or resumed, the warehouse is billed for 1 minute\u2019s worth of usage based on the hourly rate shown above.\n\nEach time a warehouse is resized to a larger size, the warehouse is billed for 1 minute\u2019s worth of usage; however, the number of credits billed are only for the additional compute resources that are provisioned. For ex, resizing from S (2 credits/hr) to M (4 credits/hr) results in billing charges for 1 minute\u2019s worth of 2 additional credits.\n\nAfter 1 minute, all subsequent billing is per-second as long as the warehouse runs continuously.\n\nSuspending and then resuming a warehouse within the first minute results in multiple charges because the 1-minute minimum starts over each time a warehouse is resumed.\n\nResizing a warehouse from 5X-Large or 6X-Large to 4X-Large (or smaller) results in a brief period during which the warehouse is billed for both the new compute resources and the old resources while the old resources are quiesced.\n\n\n\nIn this ## question, you will be billed for 130 seconds (70 + 60 s) because warehouses are billed for a minimum of one min. The price would be different if the warehouse wasn't suspended before executing the 2nd query. For example, if we had only run a query, and it had only run for 70 sec, you would be billed for these 70 sec. If it had only run for 20 sec, you would've been billed for 60 sec.\n"
    },
    "46": {
        "q": "Question 12\n\nIn which situations is the Warehouse data cache used ",
        "opt": {
            "0": "The same way as the persist query results, but locally",
            "1": "It is used to store statistics when data is loaded",
            "2": "To keep Query Profiles statistics",
            "3": "To minimize how much data needs to be read from cloud storage"
        },
        "ans": [
            3
        ],
        "exp": "\n\nWarehouse cache / Local Disk cache / SSD Cache / Raw Data cache\n\nA running warehouse maintains a cache of table data that can be accessed by queries running on the same warehouse. This can improve the performance of subsequent queries if they are able to read from the cache instead of from tables.\n\nThis cache is implemented in the compute layer and stores the micro-partitions that were used from the first query and leaves them on your warehouse for future queries. If the next query needs those same micro-partitions to complete its task, it\u2019ll use the Warehouse Cache, rather than fetching the micro-partitions again. This cache has limited size and uses the LRU (Least Recently Used) algorithm.\n\nPruning will allow queries to fetch only the micro-partitions needed to fulfill the results; however, warehouse cache is made up of the entire micro-partitions that were fetched and not just the records that were selected in the first query.\n\n\n\nWarehouse Cache and Auto-suspension\n\nThe auto-suspension setting of the warehouse can have a direct impact on query performance because the cache is dropped when the warehouse is suspended. If a warehouse is running frequent and similar queries, it might not make sense to suspend the warehouse in between queries because the cache might be dropped before the next query is executed.\n"
    },
    "47": {
        "q": "Question 13\n\nWhat is true about the Query Result Cache ",
        "opt": {
            "0": "It's valid for 31 days",
            "1": "The same exact query will return the precomputed results even if the underlying data has changed as long as the results were last accessed within the previous 24 hour period",
            "2": "The 24hr of the precomputed results gets reset every time the exact query is executed",
            "3": "The exact query will ALWAYS return the pre-computed result set for the RESULT_CACHE_ACTIVE = time period",
            "4": "The same exact query will return the precomputed results if the results were accessed within the previous 24 hour and the underlying data hasn't changed",
            "5": "Snowflake edition is Enterprise or higher"
        },
        "ans": [
            2,
            4
        ],
        "exp": "\n\nQuery Result Cache\n\nThis cache stores query results in memory for fast access. When a user executes a query, Snowflake automatically checks if the query result is already in the Result Set Cache, as long as the queries run using the same Role. If it is, Snowflake returns the cached result instead of executing the query again. These are available across virtual warehouses, so query results returned to one user is available to any other user on the system who executes the same query.\n\nThe result cache is valid for 24 hours from the last run time of the query and is invalidated if the query syntax changes or the underlying data in the queried tables changes.\n\nNote: Each time the persisted result for a query is reused, Snowflake resets the 24-hour retention period for the result, up to a maximum of 31 days from the date and time that the query was first executed. After 31 days, the result is purged and the next time the query is submitted, a new result is generated and persisted.\n"
    },
    "48": {
        "q": "Question 14\n\nAs a Data Engineer, you want to take advantage of the RESULT_SCAN table function . What is true about this function",
        "opt": {
            "0": "You can use ORDER BY clauses with the RESULT_SCAN function",
            "1": "The query result is cached for 24 hours if the underlying data has not changed",
            "2": "The query result is cached for 31 days if the underlying data has not changed",
            "3": "A materialized view stores the result set for each subsequent retrieval",
            "4": "The RESULT_SCAN function can only return rows in the same order as the original query"
        },
        "ans": [
            0,
            1
        ],
        "exp": "\n\nRESULT_SCAN function\n\nReturns the result set of a previous command (within 24 hours of when you executed the query) as if the result was a table.\n\nThis is particularly useful if you want to process the output from any of the following:\n\nSHOW or DESC[RIBE] command that you executed.\n\nQuery you executed on metadata or account usage information, such as Snowflake Information Schema or Account Usage.\n\nThe result of a stored procedure that you called.\n\n\n\nRESULT_SCAN Usage notes\n\nIf the original query is executed manually, only the user who runs the original query can use the RESULT_SCAN function to process the output of the query. Even a user with the ACCOUNTADMIN privilege cannot access the results of another user\u2019s query by calling RESULT_SCAN.\n\nIf the original query is executed via a task, the role that owns the task, instead of a specific user, triggers and runs the query. If a user or a task is operating with the same role, they can use RESULT_SCAN to access the query results.\n\nSnowflake stores all query results for 24 hours. This function only returns results for queries executed within this time period.\n\nResult sets do not have any metadata associated with them, so processing large results might be slower than if you were querying an actual table.\n\nThe query containing the RESULT_SCAN can include clauses, such as filters and ORDER BY clauses, that were not in the original query. This allows you to narrow down or modify the result set.\n\nA RESULT_SCAN is not guaranteed to return rows in the same order as the original query returned the rows. You can include an ORDER BY clause with the RESULT_SCAN query to specify a specific order.\n"
    },
    "49": {
        "q": "Question 15\n\nWhich functions can be used to determine the number of distinct values in a table, to identify queries that benefits from Search Optimization",
        "opt": {
            "0": "APPROX_COUNT_DISTINCT(<col>)",
            "1": "COUNT(DISTINCT <col_name>)",
            "2": "GET_DISTINCT_VALUES(<col>)",
            "3": "RETURN_DISTINCT(<col_name>)"
        },
        "ans": [
            0,
            1
        ],
        "exp": "\n\nIdentifying Queries that can benefit from Search Optimization\n\nSearch optimization works best to improve the performance of the following queries:\n\nThe query involves a column or columns other than the primary cluster key.\n\nThe query typically runs for a few seconds or longer (before applying search optimization). Search optimization will not substantially improve the performance of a query that has a sub-second run time.\n\nAt least one of the columns accessed by the query filter operation has on the order of 100,000 distinct values or more.\n\n\n\nTo determine the number of distinct values, you can use either of the following:\n\nUse APPROX_COUNT_DISTINCT <col> to get the approx number of distinct values:\n\nselect approx_count_distinct(column1) from table1;\n\nUse COUNT(DISTINCT <col>) to get the actual number of distinct values:\n\nselect count(distinct c1), count (distinct c2) from test_table;\n\n\n\nNote: Because you need only an approximation of the number of distinct values, consider using APPROX_COUNT_DISTINCT, generally faster and cheaper than COUNT(DISTINCT <col>).\n"
    },
    "50": {
        "q": "Question 16\n\nWhen you use Search Optimization Service, you will end up with:\n\n",
        "opt": {
            "0": "Only Compute costs",
            "1": "Both storage and compute costs",
            "2": "Only Storage costs",
            "3": "No additional costs since it is a metadata operation"
        },
        "ans": [
            1
        ]
    },
    "51": {
        "q": "Question 17\n\nWhich of the following are issues that can be identified using the Query Profile",
        "opt": {
            "0": "Incorrectly used Window functions",
            "1": "Inefficient pruning",
            "2": "When CTEs lacks a final select",
            "3": "When there are exploding join",
            "4": "When there is a UNION without ALL"
        },
        "ans": [
            1,
            3,
            4
        ],
        "exp": "\n\nQuery Profile\n\nQuery Profile is a powerful tool for understanding the mechanics of queries. It can be used to know more about the performance or behavior of a particular query.\n\nThe common query problems identified by Query Profile includes:\n\nExploding joins: A common SQL mistake is joining tables without providing a join condition (resulting in a \u201cCartesian product\u201d), or providing a condition where records from one table match multiple records from another table.\n\nUNION without ALL: The difference between them is that UNION ALL simply concatenates inputs, while UNION does the same, but also performs duplicate elimination.\n\nThese queries show in Query Profile as a UnionAll operator with an extra Aggregate operator on top (which performs duplicate elimination).\n\nQueries too large to fit in memory / Data Spilling: Warehouse memory is not sufficient to hold intermediate results. The engine will start spilling the data to local disk - If the local disk space is not sufficient, the spilled data is then saved to remote disks.\n\nInefficient pruning (most often evidenced by large table scans): Observed by comparing Partitions scanned and Partitions total statistics in the TableScan operators.\n"
    },
    "52": {
        "q": "Question 18\n\nWhich Snowflake techniques are used to optimize query performance",
        "opt": {
            "0": "Materialized views",
            "1": "Search optimization",
            "2": "Clustering a table",
            "3": "Query acceleration",
            "4": "Transient Tables",
            "5": "Regular views"
        },
        "ans": [
            0,
            1,
            2,
            3
        ],
        "exp": "\n\nOptimizing Query Performance\n\nTechniques for optimize query performance include:\n\nSearch optimization service\n\nQuery acceleration service\n\nClustering a table\n\nMaterialized views (clustered or non-clustered)\n\n\n\nSearch Optimization Service supports:\n\nEquality searches.\n\nSubstring and regular expression searches.\n\nSearches on VARIANT fields.\n\nSearches using geospatial functions.\n\nQuery Acceleration Service supports:\n\nQueries with filters or aggregation.\n\nQuery acceleration works well with ad-hoc analytics, queries with unpredictable data volume, and queries with large scans and selective filters.\n\nThe filters must be highly selective, and the ORDER BY clause must have a low cardinality.\n\nIf the query includes LIMIT, must also include ORDER BY.\n\nMaterialized View supports:\n\nEquality searches.\n\nRange searches.\n\nSort operations.\n\nClustering the Table\n\nEquality searches.\n\nRange searches.\n"
    },
    "53": {
        "q": "Question 19\n\nWhat is a symptom that a query is too large to fit in memory",
        "opt": {
            "0": "Partitions scanned is equal to partitions total",
            "1": "An aggregate operator is present",
            "2": "The query is spilling to local or remote disk",
            "3": "A single join uses 50% of the time"
        },
        "ans": [
            2
        ],
        "exp": "\n\nQuery Profile\n\nQuery Profile is a powerful tool for understanding the mechanics of queries. It can be used to know more about the performance or behavior of a particular query.\n\nThe common query problems identified by Query Profile includes:\n\nExploding joins: A common SQL mistake is joining tables without providing a join condition (resulting in a \u201cCartesian product\u201d), or providing a condition where records from one table match multiple records from another table.\n\nUNION without ALL: The difference between them is that UNION ALL simply concatenates inputs, while UNION does the same, but also performs duplicate elimination.\n\nThese queries show in Query Profile as a UnionAll operator with an extra Aggregate operator on top (which performs duplicate elimination).\n\nQueries too large to fit in memory / Data Spilling: Warehouse memory is not sufficient to hold intermediate results. The engine will start spilling the data to local disk - If the local disk space is not sufficient, the spilled data is then saved to remote disks.\n\nInefficient pruning (most often evidenced by large table scans): Observed by comparing Partitions scanned and Partitions total statistics in the TableScan operators.\n"
    },
    "54": {
        "q": "Question 20\n\nThe search optimization results in an additional persistent storage structure called\u2026\n\n",
        "opt": {
            "0": "Search Access Path",
            "1": "Micro-partitions",
            "2": "Query Acceleration"
        },
        "ans": [
            0
        ]
    },
    "55": {
        "q": "Question 21\n\nWhy do Snowflake\u2019s virtual warehouses have scaling policies",
        "opt": {
            "0": "To help control credits consumed by a multi-cluster running in auto-scale mode.",
            "1": "To help control credits consumed by a multi-cluster running in maximized mode.",
            "2": "To improve performance of serverless compute features.",
            "3": "To help save extra storage costs."
        },
        "ans": [
            0
        ],
        "exp": "\n\nSetting a Scaling Policy for a Multi-cluster Warehouse\n\nTo help control the credits consumed by a multi-cluster warehouse running in Auto-scale mode, Snowflake provides scaling policies, which are used to determine when to start or shut down a cluster. The scaling policy for a multi-cluster warehouse only applies if it is running in Auto-scale mode. In Maximized mode, all clusters run concurrently so there is no need to start or shut down individual clusters.\n"
    },
    "56": {
        "q": "Question 22\n\nWhen a multi-cluster warehouse is suspended, which of the following Snowflake cache will be purged",
        "opt": {
            "0": "All of the above",
            "1": "Local disk cache",
            "2": "Remote disk cache (Result set cache)",
            "3": "Metadata cache"
        },
        "ans": [
            1
        ],
        "exp": "\n\nWarehouse cache / Local Disk cache / SSD Cache / Raw Data cache\n\nA running warehouse maintains a cache of table data that can be accessed by queries running on the same warehouse. This can improve the performance of subsequent queries if they are able to read from the cache instead of from tables.\n\nThis cache is implemented in the compute layer and stores the micro-partitions that were used from the first query and leaves them on your warehouse for future queries. If the next query needs those same micro-partitions to complete its task, it\u2019ll use the Warehouse Cache, rather than fetching the micro-partitions again. This cache has limited size and uses the LRU (Least Recently Used) algorithm.\n\nPruning will allow queries to fetch only the micro-partitions needed to fulfill the results; however, warehouse cache is made up of the entire micro-partitions that were fetched and not just the records that were selected in the first query.\n\n\n\nWarehouse Cache and Auto-suspension\n\nThe auto-suspension setting of the warehouse can have a direct impact on query performance because the cache is dropped when the warehouse is suspended. If a warehouse is running frequent and similar queries, it might not make sense to suspend the warehouse in between queries because the cache might be dropped before the next query is executed.\n"
    },
    "57": {
        "q": "Question 23\n\nWhy do Snowflake\u2019s virtual warehouses have scaling policies",
        "opt": {
            "0": "To improve performance of serverless compute features",
            "1": "To help control credits consumed by a multi-cluster running in auto-scale mode",
            "2": "To help save extra storage costs",
            "3": "To help control credits consumed by a multi-cluster running in maximized mode"
        },
        "ans": [
            1
        ],
        "exp": "\n\nMulti-cluster Warehouse - Setting a Scaling Policy\n\nTo help control the credits consumed by a multi-cluster warehouse running in Auto-scale mode, Snowflake provides scaling policies, which are used to determine when to start or shut down a cluster. The scaling policy for a multi-cluster warehouse only applies if it is running in Auto-scale mode. In Maximized mode, all clusters run concurrently so there is no need to start or shut down individual clusters.\n"
    },
    "58": {
        "q": "Question 24\n\nWhat can be used to view warehouse usage over time",
        "opt": {
            "0": "The WAREHOUSE_COSTS_HISTORY view",
            "1": "The Load History view",
            "2": "The WAREHOUSE_METERING_HISTORY view",
            "3": "The Query History view",
            "4": "The Monitoring Cost tab in Snowsight"
        },
        "ans": [
            2,
            4
        ],
        "exp": "\n\nWarehouse credit usage views and functions\n\nWAREHOUSE_METERING_HISTORY View: This Account Usage view can be used to return the hourly credit usage for a single warehouse (or all the warehouses in your account) within the last 365 days.\n\nWAREHOUSE_METERING_HISTORY function: This table function can be used in queries to return the hourly credit usage for a single warehouse (or all the warehouses in your account) within a specified date range.\n\nSnowsight Cost Management tab: All compute resources (virtual warehouses, serverless, cloud services) consume Snowflake credits. Users can use Snowsight to view the overall cost of compute usage for any given day, week, or month. To explore compute cost:\n\nSign in to Snowsight. Switch to the ACCOUNTADMIN role. If you are not the account administrator, switch to a role with access to cost and usage data.\n\nNavigate to Admin \u00bb Cost Management.\n"
    },
    "59": {
        "q": "Question 25\n\nAs a Data Engineer, you have been asked to check why a query is taking too long. Query Profile shows that all partitions are being scanned. What is causing the performance issue",
        "opt": {
            "0": "Pruning is not being performed efficiently",
            "1": "Incorrect joins are being used",
            "2": "Data is being spilled to remote disk",
            "3": "A huge volume of data is being fetched"
        },
        "ans": [
            0
        ],
        "exp": "\n\nQuery Profile Statistics\n\nA major source of information provided in the detail pane is the various statistics, grouped in the following sections:\n\nIO \u2014 information about the input-output operations performed during the query:\n\nScan progress \u2014 the percentage of data scanned for a given table so far.\n\nBytes scanned \u2014 the number of bytes scanned so far.\n\nPercentage scanned from cache \u2014 the percentage of data scanned from the local disk cache.\n\nBytes written \u2014 bytes written (e.g. when loading into a table).\n\nBytes written to result \u2014 bytes written to the result object. For example, select * from . . . would produce a set of results in tabular format representing each field in the selection. In general, the results object represents whatever is produced as a result of the query, and Bytes written to result represents the size of the returned result.\n\nBytes read from result \u2014 bytes read from the result object.\n\nExternal bytes scanned \u2014 bytes read from an external object, e.g. a stage.\n\nSpilling \u2014 information about disk usage for operations where intermediate results do not fit in memory:\n\nBytes spilled to local storage \u2014 volume of data spilled to local disk.\n\nBytes spilled to remote storage \u2014 volume of data spilled to remote disk.\n\nDML \u2014 statistics for Data Manipulation Language (DML) queries:\n\nNumber of rows inserted \u2014 number of rows inserted into a table (or tables).\n\nNumber of rows updated \u2014 number of rows updated in a table.\n\nNumber of rows deleted \u2014 number of rows deleted from a table.\n\nNumber of rows unloaded \u2014 number of rows unloaded during data export.\n\nPruning \u2014 information on the effects of table pruning:\n\nPartitions scanned \u2014 number of partitions scanned so far.\n\nPartitions total \u2014 total number of partitions in a given table.\n\nNetwork \u2014 network communication:\n\nBytes sent over the network \u2014 amount of data sent over the network.\n\nExternal Functions \u2014 information about calls to external functions:\n\nExtension Functions \u2014 information about calls to extension functions:\n"
    },
    "60": {
        "q": "Question 26\n\nWhich cache type gets purged in a predictable way so that it ends up empty of all cached information",
        "opt": {
            "0": "Query Result cache",
            "1": "Warehouse cache",
            "2": "Metadata cache"
        },
        "ans": [
            1
        ],
        "exp": "\n\nWarehouse cache / Local Disk cache / SSD Cache / Raw Data cache\n\nA running warehouse maintains a cache of table data that can be accessed by queries running on the same warehouse. This can improve the performance of subsequent queries if they are able to read from the cache instead of from tables.\n\nThis cache is implemented in the compute layer and stores the micro-partitions that were used from the first query and leaves them on your warehouse for future queries. If the next query needs those same micro-partitions to complete its task, it\u2019ll use the Warehouse Cache, rather than fetching the micro-partitions again. This cache has limited size and uses the LRU (Least Recently Used) algorithm.\n\nPruning will allow queries to fetch only the micro-partitions needed to fulfill the results; however, warehouse cache is made up of the entire micro-partitions that were fetched and not just the records that were selected in the first query.\n\n\n\nWarehouse Cache and Auto-suspension\n\nThe auto-suspension setting of the warehouse can have a direct impact on query performance because the cache is dropped when the warehouse is suspended. If a warehouse is running frequent and similar queries, it might not make sense to suspend the warehouse in between queries because the cache might be dropped before the next query is executed.\n"
    },
    "61": {
        "q": "Question 27\n\nA data engineer wants to ensure their Python Notebook workloads do not interfere with other SQL query executions in Snowflake. What is the best way to separate these workloads",
        "opt": {
            "0": "Use SYSTEM$STREAMLIT_NOTEBOOK_WH, a dedicated Snowflake-managed warehouse for Notebooks.",
            "1": "Disable the default warehouse for Notebooks and use only manual warehouses.",
            "2": "Run all Notebook queries on the default user warehouse.",
            "3": "create a custom large warehouse specifically for Python workloads."
        },
        "ans": [
            0
        ],
        "exp": "\n\nWarehouse usage in sessions\n\nWhen a session is initiated in Snowflake, the session does not, by default, have a warehouse associated with it - queries cannot be submitted until a WH is associated to the session.\n\n\n\nDefault warehouse for users\n\nTo facilitate querying immediately after a session is initiated, Snowflake supports specifying a default warehouse for each individual user using create USER/alter USER.\n\n\n\nDefault warehouse for client utilities/drivers/connectors\n\nIn addition to default warehouses for users, any of the Snowflake clients (SnowSQL, JDBC driver, ODBC driver, Python connector, etc.) can have a default warehouse:\n\n\n\nDefault warehouse for Notebooks\n\nA dedicated Snowflake-managed warehouse with the name SYSTEM$STREAMLIT_NOTEBOOK_WH is automatically provisioned in each account for running Notebook apps. This warehouse has the following properties:\n\nThe warehouse is owned and managed by Snowflake under the SYSTEM role. You cannot drop or alter this warehouse.\n\nIt is a multi-cluster X-Small warehouse, with a maximum cluster count of 10. The default timeout is 60 seconds.\n\nThe warehouse only runs Notebook jobs. Any SQL queries issued from a Notebook app are sent to a separate, customer-managed query warehouse.\n\n\n\nUsing SYSTEM$STREAMLIT_NOTEBOOK_WH offers several benefits:\n\nSeparating Notebook Python workloads from SQL queries reduces cluster fragmentation. This optimizes your overall costs as Notebooks Python workloads are not co-located on larger warehouses, which are often used for query execution.\n\nHaving a single dedicated warehouse for all Notebook apps in an account reduces fragmentation and aids in better bin packing.\n"
    },
    "62": {
        "q": "Question 28\n\nWhat causes a Warehouse's cache to be purged",
        "opt": {
            "0": "To enable a new behavior change bundle",
            "1": "Resuming the warehouse",
            "2": "Suspending the warehouse",
            "3": "To keep the warehouse running for more than 24 hours"
        },
        "ans": [
            2
        ],
        "exp": "\n\nWarehouse cache / Local Disk cache / SSD Cache / Raw Data cache\n\nA running warehouse maintains a cache of table data that can be accessed by queries running on the same warehouse. This can improve the performance of subsequent queries if they are able to read from the cache instead of from tables.\n\nThis cache is implemented in the compute layer and stores the micro-partitions that were used from the first query and leaves them on your warehouse for future queries. If the next query needs those same micro-partitions to complete its task, it\u2019ll use the Warehouse Cache, rather than fetching the micro-partitions again. This cache has limited size and uses the LRU (Least Recently Used) algorithm.\n\nPruning will allow queries to fetch only the micro-partitions needed to fulfill the results; however, warehouse cache is made up of the entire micro-partitions that were fetched and not just the records that were selected in the first query.\n\n\n\nWarehouse Cache and Auto-suspension\n\nThe auto-suspension setting of the warehouse can have a direct impact on query performance because the cache is dropped when the warehouse is suspended. If a warehouse is running frequent and similar queries, it might not make sense to suspend the warehouse in between queries because the cache might be dropped before the next query is executed.\n"
    },
    "63": {
        "q": "Question 29\n\nA Data Engineer creates a view to fetch data which is aggregated on SALES_DATE and LOCATION, using the following command:\n\ncreate OR REPLACE VIEW VW_FETCHAGGREGATEDDATA (SALES_DATE, LOCATION, SUM_REVENUE) AS\nselect SALES_DATE, LOCATION, SUM(REVENUE) AS SUM_REVENUE\nFROM SALES\nGROUP BY SALES_DATE, LOCATION;\nThe view results are slow to process as millions of records are retrieved.\n\nHow could the Architect use cluster keys to address this latency",
        "opt": {
            "0": "Cluster the columns from this view that are most actively used in selective filters.",
            "1": "Use the command alter VIEW VW_FETCHAGGREGATEDDATA RECLUSTER;",
            "2": "Recreate this view to be a materialized view then execute the command",
            "3": "alter MATERIALIZED VIEW VW_FETCHAGGREGATEDDATA CLUSTER BY(SALES_DATE,LOCATION);",
            "4": "Use the command alter VIEW VW_FETCHAGGREGATEDDATA CLUSTER BY (SALES_DATE,LOCATION);"
        },
        "ans": [
            0
        ],
        "exp": "\n\nMaterialized Views and Clustering\n\nDefining a clustering key on a materialized view is supported and can increase performance in many situations. However, it also adds costs.\n\nIf you cluster both the materialized view(s) and the base table on which the materialized view(s) are defined, you can cluster the materialized view(s) on different columns from the columns used to cluster the base table.\n\nIf you plan to create a table, load it, and create a clustered materialized view(s) on the table, then Snowflake recommends that you create the materialized views last (after loading as much data as possible). This can save money on the initial data load, because it avoids some extra effort to maintain the clustering of the materialized view the first time that the materialized view is loaded.\n\nNote that Standard views cannot be clustered.\n\n\n\nYou can add clustering or change the clustering key. For example, to cluster on installation_year:\n\nalter MATERIALIZED VIEW vulnerable_pipes CLUSTER BY (installation_year);"
    },
    "64": {
        "q": "Question 30\n\nCompute cost in Snowflake depends on\u2026\n\n",
        "opt": {
            "0": "The query execution time and the waiting query time",
            "1": "The warehouse size and for how long it runs",
            "2": "The query execution time"
        },
        "ans": [
            1
        ]
    },
    "65": {
        "q": "Question 31\n\nWhat is the Snowflake multi-clustering feature for virtual warehouses used for",
        "opt": {
            "0": "To improve concurrency for users and queries",
            "1": "To speed up slow or stalled queries",
            "2": "To improve data unloading for large datasets",
            "3": "To improve data loading"
        },
        "ans": [
            0
        ],
        "exp": "\n\nMulti-cluster Warehouses\n\nBy default, a virtual warehouse consists of a single cluster of compute resources available to the warehouse for executing queries. If sufficient resources are not available to execute concurrent queries submitted to the warehouse, the additional queries are queue until the necessary resources become available. Multi-cluster warehouses supports allocating, statically or dynamically, additional clusters to make a larger pool of compute resources available and improve concurrency management.\n"
    },
    "66": {
        "q": "Question 32\n\nWhat is the TableScan node statistics showing",
        "opt": {
            "0": "The task reads 760 Mb of data divided in 100 thousand rows",
            "1": "The local spilling on this task is 2% of the data",
            "2": "There is 99.79% of data coming from the result cache",
            "3": "The code executed is not offering any partition pruning opportunities"
        },
        "ans": [
            3
        ],
        "exp": "\n\nQuery Profile Statistics\n\nA major source of information provided in the detail pane is the various statistics, grouped in the following sections:\n\nIO \u2014 information about the input-output operations performed during the query:\n\nScan progress \u2014 the percentage of data scanned for a given table so far.\n\nBytes scanned \u2014 the number of bytes scanned so far.\n\nPercentage scanned from cache \u2014 the percentage of data scanned from the local disk cache.\n\nBytes written \u2014 bytes written (e.g. when loading into a table).\n\nBytes written to result \u2014 bytes written to the result object. For example, select * from . . . would produce a set of results in tabular format representing each field in the selection. In general, the results object represents whatever is produced as a result of the query, and Bytes written to result represents the size of the returned result.\n\nBytes read from result \u2014 bytes read from the result object.\n\nExternal bytes scanned \u2014 bytes read from an external object, e.g. a stage.\n\nSpilling \u2014 information about disk usage for operations where intermediate results do not fit in memory:\n\nBytes spilled to local storage \u2014 volume of data spilled to local disk.\n\nBytes spilled to remote storage \u2014 volume of data spilled to remote disk.\n\nDML \u2014 statistics for Data Manipulation Language (DML) queries:\n\nNumber of rows inserted \u2014 number of rows inserted into a table (or tables).\n\nNumber of rows updated \u2014 number of rows updated in a table.\n\nNumber of rows deleted \u2014 number of rows deleted from a table.\n\nNumber of rows unloaded \u2014 number of rows unloaded during data export.\n\nPruning \u2014 information on the effects of table pruning:\n\nPartitions scanned \u2014 number of partitions scanned so far.\n\nPartitions total \u2014 total number of partitions in a given table.\n\nNetwork \u2014 network communication:\n\nBytes sent over the network \u2014 amount of data sent over the network.\n\nExternal Functions \u2014 information about calls to external functions:\n\nExtension Functions \u2014 information about calls to extension functions:\n"
    },
    "67": {
        "q": "Question 33\n\nYou executed a query that is running for a long time. Looking at the query profile, a lot of data is spilling to disk. In normal situations, what would be the cause ",
        "opt": {
            "0": "Warehouse memory is not sufficient to hold intermediate query results",
            "1": "The result cache is almost full and unable to hold intermediate query results",
            "2": "The staging area is not sufficient to hold intermediate query results",
            "3": "Clustering is not set on the table"
        },
        "ans": [
            0
        ],
        "exp": "\n\nQuery Profile\n\nQuery Profile is a powerful tool for understanding the mechanics of queries. It can be used to know more about the performance or behavior of a particular query.\n\nThe common query problems identified by Query Profile includes:\n\nExploding joins: A common SQL mistake is joining tables without providing a join condition (resulting in a \u201cCartesian product\u201d), or providing a condition where records from one table match multiple records from another table.\n\nUNION without ALL: The difference between them is that UNION ALL simply concatenates inputs, while UNION does the same, but also performs duplicate elimination.\n\nThese queries show in Query Profile as a UnionAll operator with an extra Aggregate operator on top (which performs duplicate elimination).\n\nQueries too large to fit in memory / Data Spilling: Warehouse memory is not sufficient to hold intermediate results. The engine will start spilling the data to local disk - If the local disk space is not sufficient, the spilled data is then saved to remote disks.\n\nInefficient pruning (most often evidenced by large table scans): Observed by comparing Partitions scanned and Partitions total statistics in the TableScan operators.\n"
    },
    "68": {
        "q": "Question 34\n\nA Data Analyst executed a query that spent 15 minutes to run. Five hours later, he executed the same query again. This time, however, the results were returned immediately.  Why",
        "opt": {
            "0": "Snowflake used the persisted query results from the metadata cache",
            "1": "Snowflake used the persisted query results from the warehouse cache",
            "2": "Snowflake used the persisted query results from the query result cache",
            "3": "A new behavior change bundle was enabled"
        },
        "ans": [
            2
        ],
        "exp": "\n\nQuery Result Cache\n\nThis cache stores query results in memory for fast access. When a user executes a query, Snowflake automatically checks if the query result is already in the Result Set Cache, as long as the queries run using the same Role. If it is, Snowflake returns the cached result instead of executing the query again. These are available across virtual warehouses, so query results returned to one user is available to any other user on the system who executes the same query.\n\nThe result cache is valid for 24 hours from the last run time of the query and is invalidated if the query syntax changes or the underlying data in the queried tables changes.\n\nNote: Each time the persisted result for a query is reused, Snowflake resets the 24-hour retention period for the result, up to a maximum of 31 days from the date and time that the query was first executed. After 31 days, the result is purged and the next time the query is submitted, a new result is generated and persisted.\n"
    },
    "69": {
        "q": "Question 35\n\nWhich query profile statistics help determine if efficient pruning is occurring",
        "opt": {
            "0": "Bytes spilled to remote disk",
            "1": "Bytes spilled to local disk",
            "2": "Partitions scanned",
            "3": "Bytes over the network",
            "4": "Partitions total"
        },
        "ans": [
            2,
            4
        ],
        "exp": "\n\nQuery Profile Statistics\n\nA major source of information provided in the detail pane is the various statistics, grouped in the following sections:\n\nIO \u2014 information about the input-output operations performed during the query:\n\nScan progress \u2014 the percentage of data scanned for a given table so far.\n\nBytes scanned \u2014 the number of bytes scanned so far.\n\nPercentage scanned from cache \u2014 the percentage of data scanned from the local disk cache.\n\nBytes written \u2014 bytes written (e.g. when loading into a table).\n\nBytes written to result \u2014 bytes written to the result object. For example, select * from . . . would produce a set of results in tabular format representing each field in the selection. In general, the results object represents whatever is produced as a result of the query, and Bytes written to result represents the size of the returned result.\n\nBytes read from result \u2014 bytes read from the result object.\n\nExternal bytes scanned \u2014 bytes read from an external object, e.g. a stage.\n\nSpilling \u2014 information about disk usage for operations where intermediate results do not fit in memory:\n\nBytes spilled to local storage \u2014 volume of data spilled to local disk.\n\nBytes spilled to remote storage \u2014 volume of data spilled to remote disk.\n\nDML \u2014 statistics for Data Manipulation Language (DML) queries:\n\nNumber of rows inserted \u2014 number of rows inserted into a table (or tables).\n\nNumber of rows updated \u2014 number of rows updated in a table.\n\nNumber of rows deleted \u2014 number of rows deleted from a table.\n\nNumber of rows unloaded \u2014 number of rows unloaded during data export.\n\nPruning \u2014 information on the effects of table pruning:\n\nPartitions scanned \u2014 number of partitions scanned so far.\n\nPartitions total \u2014 total number of partitions in a given table.\n\nNetwork \u2014 network communication:\n\nBytes sent over the network \u2014 amount of data sent over the network.\n\nExternal Functions \u2014 information about calls to external functions:\n\nExtension Functions \u2014 information about calls to extension functions:\n"
    },
    "70": {
        "q": "Question 36\n\nA Snowflake query took 40 minutes to run. The results indicate that \u2018Bytes spilled to local storage\u2019 was a large number. What is the issue and how can it be resolved",
        "opt": {
            "0": "The warehouse is too large. Decrease the size of the warehouse to reduce spillage",
            "1": "The warehouse is too small. Increase the size of the warehouse to reduce spillage",
            "2": "The warehouse consist of a single cluster. Use a multi-cluster warehouse",
            "3": "The snowflake console has time out"
        },
        "ans": [
            1
        ],
        "exp": "\n\nData Warehouses - Disk Spilling\n\nWhen a warehouse cannot fit an operation in memory, it starts spilling (storing) data first to disk.\n\nIn such a case, Snowflake first tries to temporarily store the data on the warehouse local disk. As this means extra IO operations, any query that requires spilling will take longer than a similar query that is capable of fitting the operations in memory. Also, if the local disk is not sufficient to fit the spilled data, Snowflake further tries to write to the remote cloud storage, which will be shown in the query profile as \"Bytes spilled to remote storage\".\n"
    },
    "71": {
        "q": "Question 37\n\nWhat are the different caching mechanisms available in Snowflake",
        "opt": {
            "0": "Query result cache",
            "1": "Warehouse cache",
            "2": "Stage cache",
            "3": "Table cache",
            "4": "Metadata cache"
        },
        "ans": [
            0,
            1,
            4
        ],
        "exp": "\n\nSnowflake Caching Architecture\n\nSnowflake\u2018s caching infrastructure comprises three types of cache:\n\nMetadata Cache,\n\nQuery Result Cache, and\n\nWarehouse Cache.\n\n\n\nSnowflake Caching Layers\n\nThe Service Layer, which accepts SQL requests from users, coordinates queries, managing transactions and results. This layer holds the Result cache and Metadata cache.\n\nThe Compute Layer, where the actual SQL is executed across the nodes of a Warehouse. This layer holds a cache of data queried, referred as Warehouse cache or Local Disk I/O (although is implemented using SSD storage). All data in the compute layer is held as long as the warehouse is active.\n\nThe Storage Layer, that provides long term storage of results. This is often referred to as Remote Disk cache, and is currently implemented on either S3 or Microsoft Blob storage.\n"
    },
    "72": {
        "q": "Question 38\n\nA Data Engineer creates a view to fetch data which is aggregated on SALES_DATE and LOCATION, using the following command:\n\ncreate OR REPLACE VIEW VW_FETCHAGGREGATEDDATA (SALES_DATE, LOCATION, SUM_REVENUE) AS\nselect SALES_DATE, LOCATION, SUM(REVENUE) AS SUM_REVENUE\nFROM SALES\nGROUP BY SALES_DATE, LOCATION;\nThe view results are slow to process as millions of records are retrieved.\n\nHow could the Architect use cluster keys to address this latency",
        "opt": {
            "0": "Use the command alter VIEW VW_FETCHAGGREGATEDDATA CLUSTER BY (SALES_DATE,LOCATION);",
            "1": "Recreate this view to be a materialized view then execute the command",
            "2": "alter MATERIALIZED VIEW VW_FETCHAGGREGATEDDATA CLUSTER BY(SALES_DATE,LOCATION);",
            "3": "Use the command alter VIEW VW_FETCHAGGREGATEDDATA RECLUSTER;",
            "4": "Cluster the columns from this view that are most actively used in selective filters."
        },
        "ans": [
            1
        ],
        "exp": "\n\nMaterialized Views and Clustering\n\nDefining a clustering key on a materialized view is supported and can increase performance in many situations. However, it also adds costs.\n\nIf you cluster both the materialized view(s) and the base table on which the materialized view(s) are defined, you can cluster the materialized view(s) on different columns from the columns used to cluster the base table.\n\nIf you plan to create a table, load it, and create a clustered materialized view(s) on the table, then Snowflake recommends that you create the materialized views last (after loading as much data as possible). This can save money on the initial data load, because it avoids some extra effort to maintain the clustering of the materialized view the first time that the materialized view is loaded.\n\nNote that Standard views cannot be clustered.\n\n\n\nYou can add clustering or change the clustering key. For example, to cluster on installation_year:\n\nalter MATERIALIZED VIEW vulnerable_pipes CLUSTER BY (installation_year);"
    },
    "73": {
        "q": "Question 39\n\nA company that currently uses the Standard edition of Snowflake implements external tables to reference data in a cloud storage data lake. Users report that accessing the external tables is slow.\n\nHow can performance be improved",
        "opt": {
            "0": "Partition by an optimized folder structure on the external tables.",
            "1": "Refresh the external tables.",
            "2": "Add search optimization service to the external tables.",
            "3": "Implement materialized views over the external tables."
        },
        "ans": [
            0
        ],
        "exp": "\n\nExternal tables\n\nAn external table is a Snowflake feature that allows you to query data stored in an external stage as if the data were inside a table in Snowflake. The external stage is not part of Snowflake, so Snowflake does not store or manage the stage.\n\n\n\nExternal Table Performance\n\nQuerying data in an external table might be slower than querying data that you store natively in a table within Snowflake. To improve query performance, you can:\n\nUse a materialized view based on the external table.\n\nAdd partitioning to the external table.- query response time is faster when processing a small part of the data instead of scanning the entire data set.\n\n\n\nNote: In this ## questions, the Materialized View option is incorrect because Standard Edition do not support Materialized Views.\n"
    },
    "74": {
        "q": "Question 40\n\nWhat Query Profile statistic shows that the query is too large to fit in memory",
        "opt": {
            "0": "Bytes spilled to remote storage",
            "1": "Bytes spilled to local storage",
            "2": "Bytes spilled to remote cache",
            "3": "Bytes spilled to local cache"
        },
        "ans": [
            0,
            1
        ],
        "exp": "\n\nQuery Profile Statistics\n\nA major source of information provided in the detail pane is the various statistics, grouped in the following sections:\n\nIO \u2014 information about the input-output operations performed during the query:\n\nScan progress \u2014 the percentage of data scanned for a given table so far.\n\nBytes scanned \u2014 the number of bytes scanned so far.\n\nPercentage scanned from cache \u2014 the percentage of data scanned from the local disk cache.\n\nBytes written \u2014 bytes written (e.g. when loading into a table).\n\nBytes written to result \u2014 bytes written to the result object. For example, select * from . . . would produce a set of results in tabular format representing each field in the selection. In general, the results object represents whatever is produced as a result of the query, and Bytes written to result represents the size of the returned result.\n\nBytes read from result \u2014 bytes read from the result object.\n\nExternal bytes scanned \u2014 bytes read from an external object, e.g. a stage.\n\nSpilling \u2014 information about disk usage for operations where intermediate results do not fit in memory:\n\nBytes spilled to local storage \u2014 volume of data spilled to local disk.\n\nBytes spilled to remote storage \u2014 volume of data spilled to remote disk.\n\nDML \u2014 statistics for Data Manipulation Language (DML) queries:\n\nNumber of rows inserted \u2014 number of rows inserted into a table (or tables).\n\nNumber of rows updated \u2014 number of rows updated in a table.\n\nNumber of rows deleted \u2014 number of rows deleted from a table.\n\nNumber of rows unloaded \u2014 number of rows unloaded during data export.\n\nPruning \u2014 information on the effects of table pruning:\n\nPartitions scanned \u2014 number of partitions scanned so far.\n\nPartitions total \u2014 total number of partitions in a given table.\n\nNetwork \u2014 network communication:\n\nBytes sent over the network \u2014 amount of data sent over the network.\n\nExternal Functions \u2014 information about calls to external functions:\n\nExtension Functions \u2014 information about calls to extension functions:\n"
    },
    "75": {
        "q": "Question 1\n\nWhich statistics show if pruning is effective",
        "opt": {
            "0": "Partitions total",
            "1": "Pruning Percentage",
            "2": "Partitions scanned",
            "3": "Bytes spilled to local storage",
            "4": "Percentage scanned from cache"
        },
        "ans": [
            0,
            2
        ],
        "exp": "\n\nInefficient Pruning\n\nSnowflake collects statistics that helps to avoid unnecessary parts of a table based on the query filters. However, for this to have an effect, the data storage order needs to be correlated with the query filter attributes.\n\nThe efficiency of pruning can be observed in the TableScan operators by comparing:\n\n- Partitions scanned, and\n\n- Partitions total\n\n\n\nIf the former is a small fraction of the latter, pruning is efficient. If not, the pruning did not have an effect. Of course, pruning can only help for queries that actually filter out a significant amount of data. If the pruning statistics do not show data reduction, but there is a Filter operator above TableScan which filters out a number of records, this might signal that a different data organization might be beneficial for this query.\n"
    },
    "76": {
        "q": "Question 2\n\nAs a best practice, which tables should be selected for having a clustering key",
        "opt": {
            "0": "Multi TB range table",
            "1": "Multi KB range table",
            "2": "Multi MB range table",
            "3": "Multi GB range table"
        },
        "ans": [
            0
        ],
        "exp": "\n\nConsiderations for Choosing Clustering for a Table\n\nWhether you want faster response times or lower overall costs, clustering is best for a table that meets all of the following criteria:\n\nThe table contains a large number of micro-partitions. Typically, this means that the table contains multiple terabytes (TB) of data.\n\nThe queries can take advantage of clustering. Typically, this means that one or both of the following are true:\n\nThe queries are selective - they need to read only a small percentage of rows (and thus usually a small percentage of micro-partitions) in the table.\n\nThe queries sort the data \u2013 it contains an ORDER BY clause on the table.\n\nA high percentage of the queries can benefit from the same clustering key(s). In other words, many/most queries select on, or sort on, the same few column(s).\n"
    },
    "77": {
        "q": "Question 3\n\nTime travel is available for which table types",
        "opt": {
            "0": "Index Table",
            "1": "Temporary Table",
            "2": "External table",
            "3": "Permanent Table",
            "4": "Transient Table"
        },
        "ans": [
            1,
            3,
            4
        ],
        "exp": "\n\nTime Travel\n\nSnowflake Time Travel enables accessing historical data (i.e. data that has been changed or deleted) at any point within a defined period. It serves as a powerful tool for performing the following tasks:\n\nRestoring data-related objects (tables, schemas, and databases) that might have been accidentally or intentionally deleted.\n\nDuplicating and backing up data from key points in the past.\n\nClone entire tables, schemas, and databases at or before specific points in the past.\n\nQuery data in the past that has since been updated or deleted.\n\nAnalyzing data usage/manipulation over specified periods of time.\n\n\n\nTime Travel Data Retention\n\nDefault Time Travel (for all version) is 1 day. Also the only retention period for Standard Edition.        Extended Time Travel (up to 90 days) for Permanent Tables requires Enterprise Edition.                      Transient and Temporary Tables can not extend Time Travel period beyond 1 day.\n\n\n"
    },
    "78": {
        "q": "Question 4\n\nWhich statement most accurately describes clustering in Snowflake",
        "opt": {
            "0": "Clustering represents the way data is grouped and stored within micro-partitions",
            "1": "The clustering key must be included in the COPY command when loading data into Snowflake",
            "2": "The database administrator must define the clustering methodology for each table",
            "3": "Clustering can be disable within a Snowflake account"
        },
        "ans": [
            0
        ],
        "exp": "\n\nMicro-partitions & Data Clustering\n\nThe Snowflake Data Platform implements a powerful and unique form of partitioning, called micro-partitioning, that delivers all the advantages of static partitioning without the known limitations, as well as providing additional significant benefits. Data Clustering represents the way data is grouped and stored within micro-partitions.\n\nAll data in Snowflake tables is automatically divided into micro-partitions, which are contiguous units of storage. The diagram illustrates a Snowflake table with four columns sorted by date:\n\nThe table consists of 24 rows stored across 4 micro-partitions, with the rows divided equally between each micro-partition. Within each micro-partition, the data is sorted and stored by column, which enables Snowflake to perform the following actions for queries on the table:\n\nFirst, prune (avoid) micro-partitions that are not needed for the query.\n\nThen, prune by column within the remaining micro-partitions.\n\n\n\nEach micro-partition contains between 50 MB and 500 MB of uncompressed data (note that the actual size in Snowflake is smaller because data is always stored compressed). Groups of rows in tables are mapped into individual micro-partitions, organized in a columnar fashion.\n"
    },
    "79": {
        "q": "Question 5\n\nWhat features does Snowflake Time Travel enable",
        "opt": {
            "0": "Conducting point-in-time analysis for reporting",
            "1": "Query data related objects that were created within the past 365 days",
            "2": "Restoring data related objects that have been deleted within the past 90 days",
            "3": "Analyzing data usage/manipulation over all periods of time."
        },
        "ans": [
            2
        ],
        "exp": "\n\nTime Travel\n\nSnowflake Time Travel enables accessing historical data (i.e. data that has been changed or deleted) at any point within a defined period. It serves as a powerful tool for performing the following tasks:\n\nRestoring data-related objects (tables, schemas, and databases) that might have been accidentally or intentionally deleted.\n\nDuplicating and backing up data from key points in the past.\n\nClone entire tables, schemas, and databases at or before specific points in the past.\n\nQuery data in the past that has since been updated or deleted.\n\nAnalyzing data usage/manipulation over specified periods of time.\n\n\n\nTime Travel Data Retention\n\nDefault Time Travel (for all version) is 1 day. Also the only retention period for Standard Edition.        Extended Time Travel (up to 90 days) for Permanent Tables requires Enterprise Edition.                      Transient and Temporary Tables can not extend Time Travel period beyond 1 day.\n"
    },
    "80": {
        "q": "Question 6\n\nWhich output is provided by both the SYSTEM$CLUSTERING_DEPTH function and the SYSTEM$CLUSTERING_INFORMATION function",
        "opt": {
            "0": "average_depth",
            "1": "average_overlaps",
            "2": "total_partition_count",
            "3": "depth histogram"
        },
        "ans": [
            0
        ],
        "exp": "\n\nClustering Depth SQL System Functions\n\n\n\nSYSTEM$CLUSTERING_DEPTH\n\n\u2013 Computes the average depth of the table according to the specified columns (or the clustering key defined for the table). The average depth of a populated table is always 1 or more. The smaller the average depth, the better clustered the table is with regards to the specified columns.\n\nselect SYSTEM$CLUSTERING_DEPTH ( '<table_name>' ,\n'( <col1> [ , <col2> ... ] )' [ , '<predicate>' ] )\nWhere:\n\ncol1 [ , col2 ... ] - Column(s) in the table used to calculate the clustering depth:\n\n- For a table with no clustering key, this argument is required. If this argument is omitted, an error is returned.\n\n- For a table with a clustering key, this argument is optional; if the argument is omitted, Snowflake uses the defined clustering key to calculate the depth.\n\nPredicate - Clause that filters the range of values in the columns on which to calculate the clustering depth. Note that predicate does not utilize a WHERE keyword.\n\nNote: You can use the columns argument to calculate the depth for any columns in the table, regardless of the clustering key defined for the table.\n\n\n\nSYSTEM$CLUSTERING_INFORMATION\n\n\u2013 Returns clustering information, including average clustering depth, for a table based on one or more columns in the table. Its output includes a partition depth histogram.\n\nselect SYSTEM$CLUSTERING_INFORMATION( '<table_name>' [ ,\n{ '( <expr1> [ , <expr2> ... ] )' | <number_of_errors> } ] )\nWhere:\n\n(expr1 [ , expr2 ... ]) - Column names or expressions for which clustering information is returned:\n\n- For a table with no clustering key, this argument is required. If this argument is omitted, an error is returned.\n\n- For a table with a clustering key, this argument is optional; if the argument is omitted, Snowflake uses the defined clustering key to return clustering information.\n\nnumber_of_errors - Number of clustering errors returned by the function. If this argument is omitted, the 10 most recent errors are returned.\n\nNote: You can use the column argument to return clustering information for any columns in the table, regardless of whether a clustering key is defined for the table.\n\nIn other words, you can use this to help you decide what clustering to use in the future.\n\nNote that the second argument of the function specifies a column name/expression or a number of errors. You cannot include both arguments in a single function call.\n"
    },
    "81": {
        "q": "Question 7\n\nHow many clustering keys can we create for a Snowflake table",
        "opt": {
            "0": "Two",
            "1": "One",
            "2": "Unlimited"
        },
        "ans": [
            1
        ],
        "exp": "\n\nClustering Keys & Clustered Tables\n\nClustering is a feature that allows you to organize data in a table based on the values of one or more columns, to improve query performance by minimizing the amount of data that needs to be scanned. In general, Snowflake produces well-clustered data in tables; however, over time, particularly as DML occurs on very large tables (as defined by the amount of data in the table, not the number of rows), the data in some table rows might no longer cluster optimally on desired dimensions. The more frequent a table changes, the more expensive it will be to keep it well clustered.\n\nOne cluster key can be created per table, and the cluster key can consist in one column or several columns or expressions. Snowflake supports automating these tasks by designating one or more table columns/expressions as a clustering key for the table.\n\nA table or MV with a clustering key defined is considered to be clustered.\n"
    },
    "82": {
        "q": "Question 8\n\nWhat metadata is stored for rows in micro-partitions",
        "opt": {
            "0": "JSON values",
            "1": "Null values",
            "2": "Index values",
            "3": "Sorted values",
            "4": "Distinct values",
            "5": "Range of values"
        },
        "ans": [
            4,
            5
        ],
        "exp": "\n\nSnowflake stores metadata about all rows stored in a micro-partition, including:\n\nThe range of values for each of the columns in the micro-partition.\n\nThe number of distinct values.\n\nAdditional properties used for both optimization and efficient query processing.\n"
    },
    "83": {
        "q": "Question 9\n\nIt has been several weeks since a Snowflake deployment and a Data Engineer notes an abnormal increase in data volumes. There have been no additional data sources added, and there were no abnormal changes in loaded data volumes. What is the Data Engineer MOST likely to find when looking at the TABLE_STORAGE_METRICS_VIEW",
        "opt": {
            "0": "Snowflake compression is turned off for new database objects.",
            "1": "A create TABLE AS select (CTAS) statement is using a permanent table with Time Travel.",
            "2": "Verbose logging has been enabled during deployment.",
            "3": "There have been additional clones in the deployment."
        },
        "ans": [
            1
        ],
        "exp": "\n\nTemporary and transient tables costs\n\nTo help manage the storage costs associated with Time Travel and Fail-safe, Snowflake provides two table types, temporary and transient. Temporary and transient tables do not incur the same fees as permanent tables:\n\nTransient and temporary tables can have a Time Travel retention period of either 0 or 1 day.\n\nTransient and temporary tables have no Fail-safe period.\n\nTransient and temporary tables can, at most, incur a one day\u2019s worth of storage cost.\n\nTransient and temporary tables contribute to the storage charges that Snowflake bills your account until explicitly dropped. Data stored in these table types contributes to the overall storage charges Snowflake bills your account while they exist.\n\nTemporary tables are typically used for non-permanent session specific transitory data such as ETL or other session specific data. Temporary tables only exist for the lifetime or their associated session. On session end, temporary table data is purged and unrecoverable. Temporary tables are not accessible outside the specific session which created them.\n\nTransient tables exist until explicitly dropped and are available to all users with permission.\n\n\n\nUsing temporary and transient tables to manage storage costs\n\nWhen choosing whether to store data in permanent, temporary, or transient tables, consider the following:\n\nTemporary tables are dropped when the session in which they were created ends. Data stored in temporary tables is not recoverable after the table is dropped.\n\nHistorical data in transient tables cannot be recovered by Snowflake after the Time Travel retention period ends. Use transient tables only for data you can replicate or reproduce independently from Snowflake.\n\nLong-lived tables, such as fact tables, should always be defined as permanent to ensure they are fully protected by Fail-safe.\n\nShort-lived tables (i.e. <1 day), such as ETL work tables, can be defined as transient to eliminate Fail-safe costs.\n\nIf downtime and the time required to reload lost data are factors, permanent tables, even with their added Fail-safe costs, may offer a better overall solution than transient tables.\n\n\n\nIn this ## questions, the reason is the Time Travel storage costs related to recreate a permanent table. Clones do not incur storage costs and database objects are always compressed. Verbose logging has no relationship with storage metrics.\n"
    },
    "84": {
        "q": "Question 10\n\nYou frequently perform modification on the table customer. Which table function will return the average clustering depth for the customer_stores column for the \"Africa\" region",
        "opt": {
            "0": "select system$clustering_depth('Customers', 'Customer_stores', 'region = ''Africa''');",
            "1": "select system$clustering_information('Customers', 'Customer_stores') where region = 'Africa\u2019;",
            "2": "select system$clustering_information('Sales', 'sales_representative', 'region = ''Africa''');",
            "3": "select system$clustering_depth('Customers', 'Customer_stores') where region = 'Africa';"
        },
        "ans": [
            0
        ],
        "exp": "\n\nOnly clustering_depth has predicate filters, and predicate does not utilize a WHERE keyword.\n\n\n\nClustering Depth SQL System Functions\n\n\n\nSYSTEM$CLUSTERING_DEPTH\n\n\u2013 Computes the average depth of the table according to the specified columns (or the clustering key defined for the table). The average depth of a populated table is always 1 or more. The smaller the average depth, the better clustered the table is with regards to the specified columns.\n\nselect SYSTEM$CLUSTERING_DEPTH ( '<table_name>' ,\n'( <col1> [ , <col2> ... ] )' [ , '<predicate>' ] )\nWhere:\n\ncol1 [ , col2 ... ] - Column(s) in the table used to calculate the clustering depth:\n\n- For a table with no clustering key, this argument is required. If this argument is omitted, an error is returned.\n\n- For a table with a clustering key, this argument is optional; if the argument is omitted, Snowflake uses the defined clustering key to calculate the depth.\n\nPredicate - Clause that filters the range of values in the columns on which to calculate the clustering depth. Note that predicate does not utilize a WHERE keyword.\n\nNote: You can use the columns argument to calculate the depth for any columns in the table, regardless of the clustering key defined for the table.\n\n\n\nSYSTEM$CLUSTERING_INFORMATION\n\n\u2013 Returns clustering information, including average clustering depth, for a table based on one or more columns in the table. Its output includes a partition depth histogram.\n\nselect SYSTEM$CLUSTERING_INFORMATION( '<table_name>' [ ,\n{ '( <expr1> [ , <expr2> ... ] )' | <number_of_errors> } ] )\nWhere:\n\n(expr1 [ , expr2 ... ]) - Column names or expressions for which clustering information is returned:\n\n- For a table with no clustering key, this argument is required. If this argument is omitted, an error is returned.\n\n- For a table with a clustering key, this argument is optional; if the argument is omitted, Snowflake uses the defined clustering key to return clustering information.\n\nnumber_of_errors - Number of clustering errors returned by the function. If this argument is omitted, the 10 most recent errors are returned.\n\nNote: You can use the column argument to return clustering information for any columns in the table, regardless of whether a clustering key is defined for the table.\n\nIn other words, you can use this to help you decide what clustering to use in the future.\n\nNote that the second argument of the function specifies a column name/expression or a number of errors. You cannot include both arguments in a single function call.\n"
    },
    "85": {
        "q": "Question 11\n\nSnowflake will attempt to avoid any micro-partitions it knows don't contain relevant data to fulfill a query. What is the name of this technique",
        "opt": {
            "0": "Clustering",
            "1": "Micro-partitioning",
            "2": "Pruning",
            "3": "Indexing"
        },
        "ans": [
            2
        ],
        "exp": "\n\nQuery Pruning\n\nPruning is a technique employed to reduce the number of micro-partitions read when executing a query. Reading micro-partitions is one of the costliest steps in a query, as it involves reading data remotely over the network. If a filter is applied in a where clause, join, or subquery, Snowflake will attempt to avoid any micro-partitions it knows don\u2019t contain relevant data. For this to work, the micro-partitions have to contain a narrow range of values for the column you're filtering on.\n"
    },
    "86": {
        "q": "Question 12\n\nYou need to calculate the average clustering depth of a table. Which statements describe the characteristics of the SYSTEM$CLUSTERING_DEPTH function",
        "opt": {
            "0": "A clustering key isn't required.",
            "1": "The column(s) argument can be used to calculate the depth for any columns in the table, regardless of the clustering key defined for the table",
            "2": "The smaller the average depth of a table, the better clustered the table will be with regards to the specified columns",
            "3": "The average depth of a table, no matter if it is empty or contains data, is always 1 or more",
            "4": "A clustering key must be defined on the table, or an error will be returned",
            "5": "The bigger the average depth of a table, the better clustered the table will be with regards to the specified columns"
        },
        "ans": [
            1,
            2
        ],
        "exp": "\n\nSYSTEM$CLUSTERING_DEPTH\n\n\u2013 Computes the average depth of the table according to the specified columns (or the clustering key defined for the table). The average depth of a populated table is always 1 or more. The smaller the average depth, the better clustered the table is with regards to the specified columns.\n\nselect SYSTEM$CLUSTERING_DEPTH ( '<table_name>' ,\n'( <col1> [ , <col2> ... ] )' [ , '<predicate>' ] )\nWhere:\n\ncol1 [ , col2 ... ] - Column(s) in the table used to calculate the clustering depth:\n\n- For a table with no clustering key, this argument is required. If this argument is omitted, an error is returned.\n\n- For a table with a clustering key, this argument is optional; if the argument is omitted, Snowflake uses the defined clustering key to calculate the depth.\n\nPredicate - Clause that filters the range of values in the columns on which to calculate the clustering depth. Note that predicate does not utilize a WHERE keyword.\n\nNote: You can use the columns argument to calculate the depth for any columns in the table, regardless of the clustering key defined for the table.\n"
    },
    "87": {
        "q": "Question 13\n\nTo choose a cluster key, which of the below columns are recommended",
        "opt": {
            "0": "Columns with no cardinality",
            "1": "A VARIANT column",
            "2": "Columns often used in JOIN predicates",
            "3": "Columns often used in selective filters",
            "4": "Columns with very low cardinality",
            "5": "Columns with very high cardinality"
        },
        "ans": [
            2,
            3
        ],
        "exp": "\n\nStrategies for selecting Clustering Keys\n\nA single clustering key can contain one or more columns or expressions. Snowflake recommends a max of 3/4 columns (or expressions) per key. Adding more than 3-4 columns tends to increase costs more than benefits.\n\nselecting the right columns/expressions for a clustering key can dramatically impact query performance. Analysis of your workload will usually yield good clustering key candidates.\n\nSnowflake recommends prioritizing keys in the order below:\n\nCluster columns that are most actively used in selective filters. For many fact tables involved in date-based queries. If you typically filter queries by two dimensions (e.g. application_id and user_status columns), then clustering on both columns can improve performance. For event tables, event type might be a good choice, if there are a large number of different event types. (If your table has only a small number of different event types, then see the comments on cardinality below before choosing an event column as a clustering key.).\n\nConsider columns frequently used in join predicates if there is room for additional cluster keys. For example \u201cFROM table1 JOIN table2 ON table2.column_A = table1.column_B\u201d.\n"
    },
    "88": {
        "q": "Question 14\n\nA Data Engineer wants to create a new development database (DEV) as a clone of the permanent production database (PROD). There is a requirement to disable Fail-safe for all tables.\n\nWhich command will meet these requirements",
        "opt": {
            "0": "create DATABASE DEV CLONE PROD DATA_RETENTION_TIME_IN DAYS = 0;",
            "1": "create TRANSIENT DATABASE DEV CLONE PROD;",
            "2": "create DATABASE DEV CLONE PROD;",
            "3": "create DATABASE DEV CLONE PROD FAIL_SAFE=FALSE;"
        },
        "ans": [
            1
        ],
        "exp": "\n\nThere is no FAIL_SAFE=FALSE property. Simplifying Dev Environments with Fast Cloning\n\nSnowflake fast clone feature support promotion processes from development to integration/testing to production.\n\nDevelopment environments can be created the old way by creating a new database and then copying the existing tables and data over from production (using a \u201ccreate TABLE AS select \u2026\u201d or CTAS operation). If they are large tables that would take some time and cost extra as it also requires more space. However, using fast clone feature you can do it way faster and not incur the cost for extra space.\n\nOne command creates the new database, schema, and tables and gives you a logical copy of the production data to work with almost instantly.\n\ncreate [TRANSIENT] DATABASE Dev CLONE Prod; -- new dev database\ncreate [TRANSIENT] DATABASE Int CLONE Prod; -- new integration database\n\n\nNote: For development environment, consider the use of transient database, schemas or tables to avoid incur in extra costs related to Fail-safe and Time Travel beyond 24 hours.\n"
    },
    "89": {
        "q": "Question 15\n\nWhich system function should be used to show the partition depth histogram of a clustered table",
        "opt": {
            "0": "SYSTEM$CLUSTERING_DEPTH",
            "1": "SYSTEM$CLUSTERING_HISTOGRAM",
            "2": "SYSTEM$CLUSTERING_INFORMATION",
            "3": "SYSTEM$CLUSTERING_RATIO"
        },
        "ans": [
            2
        ],
        "exp": "\n\nSYSTEM$CLUSTERING_INFORMATION includes a partition depth histogram.\n"
    },
    "90": {
        "q": "Question 16\n\nWhat does the average_overlaps in the output of SYSTEM$CLUSTERING_INFORMATION refers",
        "opt": {
            "0": "The average number of micro-partitions in the table associated with cloned objects.",
            "1": "The average number of micro-partitions stored in time travel.",
            "2": "The average number of micro-partitions which contains overlapping value ranges.",
            "3": "The average number of micro-partitions physically stored in the same location."
        },
        "ans": [
            2
        ],
        "exp": "\n\nSYSTEM$CLUSTERING_INFORMATION function\n\nReturns clustering information, including average clustering depth, for a table based on one or more columns in the table. Its output includes a partition depth histogram.\n\n\n\nExample\n\nselect SYSTEM$CLUSTERING_INFORMATION ('test2', '(col1, col3)');\n\n+--------------------------------------------------------------------+\n| SYSTEM$CLUSTERING_INFORMATION('TEST2', '(COL1, COL3)')             |\n|--------------------------------------------------------------------|\n| {                                                                  |\n|   \"cluster_by_keys\" : \"LINEAR(COL1, COL3)\",                        |\n|   \"total_partition_count\" : 1156,                                  |\n|   \"total_constant_partition_count\" : 0,                            |\n|   \"average_overlaps\" : 117.5484,                                   |\n|   \"average_depth\" : 64.0701,                                       |\n|   \"partition_depth_histogram\" : {                                  |\n|     \"00000\" : 0,                                                   |\n|     \"00001\" : 0,                                                   |\n|     \"00002\" : 3,                                                   |\n|     \"00003\" : 3,                                                   |\n|     \"00004\" : 4,                                                   |\n|     \"00005\" : 6,                                                   |\n|     \"00006\" : 3,                                                   |\n|     \"00007\" : 5,                                                   |\n|     \"00008\" : 10,                                                  |\n|     \"00009\" : 5,                                                   |\n|     \"00010\" : 7,                                                   |\n|     \"00011\" : 6,                                                   |\n|     \"00012\" : 8,                                                   |\n|     \"00013\" : 8,                                                   |\n|     \"00014\" : 9,                                                   |\n|     \"00015\" : 8,                                                   |\n|     \"00016\" : 6,                                                   |\n|     \"00032\" : 98,                                                  |\n|     \"00064\" : 269,                                                 |\n|     \"00128\" : 698                                                  |\n|   },                                                               |\n|   \"clustering_errors\" : [ {                                        |\n|      \"timestamp\" : \"2023-04-03 17:50:42 +0000\",                    |\n|      \"error\" : \"(003325) Clustering service has been disabled.\\n\"  |\n|      }                                                             |\n|   ]  }                                                             |\n+--------------------------------------------------------------------+\n\n\nThis example indicates that the test2 table is not well-clustered for the following reasons:\n\nZero (0) constant micro-partitions out of 1156 total micro-partitions.\n\nHigh average of overlapping micro-partitions (overlapping value ranges).\n\nHigh average of overlap depth across micro-partitions.\n\nMost of the micro-partitions are grouped at the lower-end of the histogram, with the majority of micro-partitions having an overlap depth between 64 and 128.\n\nAutomatic clustering was previously disabled.\n\nResources"
    },
    "91": {
        "q": "Question 17\n\nClustering a table is effective in the following situations:",
        "opt": {
            "0": "Frequent DML occurs on the table.",
            "1": "The table is queried frequently and do not change often.",
            "2": "The table is queried and change frequently."
        },
        "ans": [
            1
        ]
    },
    "92": {
        "q": "Question 18\n\nWhat feature can be used to establish o reorganized the data distribution of a very large table ",
        "opt": {
            "0": "Clustering Keys",
            "1": "Clustered partitions",
            "2": "Micro-partitions",
            "3": "Key partitions"
        },
        "ans": [
            0
        ],
        "exp": "\n\nClustering Keys & Clustered Tables\n\nClustering is a feature that allows you to organize data in a table based on the values of one or more columns, to improve query performance by minimizing the amount of data that needs to be scanned. In general, Snowflake produces well-clustered data in tables; however, over time, particularly as DML occurs on very large tables (as defined by the amount of data in the table, not the number of rows), the data in some table rows might no longer cluster optimally on desired dimensions. The more frequent a table changes, the more expensive it will be to keep it well clustered.\n"
    },
    "93": {
        "q": "Question 19\n\nWhat would happen if we executed the following command\n\ncreate OR REPLACE clonedTable CLONE originalTable;",
        "opt": {
            "0": "cloned Table is created with all data from orginalTable",
            "1": "Snowflake creates a new entry in the metadata store to keep track of the clone. The existing micro-partitions of originalTable are mapped to the clonedTable table.",
            "2": "cloned Table is created, and Snowflake internally executes a copy of originalTable data."
        },
        "ans": [
            1
        ],
        "exp": "\n\nCloning Tables, Schemas, and Databases\n\nSnowflake\u2019s zero-copy cloning feature provides a convenient way to quickly take a \u201csnapshot\u201d of any table, schema, or database and create a derived copy of that object which initially shares the underlying storage. This can be extremely useful for creating instant backups that do not incur any additional costs (until changes are made to the cloned object).\n\nZero-Copy cloning does NOT duplicate data; it duplicates the metadata of the micro-partitions.\n\nHowever, cloning makes calculating total storage usage more complex because each clone has its own separate life-cycle. This means that changes can be made to the original object or the clone independently of each other and these changes are protected through CDP.\n\nFor example, when a clone is created of a table, the clone utilizes no data storage because it shares all the existing micro-partitions of the original table at the time it was cloned; however, rows can then be added, deleted, or updated in the clone independently from the original table. Each change to the clone results in new micro-partitions that are owned exclusively by the clone.\n\n\n\nFor databases and schemas, cloning is recursive:\n\nCloning a database clones all the schemas and other objects in the database.\n\nCloning a schema clones all the contained objects in the schema.\n\nHowever, the following object types are not cloned:\n\n- External tables - Internal (Snowflake) stages\n"
    },
    "94": {
        "q": "Question 20\n\nA Data Engineer has inherited a database and is monitoring a table with the below query.\n\nselect SYSTEM$CLUSTERING_INFORMATION( \u2018orders\u2019, \u2018(o_orderdate)\u2019);\n\n\n\nThe Engineer gets the first two results (e.g., Day 0 and Day 30).\n\n-- DAY 0 -------\n{\n\"cluster_by_keys\" : \"LINEAR(o_orderdate)\",\n\"total_partition_count\" : 3218,\n\"total_constant_partition_count\" : 0,\n\"average_overlaps\" : 20.4133,\n\"average_depth\" : 11.4326,\n\"partition_depth_histogram\" : {\n\"00000\" : 0,\n\"00001\" : 0,\n\"00002\" : 0,\n\"00003\" : 0,\n\"00004\" : 0,\n\"00005\" : 0,\n\"00006\" : 0,\n\"00007\" : 0,\n\"00008\" : 0,\n\"00009\" : 0,\n\"00010\" : 993,\n\"00011\" : 841,\n\"00012\" : 748,\n\"00013\" : 413,\n\"00014\" : 121,\n\"00015\" : 74,\n\"00016\" : 16,\n\"00032\" : 12\n}\n}\n-- DAY 30 -------\n{\n\"cluster_by_keys\" : \"LINEAR(o_orderdate)\",\n\"total_partition_count\" : 3240,\n\"total_constant_partition_count\" : 0,\n\"average_overlaps\" : 64.1185,\n\"average_depth\" : 33.4704,\n\"partition_depth_histogram\" : {\n\"00000\" : 0,\n\"00001\" : 0,\n\"00002\" : 0,\n\"00003\" : 0,\n\"00004\" : 0,\n\"00005\" : 0,\n\"00006\" : 0,\n\"00007\" : 0,\n\"00008\" : 0,\n\"00009\" : 0,\n\"00010\" : 0,\n\"00011\" : 0,\n\"00012\" : 0,\n\"00013\" : 0,\n\"00014\" : 0,\n\"00015\" : 0,\n\"00016\" : 0,\n\"00032\" : 993,\n\"00064\" : 2247\n}\n}\nHow should the Engineer interpret these results",
        "opt": {
            "0": "The table was initially well organized for queries that range over column o_orderdate . Over time this organization has improved further.",
            "1": "The table was initially not organized for queries that range over column o_orderdate . Over time, this organization has changed.",
            "2": "The table is well organized for queries that range over column o_orderdate. Over time, this organization is degrading.",
            "3": "The table was initially poorly organized for queries that range over column o_orderdate . Over time, this organization has improved."
        },
        "ans": [
            2
        ],
        "exp": "\n\nYou can observe that (1) the average overlaps has increased from 20 to 64, (2) the average depth has increased from 11 to 33, and (3) most micro-partitions are grouped at the lower-end of the histogram - over a period of time the clustering is degrading.\n"
    },
    "95": {
        "q": "Question 21\n\nWhich statements are correct about micro-partitions",
        "opt": {
            "0": "Organized in a columnar way",
            "1": "50 and 500MB of uncompressed data",
            "2": "Non-contiguous units of storage",
            "3": "50 and 500MB of compressed data",
            "4": "Contiguous units of storage"
        },
        "ans": [
            0,
            1,
            4
        ],
        "exp": "\n\nMicro-partitions & Data Clustering\n\nThe Snowflake Data Platform implements a powerful and unique form of partitioning, called micro-partitioning, that delivers all the advantages of static partitioning without the known limitations, as well as providing additional significant benefits. Data Clustering represents the way data is grouped and stored within micro-partitions.\n\nAll data in Snowflake tables is automatically divided into micro-partitions, which are contiguous units of storage. The diagram illustrates a Snowflake table with four columns sorted by date:\n\nThe table consists of 24 rows stored across 4 micro-partitions, with the rows divided equally between each micro-partition. Within each micro-partition, the data is sorted and stored by column, which enables Snowflake to perform the following actions for queries on the table:\n\nFirst, prune (avoid) micro-partitions that are not needed for the query.\n\nThen, prune by column within the remaining micro-partitions.\n\n\n\nEach micro-partition contains between 50 MB and 500 MB of uncompressed data (note that the actual size in Snowflake is smaller because data is always stored compressed). Groups of rows in tables are mapped into individual micro-partitions, organized in a columnar fashion.\n\n\n\nSnowflake stores metadata about all rows stored in a micro-partition, including:\n\nThe range of values for each of the columns in the micro-partition.\n\nThe number of distinct values.\n\nAdditional properties used for both optimization and efficient query processing.\n"
    },
    "96": {
        "q": "Question 22\n\nYou want to increase the retention period of a table from 10 days to a month. What will happen with the table's data after this increment",
        "opt": {
            "0": "Any data that is between 10 and 31 days older will now have time-travel extended for a month",
            "1": "Any data that has not reached the ten days time-travel period will now have time-travel extended for a month",
            "2": "Changes will impact only new data",
            "3": "Any data that is ten days older and move to fail safe will not have any impact"
        },
        "ans": [
            1,
            3
        ],
        "exp": "\n\nTime Travel\n\nSnowflake Time Travel enables accessing historical data (i.e. data that has been changed or deleted) at any point within a defined period. It serves as a powerful tool for performing the following tasks:\n\nRestoring data-related objects (tables, schemas, and databases) that might have been accidentally or intentionally deleted.\n\nDuplicating and backing up data from key points in the past.\n\nClone entire tables, schemas, and databases at or before specific points in the past.\n\nQuery data in the past that has since been updated or deleted.\n\nAnalyzing data usage/manipulation over specified periods of time.\n\n\n\nTime Travel Data Retention\n\nDefault Time Travel (for all version) is 1 day. Also the only retention period for Standard Edition.        Extended Time Travel (up to 90 days) for Permanent Tables requires Enterprise Edition.                      Transient and Temporary Tables can not extend Time Travel period beyond 1 day.\n\n\n\nIncreasing Data Retention\n\nWhen increasing the time-travel retention period, the new value impacts only:\n\nNew data, and\n\nData that hasn't reached the time-travel period.\n"
    },
    "97": {
        "q": "Question 23\n\nThe following table records orders from a food delivery service application and provides metrics to understand how deliveries in each region are performing over various time frames.\n\ncreate TABLE orders (\nid NUMBER,\ncustomer_id NUMBER,\nrestaurant_id NUMBER,\nordered_at TIMESTAMP,\nitem_count INTEGER,\norder_location_state CHAR(2);\n\nHow would you establish the MOST efficient clustering statement for this table",
        "opt": {
            "0": "alter table orders_dashboard cluster by (order_location_state, ordered_at);",
            "1": "alter table orders_dashboard cluster by (order_location_state, DATE(ordered_at));",
            "2": "alter table orders_dashboard cluster by (ordered_at, order_location_state);",
            "3": "alter table orders_dashboard cluster by (DATE(ordered_at), order_location_state);"
        },
        "ans": [
            1
        ],
        "exp": "\n\nMulti-column clustering key\n\nThe cluster key can consist in one column or several columns or expressions. If you are defining a multi-column clustering key for a table, the order in which the columns are specified in the CLUSTER BY clause is important. As a general rule, Snowflake recommends ordering the columns from lowest cardinality to highest cardinality. Putting a higher cardinality column before a lower cardinality column will generally reduce the effectiveness of clustering on the latter column.\n\nSo, in this example, the location (lower cardinality) should be placed before, and timestamps column should be cast to lower the cardinality.\n"
    },
    "98": {
        "q": "Question 24\n\nWhile clustering a table, columns with which data types can be used as clustering keys",
        "opt": {
            "0": "Strings",
            "1": "Binary",
            "2": "Geography",
            "3": "Variant",
            "4": "Object",
            "5": "Geometry"
        },
        "ans": [
            1,
            5
        ],
        "exp": "\n\nEach clustering key consists of one or more table columns/expressions, which can be of any data type, except GEOGRAPHY, VARIANT, OBJECT, or ARRAY.\n\nA clustering key can contain any of the following:\n\nBase columns.\n\nExpressions on base columns.\n\nExpressions on paths in VARIANT columns.\n"
    },
    "99": {
        "q": "Question 25\n\nWhich statement describes pruning",
        "opt": {
            "0": "The return of Micro-partitions values that overlap to each other to reduce query\u2019s run time",
            "1": "The ability to allow a result of a query to be accessed as if it were a table",
            "2": "The filtering of Micro-partitions that are not needed to return a query",
            "3": "A service that is handled by Snowflake to optimize caching"
        },
        "ans": [
            2
        ],
        "exp": "\n\nQuery Pruning\n\nPruning is a technique employed to reduce the number of micro-partitions read when executing a query. Reading micro-partitions is one of the costliest steps in a query, as it involves reading data remotely over the network. If a filter is applied in a where clause, join, or subquery, Snowflake will attempt to avoid any micro-partitions it knows don\u2019t contain relevant data. For this to work, the micro-partitions have to contain a narrow range of values for the column you're filtering on.\n"
    },
    "100": {
        "q": "Question 26\n\nGiven the following columns, which is the best choice for a clustering key",
        "opt": {
            "0": "Timestamp in a 20 GB table",
            "1": "Store_id in a 5 TB table",
            "2": "UUID column from a 10 TB table",
            "3": "Gender male/female in a 20 TB table"
        },
        "ans": [
            1
        ],
        "exp": "\n\nConsiderations for Choosing Clustering for a Table\n\nWhether you want faster response times or lower overall costs, clustering is best for a table that meets all of the following criteria:\n\nThe table contains a large number of micro-partitions. Typically, this means that the table contains multiple terabytes (TB) of data.\n\nThe queries can take advantage of clustering. Typically, this means that one or both of the following are true:\n\nThe queries are selective - they need to read only a small percentage of rows (and thus usually a small percentage of micro-partitions) in the table.\n\nThe queries sort the data \u2013 it contains an ORDER BY clause on the table.\n\nA high percentage of the queries can benefit from the same clustering key(s). In other words, many/most queries select on, or sort on, the same few column(s).\n\n\n\nIn this example, the BEST option is Store_id in a 5TB table. A very low cardinality (gender) might yield minimal pruning. A very high cardinality (UUID or timestamp) is not a good candidate to use directly.\n"
    },
    "101": {
        "q": "Question 27\n\nWhich of the following are benefits of micro-partitioning",
        "opt": {
            "0": "Rows are stored independently within micro-partitions",
            "1": "Micro-partitions are immutable objects that supports the use of Time Travel",
            "2": "Micro-partitions cannot overlap in their range of values",
            "3": "Micro partitions enables extremely efficient DML and fine-grained pruning for faster queries"
        },
        "ans": [
            1,
            3
        ],
        "exp": "\n\nBenefits of Micro-partitioning\n\nThe benefits of Snowflake\u2019s approach to partitioning table data include:\n\nSnowflake micro-partitions are immutable - once written cannot be modified. This means an update against rows creates a new version (using Time Travel) of the entire micro-partition with the updated rows.\n\nIn contrast to traditional static partitioning, Snowflake micro-partitions are derived automatically; they don\u2019t need to be explicitly defined up-front or maintained by users.\n\nAs the name suggests, micro-partitions are small in size (50-500 MB, before compression), which enables extremely efficient DML and fine-grained pruning for faster queries.\n\nMicro-partitions can overlap in their range of values, which, combined with their uniformly small size, helps prevent skew.\n\nColumns are stored independently within micro-partitions, often referred to as columnar storage. This enables efficient scanning of individual columns; only the columns referenced by a query are scanned.\n\nColumns are compressed individually within micro-partitions. Snowflake automatically determines the most efficient compression algorithm for the cols in each micro-partition.\n"
    },
    "102": {
        "q": "Question 28\n\nWhat is the clustering depth value of the newly created table with 0 records",
        "opt": {
            "0": "0",
            "1": "1",
            "2": "100"
        },
        "ans": [
            0
        ],
        "exp": "\n\nClustering Depth\n\nThe clustering depth for a populated table measures the average depth (1 or greater) of the overlapping micro-partitions for specified columns in a table. The smaller the average depth, the better clustered the table is with regards to the specified columns. A table with no micro-partitions (i.e. an unpopulated/empty table) has a clustering depth of 0.\n"
    },
    "103": {
        "q": "Question 29\n\nWhich of the following objects are NOT automatically cloned when a database containing these objects is cloned",
        "opt": {
            "0": "Streams",
            "1": "Internal Stages",
            "2": "Tasks",
            "3": "External Tables",
            "4": "Materialized Views",
            "5": "Pipes that reference an External stage"
        },
        "ans": [
            1,
            3
        ],
        "exp": "\n\nCloning Tables, Schemas, and Databases\n\nSnowflake\u2019s zero-copy cloning feature provides a convenient way to quickly take a \u201csnapshot\u201d of any table, schema, or database and create a derived copy of that object which initially shares the underlying storage. This can be extremely useful for creating instant backups that do not incur any additional costs (until changes are made to the cloned object).\n\nZero-Copy cloning does NOT duplicate data; it duplicates the metadata of the micro-partitions.\n\nHowever, cloning makes calculating total storage usage more complex because each clone has its own separate life-cycle. This means that changes can be made to the original object or the clone independently of each other and these changes are protected through CDP.\n\nFor example, when a clone is created of a table, the clone utilizes no data storage because it shares all the existing micro-partitions of the original table at the time it was cloned; however, rows can then be added, deleted, or updated in the clone independently from the original table. Each change to the clone results in new micro-partitions that are owned exclusively by the clone.\n\n\n\nFor databases and schemas, cloning is recursive:\n\nCloning a database clones all the schemas and other objects in the database.\n\nCloning a schema clones all the contained objects in the schema.\n\nHowever, the following object types are not cloned:\n\n- External tables - Internal (Snowflake) stages\n"
    },
    "104": {
        "q": "Question 30\n\nWhat property is used to see if a table will benefit from explicitly defining a clustering key",
        "opt": {
            "0": "Clustering ratio",
            "1": "Number of micro-partitions",
            "2": "Clustering depth"
        },
        "ans": [
            2
        ],
        "exp": "\n\nClustering Depth\n\nThe clustering depth for a populated table measures the average depth (1 or greater) of the overlapping micro-partitions for specified columns in a table. The smaller the average depth, the better clustered the table is with regards to the specified columns. A table with no micro-partitions (i.e. an unpopulated/empty table) has a clustering depth of 0.\n"
    },
    "105": {
        "q": "Question 31\n\nWhich of the following Snowflake features provide continuous data protection automatically",
        "opt": {
            "0": "Row-access policies",
            "1": "Column-level access policies",
            "2": "Time Travel",
            "3": "Zero-copy clones",
            "4": "Incremental backups",
            "5": "Fail-safe"
        },
        "ans": [
            2,
            5
        ],
        "exp": "\n\nSnowflake provides powerful CDP features for ensuring the maintenance and availability of your historical data (i.e. data that has been changed or deleted):\n\nTime Travel: Querying, cloning, and restoring historical data in\n\ntables, schemas, and databases up to 90 days.\n\nFail-safe: Disaster recovery of historical data for 7 days (by Snowflake Support).\n"
    },
    "106": {
        "q": "Question 32\n\nWhich statements are NOT correct about micro-partitions",
        "opt": {
            "0": "Organized in a tabular way",
            "1": "Contiguous units of storage",
            "2": "Organized in a columnar way",
            "3": "Non-contiguous units of storage",
            "4": "50 and 500MB of compressed data."
        },
        "ans": [
            3,
            4
        ],
        "exp": "\n\nMicro-partitions & Data Clustering\n\nThe Snowflake Data Platform implements a powerful and unique form of partitioning, called micro-partitioning, that delivers all the advantages of static partitioning without the known limitations, as well as providing additional significant benefits. Data Clustering represents the way data is grouped and stored within micro-partitions.\n\nAll data in Snowflake tables is automatically divided into micro-partitions, which are contiguous units of storage. The diagram illustrates a Snowflake table with four columns sorted by date:\n\nThe table consists of 24 rows stored across 4 micro-partitions, with the rows divided equally between each micro-partition. Within each micro-partition, the data is sorted and stored by column, which enables Snowflake to perform the following actions for queries on the table:\n\nFirst, prune (avoid) micro-partitions that are not needed for the query.\n\nThen, prune by column within the remaining micro-partitions.\n\n\n\nEach micro-partition contains between 50 MB and 500 MB of uncompressed data (note that the actual size in Snowflake is smaller because data is always stored compressed). Groups of rows in tables are mapped into individual micro-partitions, organized in a columnar fashion.\n\n\n\nSnowflake stores metadata about all rows stored in a micro-partition, including:\n\nThe range of values for each of the columns in the micro-partition.\n\nThe number of distinct values.\n\nAdditional properties used for both optimization and efficient query processing.\n"
    },
    "107": {
        "q": "Question 1\n\nWhat is the purpose of the MINS_TO_BYPASS_MFA property",
        "opt": {
            "0": "To temporary disable MFA for a user",
            "1": "To disable MFA for a user",
            "2": "To activate MFA for a user"
        },
        "ans": [
            1
        ],
        "exp": "\n\nMulti-factor authentication (MFA)\n\nSnowflake supports multi-factor authentication (i.e. MFA) to provide increased login security for users connecting to Snowflake. MFA support is provided as an integrated Snowflake feature, powered by the Duo Security service, which is managed completely by Snowflake.\n\nSnowflake recommends that all users with the ACCOUNTADMIN role be required to use MFA.\n\nAny Snowflake user can self-enroll in MFA through the web interface.\n\n\n\nManaging MFA for an account and users\n\nAt the account level, MFA requires no management. It is automatically enabled for an account and available for all users to self-enroll. However, the account administrator (i.e. the user granted the ACCOUNTADMIN system role) may find the need to disable MFA for a user, either temporarily or permanently, for example if the user loses their phone or changes their phone number and cannot log in with MFA.\n\n\n\nThe account administrator can use the following properties for the alter USER command:\n\nMINS_TO_BYPASS_MFA.- Specifies the number of minutes to temporarily disable MFA for the user so that they can log in. After the time passes, MFA is enforced and the user cannot log in without the temporary token generated by the Duo Mobile application.\n\nDISABLE_MFA.- Disables MFA for the user, effectively canceling their enrollment. It may be necessary to refresh the browser to verify that the user is no longer enrolled in MFA. To use MFA again, the user must re-enroll.\n"
    },
    "108": {
        "q": "Question 2\n\nWhat role has the privileges to create and manage data shares and resource monitors, by default",
        "opt": {
            "0": "ACCOUNTADMIN",
            "1": "SECURITYADMIN",
            "2": "ORGADMIN"
        },
        "ans": [
            0
        ],
        "exp": "\n\nACCOUNTADMIN (aka Account Administrator)\n\nRole that encapsulates the SYSADMIN and SECURITYADMIN system-defined roles. It is the top-level role in the system and should be granted only to a limited number of users.\n\nThe ACCOUNTADMIN role has the privileges required to:\n\ncreate and modify users properties.\n\ncreate and manage data sharing.\n\ncreate resource monitors (an account admin can grant privileges to other roles to allow other users to view and modify resource monitors).\n"
    },
    "109": {
        "q": "Question 3\n\nA masking policy was created with the following syntax:\n\ncreate OR REPLACE masking policy mask1AS\n(val string) returns string ->\nCASE\nWHEN current_role() in ('DE')\nTHEN val ELSE '*********'\nEND;\nWhat is this policy doing",
        "opt": {
            "0": "The owner of the table can see the plain-text value as stored in the table",
            "1": "SECURITYADMIN role can see the plain-text value as stored in the table",
            "2": "Everyone except DE role can see the plain-text value as '*********'",
            "3": "Anyone with DE role can see the plain-text value as stored in the table"
        },
        "ans": [
            2,
            3
        ],
        "exp": "\n\nData Masking Policies\n\nHow does a masking policy work?\n\nMasking policies for Dynamic Data Masking and External Tokenization adopt the same structure and format with one notable exception: masking policies for External Tokenization require using Writing External Functions in the masking policy body.\n\nSnowflake supports creating masking policies using create MASKING POLICY. In this policy, users whose CURRENT_ROLE is DE can view the email address. If the email address also has a visibility column value of Public, then the email address is visible in the query result. Otherwise, Snowflake returns a query result with a fixed masked value for the ssn column.\n\n\n\n-- Dynamic Data Masking\ncreate MASKING POLICY employee_ssn_mask AS (val string) RETURNS string ->\nCASE\nWHEN CURRENT_ROLE() IN ('DE') THEN val\nELSE '******'\nEND;\n\n"
    },
    "110": {
        "q": "Question 4\n\nWhich SQL command can be used to verify the privileges that are granted to a role",
        "opt": {
            "0": "SHOW GRANTS ON ROLE",
            "1": "SHOW GRANTS TO ROLE",
            "2": "SHOW GRANTS FOR ROLE"
        },
        "ans": [
            1
        ],
        "exp": "\n\nSHOW GRANTS SQL Command Reference\n\n\n\nList all privileges granted to the analyst role:\n\nSHOW GRANTS TO ROLE analyst;\nList all roles and users who have been granted the analyst role:\n\nSHOW GRANTS OF ROLE analyst;\nList all privileges that have been granted on the sales database:\n\nSHOW GRANTS ON DATABASE sales;\nSHOW GRANTS ON SCHEMA sales.sales;\n\n"
    },
    "111": {
        "q": "Question 5\n\nWhich Snowflake feature allows a user to track sensitive data for compliance, discovery, protection, and resource usage",
        "opt": {
            "0": "Row Access Policies",
            "1": "Tags",
            "2": "External Tokenization",
            "3": "Column Access Policies"
        },
        "ans": [
            1
        ],
        "exp": "\n\nObject Tagging (requires Enterprise Edition or higher)\n\nTags enable tracking and monitoring sensitive data for compliance, discovery, protection, and resource usage through a centralized or decentralized data governance management approach.\n\nA tag is a schema-level object that can be assigned to one o several objects at the same time.\n\nA tag can be assigned an arbitrary string value upon assigning the tag to an object. Snowflake stores the tag and its string value as a key-value pair. The tag must be unique for your schema, and the tag value is always a string.\n"
    },
    "112": {
        "q": "Question 6\n\nAccording to Snowflake best practice recommendations, which role should be used to create databases",
        "opt": {
            "0": "USERADMIN",
            "1": "SYSADMIN",
            "2": "PUBLIC",
            "3": "SECURITYADMIN"
        },
        "ans": [
            1
        ],
        "exp": "\n\nSYSADMIN (aka System Administrator)\n\nRole that has privileges to create warehouses and databases (and other objects) in an account.\n\nSnow recommends creating a hierarchy of roles aligned with business functions in your organization and ultimately assigning these roles to the SYSADMIN role.\n\nIf, as recommended, you create a role hierarchy that assigns all custom roles to the SYSADMIN role, this role also has the ability to grant privileges on warehouses, databases, and other objects to other roles. Snowflake recommends this role for creating objects.\n"
    },
    "113": {
        "q": "Question 7\n\nWhich view will return users who have queried a table",
        "opt": {
            "0": "OBJECT_ACCESS",
            "1": "OBJECT_DEPENDENCIES",
            "2": "WAREHOUSE_EVENT_HISTORY",
            "3": "ACCESS_HISTORY"
        },
        "ans": [
            3
        ],
        "exp": "\n\nACCESS_HISTORY view (requires Enterprise Edition or higher)\n\nThis Account Usage view can be used to query the access history of Snowflake objects (e.g. table, view, column) within the last 365 days (1 year).\n\n\nTracking read and write operations\n\nThe ACCESS_HISTORY view contains these columns:\n\nquery_id | query_start_time | user_name | direct_objects_accessed | base_objects_accessed | objects_modified | object_modified_by_ddl | policies_referenced | parent_query_id | root_query_id\n\n\nRead operations are tracked through the first five columns, while the last column, objects_modified, specifies the data write information that involved Snowflake columns, tables, and stages.\n\n\n\nThe query in Snowflake and how the database objects were created determines the information Snowflake returns in the direct_objects_accessed, base_objects_accessed, and objects_modified columns. Similarly, if the query references an object that is protected (by a row access policy) or a column that is protected (by a masking policy), Snowflake records the policy information in the policies_referenced column.\n\n\nThe object_modified_by_ddl column records the DDL operation on a database, schema, table, view, and column. These operations also include statements that specify a row access policy on a table or view, a masking policy on a column, and tag updates (e.g. set a tag, change a tag value) on the object or column.\n\n\nThe parent_query_id and root_query_id columns record query IDs that correspond to:\n\nA query that performs a read or write operation on another object.\n\nA query that performs a read or write operation on an object that calls a stored procedure, including nested stored procedure calls.\n"
    },
    "114": {
        "q": "Question 8\n\nWhat are the two models that Snowflake combines as an approach to access control",
        "opt": {
            "0": "MAC & RBAC",
            "1": "DAC & RBAC",
            "2": "DAC & ABAC",
            "3": "MAC & ABAC"
        },
        "ans": [
            1
        ],
        "exp": "\n\nAccess control framework\n\nSnowflake\u2019s approach to access control combines aspects from both of the following models:\n\nDiscretionary Access Control (DAC): Each object has an owner, who can in turn grant access to that object.\n\nRole-based Access Control (RBAC): Access privileges are assigned to roles (not directly to users), which are in turn assigned to users.\n"
    },
    "115": {
        "q": "Question 9\n\nAs a BEST Practice, what Role should you use to create Users and Roles",
        "opt": {
            "0": "SYSADMIN",
            "1": "ACCOUNTADMIN",
            "2": "USERADMIN and optionally SECURITYADMIN",
            "3": "SECURITYADMIN"
        },
        "ans": [
            2
        ],
        "exp": "\n\nSECURITYADMIN (aka Security Administrator)\n\nRole that can manage any object grant globally, as well as create, monitor, and manage users and roles. More specifically, this role:\n\nIs granted the MANAGE GRANTS security privilege to be able to modify any grant, including revoking it, without having privileges on the object.\n\nInherits the privileges of the USERADMIN role via the system role hierarchy (i.e. USERADMIN role is granted to SECURITYADMIN).\n\nIs used to create and enforce Network Policies on a Snowflake Account.\n\nThe SECURITYADMIN role should be used only in exceptional circumstances to deploy or control access to roles. As this role has the powerful MANAGE GRANTS privilege, it can alter any grant on the entire system including granting ACCOUNTADMIN role to any user.\n\n\n\nUSERADMIN (aka User and Role Administrator)\n\nRole that is dedicated to user and role management only. More specifically, this role:\n\nIs granted the create USER and create ROLE security privileges.\n\nCan create users and roles in the account.\n\nunlike SECURITYADMIN, it can only affect the objects it has created.\n\nThis role can also manage users and roles that it owns. Only the role with the OWNERSHIP privilege on an object (i.e. user or role), or a higher role, can modify the object properties.\n\nBy default, when your account is provisioned, the first user is assigned the ACCOUNTADMIN role. This user should then create one or more additional users who are assigned the USERADMIN role. All remaining users should be created by the user(s) with the USERADMIN role or another role that is granted the global create USER privilege.\n"
    },
    "116": {
        "q": "Question 10\n\nWhat are the differences between the account_usage and information_schema views",
        "opt": {
            "0": "Account Usage views has 48h latency",
            "1": "Information Schema views has longer retention time for historical usage data.",
            "2": "Records for dropped objects are not included in Account Usage views",
            "3": "Account Usage views has longer retention time for historical usage data.",
            "4": "Records for dropped objects are included in each Account Usage view",
            "5": "Account Usage views has no latency"
        },
        "ans": [
            3,
            4,
            5
        ],
        "exp": "\n\nDifferences Between Account Usage and Information Schema\n\nThe Account Usage views and the Information Schema views (and table functions) utilize identical structures and naming conventions, but with some key differences:\n\n\n\nAccount Usage\n\nIncludes dropped objects\n\nLatency varies rom 45 minutes to 3 hours (varies by view)\n\nRetention of historical data = 1 Year\n\n\nInformation Schema\n\nDoes not include dropped objects\n\nNo latency\n\nRetention of historical data = From 7 days to 6 months (varies by view/table function)\n\n\n"
    },
    "117": {
        "q": "Question 11\n\nWhat is a best practice after creating a custom role",
        "opt": {
            "0": "Add _CUSTOM to all custom role names",
            "1": "Assign the custom role to USERADMIN",
            "2": "Assign the custom role to the SYSADMIN",
            "3": "create the custom role using the SYSADMIN"
        },
        "ans": [
            2
        ],
        "exp": "\n\nSYSADMIN (aka System Administrator)\n\nRole that has privileges to create warehouses and databases (and other objects) in an account.\n\nSnow recommends creating a hierarchy of roles aligned with business functions in your organization and ultimately assigning these roles to the SYSADMIN role.\n\nIf, as recommended, you create a role hierarchy that assigns all custom roles to the SYSADMIN role, this role also has the ability to grant privileges on warehouses, databases, and other objects to other roles. Snowflake recommends this role for creating objects.\n"
    },
    "118": {
        "q": "Question 12\n\nWhat is the relationship between a tag and a masking policy",
        "opt": {
            "0": "A tag can have multiple masking policies with varying data types",
            "1": "A tag can have only one masking policy for each data type",
            "2": "A tag can be dropped after a Masking Policy is assigned",
            "3": "A tag can have multiple masking policies for each data type"
        },
        "ans": [
            1
        ],
        "exp": "\n\nObject Tagging (requires Enterprise Edition or higher)\n\nTags enable tracking and monitoring sensitive data for compliance, discovery, protection, and resource usage through a centralized or decentralized data governance management approach.\n\nA single tag can be assigned to different object types at the same time (e.g. warehouse and table simultaneously). At the time of assignment, the tag string value can be duplicated or remain unique. For example, multiple tables can be assigned the cost_center tag and the tag can always have the string value be sales. alternatively, the string value could be different (e.g. engineering, marketing, finance). After defining the tags and assigning the tags to Snowflake objects, tags can be queried to monitor usage on the objects to facilitate data governance operations, such as monitoring, auditing, and reporting.\n\nBecause tags can be assigned to tables, views, and columns, setting a tag and then querying the tag enables the discovery of a multitude of database objects and columns that contain sensitive information. Upon discovery, you can apply row access policies, or using masking policies to determine whether the data is tokenized, fully masked, partially masked, or unmasked.\n\nThe tag can support one masking policy for each data type that Snowflake supports. To simplify the initial column data protection efforts, create a generic masking policy for each data type (e.g. STRING, NUMBER, TIMESTAMP_LTZ) that allows authorized roles to see the raw data and unauthorized roles to see a fixed masked value.\n"
    },
    "119": {
        "q": "Question 13\n\nWhich database objects can be shared with Secure Data Sharing",
        "opt": {
            "0": "Non-secure UDFs",
            "1": "External Functions",
            "2": "Standard Views",
            "3": "Secure Views",
            "4": "Secure UDFs"
        },
        "ans": [
            3,
            4
        ],
        "exp": "\n\nSecure Data Sharing\n\nSecure Data Sharing lets you share selected objects in a database in your account with other Snowflake accounts. All database objects shared between accounts are read-only. Snowflake enables the sharing of databases through shares, which are created by data providers and \u201cimported\u201d by data consumers. Note that shared objects can not be re-shared.\n\n\n\nHow does Secure Data Sharing work?\n\nWith Secure Data Sharing, no actual data is copied or transferred between accounts. All sharing uses Snowflake\u2019s services layer and metadata store. Shared data does not take up any storage in a consumer account and therefore does not contribute to the consumer\u2019s monthly data storage charges. The only charges to consumers are for the compute resources (i.e. virtual warehouses) used to query the shared data.\n\nBecause no data is copied or exchanged, Secure Data Sharing setup is quick and easy for providers and access to the shared data is near-instantaneous for consumers:\n\nThe provider creates a share of a database in their account and grants access to specific objects in the database. The provider can also share data from multiple databases, as long as these databases belong to the same account. One or more accounts are then added to the share, which can include your own accounts (if you have multiple Snowflake accounts).\n\nOn the consumer side, a read-only database is created from the share. Access to this database is configurable using the same, standard role-based access control that Snowflake provides for all objects in the system.\n\n\n\nShared databases Limitations\n\nShared databases are read-only. Users in a consumer account can view/query data, but cannot insert or update data, or create any objects in the database.\n\nThe following actions are not supported:\n\nCreating a clone of a shared database or any schemas/tables in the database.\n\nTime Travel for a shared database or any schemas/tables in the database.\n\nEditing the comments for a shared database.\n\nShared databases and all the objects in the database cannot be re-shared with other accounts.\n\nShared databases cannot be replicated.\n\n\n\nGeneral limitations for shared databases\n\nYou can share the following Snowflake database objects:\n\nDatabases\n\nTables\n\nDynamic tables\n\nExternal tables\n\nIceberg Tables\n\nSecure views\n\nSecure materialized views\n\nSecure UDFs\n"
    },
    "120": {
        "q": "Question 14\n\nAs the Data Engineer for a new project, you have been asked to share unstructured data using secure views. Which URL types can be used",
        "opt": {
            "0": "Non-secure HTTP URL",
            "1": "File URL",
            "2": "HTTPS URL",
            "3": "Scoped URL",
            "4": "Pre-signed URL"
        },
        "ans": [
            3,
            4
        ],
        "exp": "\n\nSharing unstructured data with Secure Views\n\nData providers can share selected unstructured data to consumers using a secure view (create SECURE VIEW). A view allows the result of a query to be accessed like a table, and a secure view is specifically designated for data privacy.\n\nFrom the secure view, you can allow data consumers to retrieve either:\n\nScoped URL, or\n\nPre-signed URLs.\n\nScoped URLs provide better security, while pre-signed URLs can be accessed without authorization or authentication.\n\nYou can call the BUILD_SCOPED_FILE_URL function to create a secure view with the scoped URLs for a set of staged files, or the GET_PRESIGNED_URL function to retrieve the pre-signed URLs for a set of staged files.\n"
    },
    "121": {
        "q": "Question 15\n\nYou need to share unstructured data through a secure view. Which functions can be used",
        "opt": {
            "0": "BUILD_FILE_URL",
            "1": "BUILD_STAGE_FILE_URL",
            "2": "GET_RELATIVE_PATH",
            "3": "GET_PRESIGNED_URL",
            "4": "BUILD_SCOPED_FILE_URL"
        },
        "ans": [
            3,
            4
        ],
        "exp": "\n\nSharing unstructured data with Secure Views\n\nData providers can share selected unstructured data to consumers using a secure view (create SECURE VIEW). A view allows the result of a query to be accessed like a table, and a secure view is specifically designated for data privacy.\n\nFrom the secure view, you can allow data consumers to retrieve either:\n\nScoped URL, or\n\nPre-signed URLs.\n\nScoped URLs provide better security, while pre-signed URLs can be accessed without authorization or authentication.\n\nYou can call the BUILD_SCOPED_FILE_URL function to create a secure view with the scoped URLs for a set of staged files, or the GET_PRESIGNED_URL function to retrieve the pre-signed URLs for a set of staged files.\n"
    },
    "122": {
        "q": "Question 16\n\nWhy should a Snowflake user implement a secure view",
        "opt": {
            "0": "To show the view definition",
            "1": "To enforce MANAGED ACCESS schemas",
            "2": "To hide the view definition",
            "3": "To encrypt data in transit",
            "4": "To limit access to sensitive data"
        },
        "ans": [
            2,
            4
        ],
        "exp": "\n\nSecure Views\n\nFor security or privacy reasons, you might not wish to expose the underlying tables or internal structural details for a view. With secure views, the view definition and details are visible only to authorized users (i.e. users who are granted the role that owns the view).\n\nFor a non-secure view, internal optimizations can indirectly expose data.\n\nSome of the internal optimizations for views require access to the underlying data in the base tables for the view. This access might allow data that is hidden from users of the view to be exposed through user code, such as UDFs, or other programmatic methods. Secure views do not utilize these optimizations, ensuring that users have no access to the underlying data.\n\n\n\nFor a non-secure view, the view definition is visible to other users.\n\nBy default, the query expression used to create a standard view, also known as the view definition or text, is visible to users in various commands and interfaces.\n\n\n\nNote: When deciding whether to use a secure view, you should consider the purpose of the view and weigh the trade-off between data privacy/security and query performance.\n\n\n"
    },
    "123": {
        "q": "Question 17\n\nWhich command will you run to list all users and roles to which a role has been granted",
        "opt": {
            "0": "SHOW GRANTS TO ROLE",
            "1": "SHOW USER OF ROLE",
            "2": "SHOW GRANTS IN ROLE",
            "3": "SHOW GRANTS OF ROLE"
        },
        "ans": [
            3
        ],
        "exp": "\n\nSHOW GRANTS SQL Command Reference\n\n\n\nList all privileges granted to the analyst role:\n\nSHOW GRANTS TO ROLE analyst;\nList all roles and users who have been granted the analyst role:\n\nSHOW GRANTS OF ROLE analyst;\nList all privileges that have been granted on the sales database:\n\nSHOW GRANTS ON DATABASE sales;\nSHOW GRANTS ON SCHEMA sales.sales;\n\n"
    },
    "124": {
        "q": "Question 18\n\nWhich feature is integrated to support Multi-Factor Authentication (MFA) at Snowflake",
        "opt": {
            "0": "Privacy Enhanced Mail (PEM)",
            "1": "Duo Security",
            "2": "SCIM"
        },
        "ans": [
            1
        ],
        "exp": "\n\nMulti-factor authentication (MFA)\n\nSnowflake supports multi-factor authentication (i.e. MFA) to provide increased login security for users connecting to Snowflake. MFA support is provided as an integrated Snowflake feature, powered by the Duo Security service, which is managed completely by Snowflake.\n\nSnowflake recommends that all users with the ACCOUNTADMIN role be required to use MFA.\n\nAny Snowflake user can self-enroll in MFA through the web interface.\n\n\n\nManaging MFA for an account and users\n\nAt the account level, MFA requires no management. It is automatically enabled for an account and available for all users to self-enroll. However, the account administrator (i.e. the user granted the ACCOUNTADMIN system role) may find the need to disable MFA for a user, either temporarily or permanently, for example if the user loses their phone or changes their phone number and cannot log in with MFA.\n\n\n\nThe account administrator can use the following properties for the alter USER command:\n\nMINS_TO_BYPASS_MFA.- Specifies the number of minutes to temporarily disable MFA for the user so that they can log in. After the time passes, MFA is enforced and the user cannot log in without the temporary token generated by the Duo Mobile application.\n\nDISABLE_MFA.- Disables MFA for the user, effectively canceling their enrollment. It may be necessary to refresh the browser to verify that the user is no longer enrolled in MFA. To use MFA again, the user must re-enroll.\n"
    },
    "125": {
        "q": "Question 19\n\nHow can you allow a user to have only OWNERSHIP privilege on a table and not manage the GRANTS for that table",
        "opt": {
            "0": "Creating a \"managed\" schema",
            "1": "Creating a \"governed\" schema",
            "2": "By default object owner would not be able to manage the privilege on the object",
            "3": "Creating a \"regular\" schema"
        },
        "ans": [
            0
        ],
        "exp": "\n\nManaged Access Schemas\n\nWith managed access schemas, object owners lose the ability to make grant decisions. Only the schema owner (i.e. the role with the OWNERSHIP privilege on the schema) or a role with the MANAGE GRANTS privilege (SECURITYADMIN or higher) can grant privileges on objects in the schema, including future grants, centralizing privilege management.\n"
    },
    "126": {
        "q": "Question 20\n\nHow can data sharing be used across regions and cloud platforms",
        "opt": {
            "0": "Data cannot be cloned across regions and a share can be created on the clone",
            "1": "Data can be shared across regions using database replication, but not across different cloud providers",
            "2": "A share cannot be created on a replica, so sharing across regions and cloud platforms is not possible",
            "3": "Data can be cloned across regions and a share can be created on the clone",
            "4": "Data can be replicated to a different cloud provider and a share can be created on the replica",
            "5": "Data can be replicated to a different region and a share can be created on that replica"
        },
        "ans": [
            4,
            5
        ],
        "exp": "\n\nSharing data securely across regions and cloud platforms\n\nCross-region data sharing utilizes Snowflake data replication functionality.\n\n\nSnowflake data providers can share data with data consumers in a different region in a few steps.\n\nStep 1: Set up data replication\n\nEnable replication for your accounts. A user with the ORGADMIN role must enable replication for the source account that contains the data to share and the target accounts in regions where you want to share data with consumers.\n\ncreate a replication group and add databases and shares.\n\nReplicate the group with the databases and shares to the regions where you want to share data with consumers.\n\nStep 2: Share data with data consumers\n\nSharing data with data consumers in the same region involves adding one or more consumer accounts to the secondary shares that you replicated from the source account\n"
    },
    "127": {
        "q": "Question 21\n\nFrom the following list, which objects can be shared using a Secure Data Sharing",
        "opt": {
            "0": "Temporary tables",
            "1": "Transient Tables",
            "2": "Secure Views",
            "3": "Standard Views",
            "4": "UDFs",
            "5": "Permanent Tables",
            "6": "External Tables"
        },
        "ans": [
            2,
            6
        ],
        "exp": "\n\nSecure Data Sharing\n\nSecure Data Sharing lets you share selected objects in a database in your account with other Snowflake accounts. All database objects shared between accounts are read-only. Snowflake enables the sharing of databases through shares, which are created by data providers and \u201cimported\u201d by data consumers. Note that shared objects can not be re-shared.\n\n\n\nHow does Secure Data Sharing work?\n\nWith Secure Data Sharing, no actual data is copied or transferred between accounts. All sharing uses Snowflake\u2019s services layer and metadata store. Shared data does not take up any storage in a consumer account and therefore does not contribute to the consumer\u2019s monthly data storage charges. The only charges to consumers are for the compute resources (i.e. virtual warehouses) used to query the shared data.\n\nBecause no data is copied or exchanged, Secure Data Sharing setup is quick and easy for providers and access to the shared data is near-instantaneous for consumers:\n\nThe provider creates a share of a database in their account and grants access to specific objects in the database. The provider can also share data from multiple databases, as long as these databases belong to the same account. One or more accounts are then added to the share, which can include your own accounts (if you have multiple Snowflake accounts).\n\nOn the consumer side, a read-only database is created from the share. Access to this database is configurable using the same, standard role-based access control that Snowflake provides for all objects in the system.\n\n\n\nShared databases Limitations\n\nShared databases are read-only. Users in a consumer account can view/query data, but cannot insert or update data, or create any objects in the database.\n\nThe following actions are not supported:\n\nCreating a clone of a shared database or any schemas/tables in the database.\n\nTime Travel for a shared database or any schemas/tables in the database.\n\nEditing the comments for a shared database.\n\nShared databases and all the objects in the database cannot be re-shared with other accounts.\n\nShared databases cannot be replicated.\n\n\n\nGeneral limitations for shared databases\n\nYou can share the following Snowflake database objects:\n\nDatabases\n\nTables\n\nDynamic tables\n\nExternal tables\n\nIceberg Tables\n\nSecure views\n\nSecure materialized views\n\nSecure UDFs\n"
    },
    "128": {
        "q": "Question 22\n\nAs a Data Governance policy, you need to give others the ability to create their own tables but should not have permission to grant access to those tables. How can you do it",
        "opt": {
            "0": "create a new schema and grant create TABLE in the schema",
            "1": "create a schema with MANAGED ACCESS and grant create TABLE in the schema",
            "2": "create a schema with MANAGED ACCESS and grant OWNERSHIP of the schema",
            "3": "Grant the create SCHEMA privilege, let users create the schema, and then remove manage grants from the new schema"
        },
        "ans": [
            1
        ],
        "exp": "\n\nManaged Access Schemas\n\nWith managed access schemas, object owners lose the ability to make grant decisions. Only the schema owner (i.e. the role with the OWNERSHIP privilege on the schema) or a role with the MANAGE GRANTS privilege (SECURITYADMIN or higher) can grant privileges on objects in the schema, including future grants, centralizing privilege management.\n"
    },
    "129": {
        "q": "Question 23\n\nAs a Data Engineer, you need to limit some users from seeing the full value of PII columns. What feature supports this",
        "opt": {
            "0": "Row access policies",
            "1": "Data Masking policies",
            "2": "Role Based Access Control",
            "3": "Data Encryption"
        },
        "ans": [
            1
        ],
        "exp": "\n\nData Masking Policies\n\nSnowflake supports masking policies as a schema-level object to protect sensitive data (PII) from unauthorized access while allowing authorized users to access sensitive data at query runtime - when users execute a query in which a masking policy applies, the masking policy conditions determine whether unauthorized users see masked, partially masked, obfuscated, or tokenized data. Masking policies as a schema-level object also provide flexibility in choosing a centralized, decentralized, or hybrid management approach.\n"
    },
    "130": {
        "q": "Question 24\n\nA secure view is exposed only to users with what privilege",
        "opt": {
            "0": "Manage",
            "1": "Ownership",
            "2": "Reference",
            "3": "Usage"
        },
        "ans": [
            1
        ],
        "exp": "\n\nSecure Views\n\nFor security or privacy reasons, you might not wish to expose the underlying tables or internal structural details for a view. With secure views, the view definition and details are visible only to authorized users (i.e. users who are granted the role that owns the view).\n"
    },
    "131": {
        "q": "Question 25\n\nSnowflake strongly recommends that all users with what type of role be required to use Multi-Factor Authentication (MFA)",
        "opt": {
            "0": "SECURITYADMIN",
            "1": "ORGADMIN",
            "2": "ACCOUNTADMIN"
        },
        "ans": [
            2
        ],
        "exp": "\n\nACCOUNTADMIN (aka Account Administrator)\n\nRole that encapsulates the SYSADMIN and SECURITYADMIN system-defined roles. It is the top-level role in the system and should be granted only to a limited number of users.\n\nThe ACCOUNTADMIN role has the privileges required to:\n\ncreate and modify users properties.\n\ncreate and manage data sharing.\n\ncreate resource monitors (an account admin can grant privileges to other roles to allow other users to view and modify resource monitors).\n\nSnowflake strongly recommend the following when assigning the ACCOUNTADMIN role to users:\n\nAssign this role only to a select/limited number of people in your organization.\n\nAssign this role to at least two users.\n\nAll users with the ACCOUNTADMIN role should have MFA for login.\n"
    },
    "132": {
        "q": "Question 26\n\nWhich of these are considered best practices for Data Engineers",
        "opt": {
            "0": "Always explore your source data files to understand them",
            "1": "Use ACCOUNTADMIN for almost nothing",
            "2": "Use SYSADMIN for nearly everything",
            "3": "All of the above"
        },
        "ans": [
            0
        ],
        "exp": "\n\nThis QA is taken from a Data Engineer Hands-on Lab, and I got a similar QA on the exam. The answer is \"All of the above\". Data Engineers will use a role like SYSADMIN for most tasks. SYSADMIN is a creative role, and Data Engineers are creative creators. The SYSADMIN role should own all the objects, like databases or schemas. The ACCOUNTADMIN role should not be used to create objects apart from only being assigned to a limited number of people in your Snowflake account. Also, it's not a good idea to use it to create automated scripts, as the ACCOUNTADMIN role should be limited to performing the initial setup in the system and managing account-level objects.\n"
    },
    "133": {
        "q": "Question 27\n\nWhich view in SNOWFLAKE.ACCOUNT_USAGE shows from which IP address a user connected to Snowflake",
        "opt": {
            "0": "ACCESS_HISTORY",
            "1": "QUERY_HISTORY",
            "2": "SESSIONS",
            "3": "LOGIN_HISTORY"
        },
        "ans": [
            3
        ],
        "exp": "\n\nLOGIN_HISTORY view\n\nThis ACCOUNT_USAGE schema view can be used to query login attempts by Snowflake users within the last 365 days (1 year).\n\nDisplays information for each login attempt such as, IP address, error codes and event_timestap.\n"
    },
    "134": {
        "q": "Question 28\n\nWhich of the following options are correct regarding SECURITYADMIN and USERADMIN ",
        "opt": {
            "0": "The USERADMIN role includes the privileges to create and manage users and roles, and it\u2019s a child of the SECURITYADMIN role",
            "1": "The SECURITYADMIN role includes the global MANAGE GRANTS privilege to grant or revoke privileges on objects in the account",
            "2": "The USERADMIN role includes the global MANAGE GRANTS privilege to grant or revoke privileges on objects in the account",
            "3": "The SECURITYADMIN role includes the privileges to create and manage users and roles, and it\u2019s a child of the USERADMIN role"
        },
        "ans": [
            0,
            1
        ],
        "exp": "\n\nSECURITYADMIN (aka Security Administrator)\n\nRole that can manage any object grant globally, as well as create, monitor, and manage users and roles. More specifically, this role:\n\nIs granted the MANAGE GRANTS security privilege to be able to modify any grant, including revoking it, without having privileges on the object.\n\nInherits the privileges of the USERADMIN role via the system role hierarchy (i.e. USERADMIN role is granted to SECURITYADMIN).\n\nIs used to create and enforce Network Policies on a Snowflake Account.\n\nThe SECURITYADMIN role should be used only in exceptional circumstances to deploy or control access to roles. As this role has the powerful MANAGE GRANTS privilege, it can alter any grant on the entire system including granting ACCOUNTADMIN role to any user.\n\n\n\nUSERADMIN (aka User and Role Administrator)\n\nRole that is dedicated to user and role management only. More specifically, this role:\n\nIs granted the create USER and create ROLE security privileges.\n\nCan create users and roles in the account.\n\nunlike SECURITYADMIN, it can only affect the objects it has created.\n\nThis role can also manage users and roles that it owns. Only the role with the OWNERSHIP privilege on an object (i.e. user or role), or a higher role, can modify the object properties.\n\nBy default, when your account is provisioned, the first user is assigned the ACCOUNTADMIN role. This user should then create one or more additional users who are assigned the USERADMIN role. All remaining users should be created by the user(s) with the USERADMIN role or another role that is granted the global create USER privilege.\n\n\n"
    },
    "135": {
        "q": "Question 29\n\nWhat role in Snowflake separates the management of users and roles from the management of all grants",
        "opt": {
            "0": "ACCOUNTADMIN",
            "1": "SECURITYADMIN",
            "2": "USERADMIN",
            "3": "SYSADMIN"
        },
        "ans": [
            2
        ],
        "exp": "\n\nSystem-defined roles\n\nThere are a small number of system-defined roles in a Snowflake account. System-defined roles cannot be dropped. In addition, the privileges granted to these roles by Snowflake cannot be revoked.\n\n\n\nORGADMIN (aka Organization Administrator)\n\nRole that manages operations at the organization level. This role can:\n\ncreate accounts in the organization.\n\nView all accounts in the organization (using SHOW ORGANIZATION ACCOUNTS).\n\nView all regions enabled for the organization (using SHOW REGIONS).\n\nView usage information across the organization (use ORGANIZATION_USAGE schema).\n\nEnable database replication for an account in the organization.\n\nNote: Once an account is created, ORGADMIN can view the account properties but does not have access to the account data.\n\n\n\nACCOUNTADMIN (aka Account Administrator)\n\nRole that encapsulates the SYSADMIN and SECURITYADMIN system-defined roles. It is the top-level role in the system and should be granted only to a limited number of users.\n\nThe ACCOUNTADMIN role has the privileges required to:\n\ncreate and modify users properties.\n\ncreate and manage data sharing.\n\ncreate resource monitors (an account admin can grant privileges to other roles to allow other users to view and modify resource monitors).\n\n\n\nSECURITYADMIN (aka Security Administrator)\n\nRole that can manage any object grant globally, as well as create, monitor, and manage users and roles. More specifically, this role:\n\nIs granted the MANAGE GRANTS security privilege to be able to modify any grant, including revoking it, without having privileges on the object.\n\nInherits the privileges of the USERADMIN role via the system role hierarchy (i.e. USERADMIN role is granted to SECURITYADMIN).\n\nIs used to create and enforce Network Policies on a Snowflake Account.\n\nThe SECURITYADMIN role should be used only in exceptional circumstances to deploy or control access to roles. As this role has the powerful MANAGE GRANTS privilege, it can alter any grant on the entire system including granting ACCOUNTADMIN role to any user.\n\n\n\n\n\nUSERADMIN (aka User and Role Administrator)\n\nRole that is dedicated to user and role management only. More specifically, this role:\n\nIs granted the create USER and create ROLE security privileges.\n\nCan create users and roles in the account.\n\nunlike SECURITYADMIN, it can only affect the objects it has created.\n\nThis role can also manage users and roles that it owns. Only the role with the OWNERSHIP privilege on an object (i.e. user or role), or a higher role, can modify the object properties.\n\nBy default, when your account is provisioned, the first user is assigned the ACCOUNTADMIN role. This user should then create one or more additional users who are assigned the USERADMIN role. All remaining users should be created by the user(s) with the USERADMIN role or another role that is granted the global create USER privilege.\n\n\n\n\n\nSYSADMIN (aka System Administrator)\n\nRole that has privileges to create warehouses and databases (and other objects) in an account.\n\nSnow recommends creating a hierarchy of roles aligned with business functions in your organization and ultimately assigning these roles to the SYSADMIN role.\n\nIf, as recommended, you create a role hierarchy that assigns all custom roles to the SYSADMIN role, this role also has the ability to grant privileges on warehouses, databases, and other objects to other roles. Snowflake recommends this role for creating objects.\n\n\n\n\n\nResources"
    },
    "136": {
        "q": "Question 30\n\nWhat privileges can be granted to a role to specify which operations that role can perform on a virtual warehouse",
        "opt": {
            "0": "Delete",
            "1": "Monitor",
            "2": "Ownership",
            "3": "Update",
            "4": "Failover",
            "5": "Apply Monitor"
        },
        "ans": [
            1,
            2
        ],
        "exp": "\n\nManaging Virtual Warehouse\n\nThis is the full list of privilege that can be granted to a role to manage a Virtual Warehouse:\n\n\n\nAPPLYBUDGET. Enables adding or removing a warehouse from a budget.\n\nMODIFY. Enables altering any properties of a warehouse, including changing its size. Required to assign a warehouse to a resource monitor. Note that only the ACCOUNTADMIN role can assign warehouses to resource monitors.\n\nMONITOR. Enables viewing current and past queries executed on a warehouse as well as usage statistics on that warehouse.\n\nOPERATE. Enables changing the state of a warehouse (stop, start, suspend, resume) and viewing current and past queries executed on a warehouse and aborting any executing queries.\n\nUSAGE. Enables using a virtual warehouse and, as a result, executing queries on the warehouse. If the warehouse is configured to auto-resume when a SQL statement (e.g. query) is submitted to it, the warehouse resumes automatically and executes the statement.\n\nOWNERSHIP. Grants full control over a warehouse. Only a single role can hold this privilege on a specific object at a time.\n\nALL [ PRIVILEGES ]. Grants all privileges, except OWNERSHIP, on the warehouse.\n\n\nThe granting of the global MANAGE WAREHOUSES privilege is equivalent to granting the MODIFY, MONITOR, and OPERATE privileges on all warehouses in an account. You can grant this privilege to a role whose purpose includes managing a warehouse\n"
    },
    "137": {
        "q": "Question 1\n\nWhat will occur if a User-Defined Function (UDF) executes a MERGE INTO command",
        "opt": {
            "0": "The command would not qualify as a scalar or table function.",
            "1": "The command would run as expected.",
            "2": "The command would fail because only updates or inserts can be performed.",
            "3": "Privileges would need to be applied to the function owner to access the tables."
        },
        "ans": [
            0
        ],
        "exp": "\n\nIn a UDF, you can use SQL to execute queries only (not DML or DDL statements).\n"
    },
    "138": {
        "q": "Question 2\n\nWhat type of NULL values are supported in semi-structured data in Snowflake",
        "opt": {
            "0": "All of the options below",
            "1": "Parquet NULLs",
            "2": "JSON NULLs",
            "3": "SQL NULLs",
            "4": "ARRAY NULLs"
        },
        "ans": [
            2,
            3
        ],
        "exp": "\n\nNULL values in semi-structured data\n\nSnowflake supports two types of NULL values in semi-structured data:\n\nSQL NULL: SQL NULL means the same thing for semi-structured data types as it means for structured data types: the value is missing or unknown.\n\nJSON null (sometimes called \u201cVARIANT NULL\u201d): In a VARIANT column, JSON null values are stored as a string containing the word \u201cnull\u201d to distinguish them from SQL NULL values.\n"
    },
    "139": {
        "q": "Question 3\n\nWhen does a scoped URL expire",
        "opt": {
            "0": "When the persisted query result period ends",
            "1": "It does not expire, the URL is permanent",
            "2": "When expiration_time period is reached"
        },
        "ans": [
            0
        ],
        "exp": "\n\nLoading and Access Unstructured data\n\nUnstructured data is information that does not fit into a predefined data model or schema. Typically text-heavy, such as form responses and social media conversations, unstructured data also encompasses images, video, and audio. Industry-specific file types such as VCF (genomics), KDF (semiconductors), or HDF5 (aeronautics) are included in this category.\n\nSnowflake supports the following actions:\n\nSecurely access data files located in cloud storage.\n\nShare file access URLs with collaborators and partners.\n\nLoad file access URLs and other file metadata into Snowflake tables.\n\nProcess unstructured data.\n\n\n\nBoth external and internal stages support unstructured data.\n\nTypes of URLs available to access files\n\nScoped URL: Encoded URL that permits temporary access to a staged file without granting privileges to the stage. The URL expires when the persisted query result period ends (i.e. the results cache expires), which is currently 24 hours.\n\nFile URL: URL that identifies the database, schema, stage, and file path to a set of files. A role that has sufficient privileges on the stage can access the files. The URL is permanent.\n\nPre-signed URL: Simple HTTPS URL used to access a file via a web browser. A file is temporarily accessible to ALL users via this URL using a pre-signed access token.\n\nThe expiration time for the access token is configurable.\n"
    },
    "140": {
        "q": "Question 4\n\nA Snowflake user is writing a User-Defined Function (UDF) with some unqualified object names. How will those object names be resolved during execution",
        "opt": {
            "0": "Snowflake first check the current schema, then the schema the previous query used",
            "1": "Snowflake will check only the schema the UDF belongs to",
            "2": "Snowflake first check the current schema, then the PUBLIC schema",
            "3": "Snowflake will resolve according the SEARCH_PATH parameter"
        },
        "ans": [
            1
        ],
        "exp": "\n\nUser Defined Functions (UDF)\n\nUnqualified Objects inside UDFs\n\nAll unqualified objects in a UDF definition will be resolved inside the UDF\u2019s schema only. The SEARCH_PATH is not used inside views or UDFs.\n\nResources"
    },
    "141": {
        "q": "Question 5\n\nYou need to load JSON data into a VARIANT column using the COPY INTO command. However, the load process sometimes fails with parsing errors, due to malformed JSON values. Your team decided to set the VARIANT column to NULL when a parsing error is encountered.\n\nWhich function should be used to meet this requirement",
        "opt": {
            "0": "PARSE_JSON",
            "1": "TRY_PARSE_JSON",
            "2": "VALIDATE"
        },
        "ans": [
            1
        ],
        "exp": "\n\nSemi-structured and Structured Data Functions\n\nThese functions are used with:\n\nsemi-structured data formats (including JSON, Avro, and XML)\n\nsemi-structured data types (including VARIANT, OBJECT, and ARRAY).\n\nstructured data types (including structured OBJECTs, structured ARRAYs, and MAPs).\n\nThe functions are grouped by type of operation performed:\n\nParsing JSON and XML data.\n\nCreating and manipulating ARRAYs and OBJECTs.\n\nExtracting values from semi-structured and structured data (e.g. from an ARRAY, OBJECT, or MAP).\n\nConverting/casting semi-structured data types and structured data types to/from other data types.\n\nDetermining the data type for values in semi-structured data (i.e. type predicates).\n\n\n\nTRY_PARSE_JSON function\n\nA special version of PARSE_JSON that returns a NULL value if an error occurs during parsing.\n\n\n\nResources"
    },
    "142": {
        "q": "Question 6\n\nWhich methods can be used to create a DataFrame object in Snowpark",
        "opt": {
            "0": "session.sql()",
            "1": "dataframe.write()",
            "2": "session.table()",
            "3": "session.builder()",
            "4": "session.create_dataframe()",
            "5": "session.df.create()"
        },
        "ans": [
            0,
            2,
            4
        ],
        "exp": "\n\nSnowpark\n\nSnowpark is essentially a wrapper that converts Spark or Pandas APIs into SQL when executing and data engineers don't need to spend excessive time to find out how it works behind scenes. The Snowpark library provides an intuitive library for querying and processing data at scale in Snowflake. Using a library for any of three languages, you can build applications that process data in Snowflake without moving data to the system where your application code runs, and process at scale as part of the elastic and serverless Snowflake engine.\n\n\n\nConstructing a DataFrame\n\nTo construct a DataFrame, you can use the methods and properties of the Session class. Each of the following methods constructs a DataFrame from a different type of data source.\n\nsession.table() method\n\nsession.create_dataframe() method\n\nsession.sql() method\n\nsession.read.json() method\n\n\n\nYou can run these examples in your local development environment or call them within the main function defined in a Python worksheet.\n\n\n\nTo create a DataFrame from data in a table, view, or stream, call the table method:\n\n# create a DataFrame from the data in the \"sample_product_data\" table.\ndf_table = session.table(\"sample_product_data\")\n \n# To print out the first 10 rows, call df_table.show()\nTo create a DataFrame from specified values, call the create_dataframe method:\n\n# create a DataFrame with one column named a from specified values.\ndf1 = session.create_dataframe([1, 2, 3, 4]).to_df(\"a\")\ndf1.show()\n# To return the DataFrame as a table in a Python worksheet use return instead of show()\n# return df1\nTo create a DataFrame to hold the results of a SQL query, call the sql method:\n\n# create a DataFrame from a SQL query\ndf_sql = session.sql(\"select name from sample_product_data\")\ndf_sql.show()\n# To return the DataFrame as a table in a Python worksheet use return instead of show()\n# return df_sql\nResources"
    },
    "143": {
        "q": "Question 7\n\nWhich Snowflake object can be accessed in the FROM clause of a query, returning a set of rows having one or more columns",
        "opt": {
            "0": "Store Procedure",
            "1": "UDF",
            "2": "UDTF"
        },
        "ans": [
            2
        ],
        "exp": "\n\nA tabular function (UDTF) returns a tabular value for each input row. In the handler for a UDTF, you write methods that conform to an interface required by Snowflake. It is accessed in the FROM clause of a query These methods will:\n\nProcess each row in a partition (required).\n\nInitialize the handler once for each partition (optional).\n\nFinalize processing for each partition (optional).\n"
    },
    "144": {
        "q": "Question 8\n\nYou are designing a data model and needs to decide how to generate and store primary keys with hashed values for a large amount of data. Which hash function in Snowflake will MINIMIZE the likelihood of collisions",
        "opt": {
            "0": "MD5_NUMBER_LOWER64",
            "1": "SHA2 with digest_size set to 512",
            "2": "SHA2",
            "3": "MD5"
        },
        "ans": [
            1
        ],
        "exp": "\n\nCryptographic Functions on Snowflake\n\n\n\nSHA2 , SHA2_HEX\n\nReturns a hex-encoded string containing the N-bit SHA-2 message digest, where N is the specified output digest size. These functions are synonymous.\n\nSHA2/SHA2_HEX ( <msg> [, <digest_size>] )\n\nArguments:\n\nRequired: msg : A string expression, the message to be hashed\n\nOptional: digest_size : Size (in bits) of the output, corresponding to the specific SHA-2 function used to encrypt the string:\n\n224 = SHA-224\n\n256 = SHA-256 (Default)\n\n384 = SHA-384\n\n512 = SHA-512\n\nDo not use this function to encrypt a message that you need to decrypt. This function has no corresponding decryption function. (The length of the output is independent of the length of the input. The output does not necessarily have enough bits to hold all of the information from the input, so it is not possible to write a function that can decrypt all possible valid inputs.). This function is intended for other purposes, such as calculating a checksum to detect data corruption.\n\n\n\nMD5 , MD5_HEX\n\nReturns a 32-character hex-encoded string containing the 128-bit MD5 message digest. These functions are synonymous.\n\nMD5/MD5_HEX (<msg>)\n\nAlthough the MD5* functions were originally developed as cryptographic functions, they are now obsolete for cryptography and should not be used for that purpose. They can be used for other purposes (for example, as \u201cchecksum\u201d functions to detect accidental data corruption).\n\n\n\nMD5_NUMBER_LOWER64\n\nCalculates the 128-bit MD5 message digest, interprets it as a signed 128-bit big endian number, and returns the lower 64 bits of the number as an unsigned integer. This representation is useful for maximally efficient storage and comparison of MD5 digests.\n\nMD5_NUMBER_LOWER64(<msg>)\n\nReturns a 64 bit unsigned integer that represents the lower 64 bits of the message digest.\n\n\n\nRisk of Collisions\n\nThe digest size of a hash function is directly related to the risk of collision. Here's how:\n\nDigest Size: This refers to the fixed length of the output (hash) produced by a hash function, typically measured in bits. Common examples include:\n\nMD5: 128 bits\n\nSHA-1: 160 bits\n\nSHA-256: 256 bits\n\nCollision: A collision occurs when two different inputs produce the same hash output. The larger the digest size, the less likely it is to encounter a collision, because there are more possible hash values.\n\nBirthday Paradox and Collisions:\n\nThe risk of a collision is not proportional to the number of possible outputs but instead follows the birthday problem.\n\nFor a hash function with a digest size of n bits, the number of possible hashes is 2n.\n\nDue to the birthday paradox, the probability of finding a collision becomes significant after approximately 2n/2 hashes. For example:\n\nA 128-bit hash can have a collision after 264 hashes.\n\nA 256-bit hash can have a collision after 2128 hashes.\n\nKey Insights:\n\nLarger digest sizes reduce the probability of a collision.\n\nFor secure applications, the digest size should be chosen to make 2n/2 computations infeasible given the current computational power.\n\n\n\nPractical Implications\n\nMD5 (128 bits) is now considered insecure due to its vulnerability to collisions.\n\nSHA-1 (160 bits) is also deprecated for security-sensitive uses.\n\nSHA-256 (256 bits) and SHA-3 are commonly used in modern cryptographic applications to ensure collision resistance.\n"
    },
    "145": {
        "q": "Question 9\n\nHow can you convert the datatype of a JSON element ",
        "opt": {
            "0": "Add a double slash (//) and a target datatype to the returned element",
            "1": "Add a double colon (::) and a target datatype to the returned element",
            "2": "Add a single colon (:) and a target datatype to the returned element",
            "3": "You cannot convert datatypes on JSON elements"
        },
        "ans": [
            1
        ],
        "exp": "\n\nSemi-structured Data\n\nData Type Conversion\n\nYou can perform Data Type Conversion on any element. By default, elements retrieved from a VARIANT column are returned as string literals.\n\nTo convert a returned element to a specific type, add the double colon (::) operator and the target data type.\n\n\n\nselect item:firstName::string FROM mytable;\n------------------------+\nitem:firstName::string  |\n------------------------+\nJohn                    |\n------------------------+"
    },
    "146": {
        "q": "Question 10\n\nA development environment is configured with the below settings:\n\n1. DATA_ENGINEER role does not have privileges to delete rows from the sys_logs table\n\n2. OPS role has privileges to delete rows from the sys_logs table\n\n3. OPS role creates an owner's rights to the procedure that deletes rows from the sys_logs table\n\n4. OPS role grants appropriate privileges on the stored procedure to the DATA_ENGINEER role\n\n\nIf a user with the role DATA_ENGINEER calls the stored procedure, what will occur",
        "opt": {
            "0": "The procedure will run with the privileges of DATA_ENGINEER and not the privileges of OPS.",
            "1": "The procedure will error when deleting rows from the sys_logs table.",
            "2": "The procedure will run with the privileges of OPS and not the privileges of DATA_ENGINEER.",
            "3": "The procedure will inherit the current virtual warehouse of the OPS role."
        },
        "ans": [
            2
        ],
        "exp": "\n\nCaller\u2019s Rights vs. Owner\u2019s Rights\n\nA stored procedure runs with either the caller\u2019s rights or the owner\u2019s rights. It cannot run with both at the same time. This topic describes the differences between a caller\u2019s rights stored procedure and an owner\u2019s rights stored procedure.\n\nA caller\u2019s rights stored procedure runs with the privileges of the caller. The primary advantage of a caller\u2019s rights stored procedure is that it can access information about that caller or about the caller\u2019s current session. For example, a caller\u2019s rights stored procedure can read the caller\u2019s session variables and use them in a query.\n\nAn owner\u2019s rights stored procedure runs mostly with the privileges of the stored procedure\u2019s owner. The primary advantage of an owner\u2019s rights stored procedure is that the owner can delegate specific administrative tasks, such as cleaning up old data, to another role without granting that role more general privileges, such as privileges to delete all data from a specific table.\n\nAt the time that the stored procedure is created, the creator specifies whether the procedure runs with owner\u2019s rights or caller\u2019s rights. The default is owner\u2019s rights.\n"
    },
    "147": {
        "q": "Question 11\n\nWhich service provides an intuitive library for querying and processing data at scale in Snowflake",
        "opt": {
            "0": "SNOWPARK",
            "1": "SNOWSQL",
            "2": "STORED PROCEDURES"
        },
        "ans": [
            0
        ],
        "exp": "\n\nSnowpark\n\nSnowpark is essentially a wrapper that converts Spark or Pandas APIs into SQL when executing and data engineers don't need to spend excessive time to find out how it works behind scenes. The Snowpark library provides an intuitive library for querying and processing data at scale in Snowflake. Using a library for any of three languages, you can build applications that process data in Snowflake without moving data to the system where your application code runs, and process at scale as part of the elastic and serverless Snowflake engine.\n"
    },
    "148": {
        "q": "Question 12\n\nWhat does the term overloading mean when referring to User-Defined Function (UDF) names in Snowflake",
        "opt": {
            "0": "There are multiple SQL UDFs with the same name and the same number of arguments types",
            "1": "There are multiple SQL UDFs with the same name but with a different number of arguments or arguments type",
            "2": "There are multiple SQL UDFs with different names but the same number of arguments or arguments type",
            "3": "There are multiple SQL UDFs with the same name and the same number of arguments"
        },
        "ans": [
            1
        ],
        "exp": "\n\nUDF Overloading Functions\n\nSnowflake supports overloading functions. In a given schema, you can define multiple procedures or functions that have the same name but different signatures.\n\nThe signatures must differ by the number of arguments, the types of the arguments, or both.\n\n--For example, for UDFs:\ncreate OR REPLACE FUNCTION myudf (number_argument NUMBER) ...\ncreate OR REPLACE FUNCTION myudf (varchar_argument VARCHAR) ...\ncreate OR REPLACE FUNCTION myudf (number_argument NUMBER, varchar_argument VARCHAR) ...\n \n--For stored procedures:\ncreate OR REPLACE PROCEDURE myproc (number_argument NUMBER) ...\ncreate OR REPLACE PROCEDURE myproc (varchar_argument VARCHAR) ...\ncreate OR REPLACE PROCEDURE myproc (number_argument NUMBER, varchar_argument VARCHAR) ...\nIf multiple signatures use the same number of arguments but have different types of arguments, you can use different names for the arguments to indicate which signature to use when you call the function or procedure.\n\n\n\nCalling overloaded procedures and functions\n\nAs is the case with calling any other procedure or function, you can specify the arguments by name or by position.\n\nselect myudf(text_input => 'hello world');\nselect myudf('hello world');\n\n\nIf you omit the argument names or if you use the same argument name for arguments of different types, Snowflake uses the number of arguments and the types of the arguments to determine the signature to use. In these cases, automatic type conversion (coercion) can affect the signature that is selected. For details, refer to Caveat about relying on the argument data type to identify the signature to call.\n"
    },
    "149": {
        "q": "Question 13\n\nWhich of the following is a characteristic of Snowflake external functions",
        "opt": {
            "0": "They incur costs through virtual warehouse usage and data transfer",
            "1": "They do not require a virtual warehouse or a schema",
            "2": "They must be written in JavaScript or SQL",
            "3": "They must be processed through a HTTP proxy service with a GET request"
        },
        "ans": [
            0,
            3
        ],
        "exp": "\n\nWhat is an External Function?\n\nAn external function calls code that is executed outside Snowflake. An external function is a type of UDF. Unlike other UDFs, an external function does not contain its own code; instead, the external function calls code that is stored and executed outside Snowflake.\n\nThe remotely executed code is known as a remote service. Information sent to a remote service is usually relayed through a proxy service. Snowflake stores security-related external function information in an API integration.\n\n\n\nAdvantages of External Functions\n\nExternal functions have the following advantages over other UDFs:\n\nThe code for the remote service can be written in languages that other UDFs cannot be written in, including Go and C#.\n\nRemote services can use functions and libraries that can\u2019t be accessed by internal UDFs. For example, remote services can interface with commercially available third-party libraries, such as machine-learning scoring libraries.\n\nDevelopers can write remote services that can be called both from Snowflake and from other software written to use the same interface.\n\n\n\nHow External Functions Work\n\nSnowflake does not call a remote service directly. Instead, Snowflake calls the remote service through a cloud provider\u2019s native HTTPS proxy service, for example API Gateway on AWS.\n\nThe main steps to call an external function are:\n\nA user\u2019s client program passes Snowflake a SQL statement that calls an external function.\n\nWhen evaluating the external function as part of the query execution, Snowflake reads the external function definition and the corresponding API integration information.\n\nThe information from the external function definition includes:\n\nThe URL of the proxy service.\n\nThe name of the corresponding API integration.\n\nThe information from the API integration includes:\n\nThe proxy service resource to use. The resource contains information about the remote service, such as the location of that service.\n\nThe authentication information for that proxy service resource.\n\nSnowflake then composes an HTTP POST command that includes:\n\nThe data to be processed. This data is in JSON format.\n\nHTTP header information.\n\nAuthentication information from the API integration.\n\nSnowflake then sends the POST request to the proxy service.\n\nThe proxy service receives the POST and then processes and forwards the request to the actual remote service. You can loosely think of the proxy service and resource as a \u201crelay function\u201d that calls the remote service.\n\nThe remote service processes the data and returns the result, which is passed back through the chain to the original SQL statement.\n\nIf the remote service responds with an HTTP code to signal asynchronous processing, then Snowflake sends one or more HTTP GET requests to retrieve the result from the remote service. Snowflake continues to send GET requests as long as it receives the response code to keep requesting, or until the external function times out or returns an error.\n\n\n\nTypically, when a query has a large number of rows to send to a remote service, the rows are split into batches. Batches typically allow more parallelism and faster queries. In some cases, batches reduce overloading of the remote service.\n\nA remote service returns 1 batch of rows for each batch received. For a scalar external function, the number of rows in the returned batch is equal to the number of rows in the received batch.\n\nEach batch has a unique batch ID, which is included in each request sent from Snowflake to the remote service. Retry operations (e.g. due to timeouts) are typically done at the batch level.\n"
    },
    "150": {
        "q": "Question 14\n\nWhich are valid ways to call a SP in Snowflake ",
        "opt": {
            "0": "call (proc1(1), proc2(2));",
            "1": "select * from (call proc1(1));",
            "2": "call proc1(1), proc2(2);",
            "3": "CALL stproc1(2 * 5.14::float);",
            "4": "CALL stproc1(select COUNT(*) FROM stproc_test_table1);",
            "5": "call proc1(1) + proc1(2);"
        },
        "ans": [
            3,
            4
        ],
        "exp": "\n\nWhat is a stored procedure (SP)?\n\nThere are times when you have complex and quite long statements that are used to perform different tasks weekly or even daily. A stored procedure in SQL is a group of SQL statements that are stored together in a database. Note that you can also write SPs in other languages, such as JavaScript or Python. Based on the statements in the procedure and the parameters you pass, it can perform one or multiple DML operations on the database. Stored procedures join SQL queries into transactions and communicate with the outside world.\n\n\n\nCalling a Stored Procedure\n\nYou can call a stored procedure using the SQL CALL command. In order for a user to call a stored procedure, the user\u2019s role must have the USAGE privilege for the stored procedure.\n\n\n\nThe following statement uses the fully-qualified name to call a stored procedure:\n\nCALL mydatabase.myschema.myprocedure();\n\nWhen called without their fully-qualified name, procedures are resolved according to the database and schema in use for the session.\n\nWhen calling the procedure, you can specify the arguments by name or position:\n\nCALL sp_concatenate_strings(\nfirst_arg => 'one',\nsecond_arg => 'two',\nthird_arg => 'three');\n\n\nExamples\n\ncall stproc1(5.14::FLOAT);\n \n--Each argument to a stored procedure can be a general expression:\nCALL stproc1(2 * 5.14::FLOAT);\n \n--An argument can be a subquery:\nCALL stproc1(select COUNT(*) FROM stproc_test_table1);\n \n--You can call only one SP per CALL statement. The following statement fails:\ncall proc1(1), proc2(2); -- Not allowed\n \n--Also, you cannot use a SP CALL as part of an expression. All this statements fail:\ncall proc1(1) + proc1(2); -- Not allowed\ncall proc1(1) + 1; -- Not allowed\ncall proc1(proc2(x)); -- Not allowed\nselect * from (call proc1(1)); -- Not allowed\n\nHowever, inside a stored procedure, the stored procedure can call another stored procedure, or call itself recursively.\n"
    },
    "151": {
        "q": "Question 15\n\nWhich Snowflake feature facilitates access to external API services such as geocoders, data transformation, machine learning models, and other custom code",
        "opt": {
            "0": "External Tables",
            "1": "Security Integration",
            "2": "External Functions",
            "3": "Java User-Defined Functions"
        },
        "ans": [
            2
        ],
        "exp": "\n\nWhat is an External Function?\n\nAn external function calls code that is executed outside Snowflake. An external function is a type of UDF. Unlike other UDFs, an external function does not contain its own code; instead, the external function calls code that is stored and executed outside Snowflake.\n\nThe remotely executed code is known as a remote service. Information sent to a remote service is usually relayed through a proxy service. Snowflake stores security-related external function information in an API integration.\n\n\n\nAdvantages of External Functions\n\nExternal functions have the following advantages over other UDFs:\n\nThe code for the remote service can be written in languages that other UDFs cannot be written in, including Go and C#.\n\nRemote services can use functions and libraries that can\u2019t be accessed by internal UDFs. For example, remote services can interface with commercially available third-party libraries, such as machine-learning scoring libraries.\n\nDevelopers can write remote services that can be called both from Snowflake and from other software written to use the same interface.\n\n\n"
    },
    "152": {
        "q": "Question 16\n\nWhich function can be used to insert JSON formatted string data into a VARIANT field",
        "opt": {
            "0": "FLATTEN",
            "1": "TO_JSON",
            "2": "PARSE_JSON"
        },
        "ans": [
            2
        ],
        "exp": "\n\nSemi-structured and Structured Data Functions\n\nThese functions are used with:\n\nsemi-structured data formats (including JSON, Avro, and XML)\n\nsemi-structured data types (including VARIANT, OBJECT, and ARRAY).\n\nstructured data types (including structured OBJECTs, structured ARRAYs, and MAPs).\n\nThe functions are grouped by type of operation performed:\n\nParsing JSON and XML data.\n\nCreating and manipulating ARRAYs and OBJECTs.\n\nExtracting values from semi-structured and structured data (e.g. from an ARRAY, OBJECT, or MAP).\n\nConverting/casting semi-structured data types and structured data types to/from other data types.\n\nDetermining the data type for values in semi-structured data (i.e. type predicates).\n\n\n\nPARSE_JSON | TO_JSON functions\n\nPARSE_JSON takes a string as input (JSON formatted) and produces a VARIANT value.\n\nIt is used to insert JSON formatted string data into a VARIANT field. Note that PARSE_JSON does not return a structured type. TO_JSON and PARSE_JSON are (almost) converse or reciprocal functions; The TO_JSON function takes a JSON-compatible VARIANT and returns a string.\n"
    },
    "153": {
        "q": "Question 17\n\nWhich Snowflake URL type allows users or applications to download or access files directly from Snowflake stage without authentication",
        "opt": {
            "0": "Directory",
            "1": "Pre-signed URL",
            "2": "File URL",
            "3": "Scoped URL"
        },
        "ans": [
            1
        ],
        "exp": "\n\nAccess Unstructured data\n\nUnstructured data is information that does not fit into a predefined data model or schema. Typically text-heavy, such as form responses and social media conversations, unstructured data also encompasses images, video, and audio. Industry-specific file types such as VCF (genomics), KDF (semiconductors), or HDF5 (aeronautics) are included in this category.\n\nSnowflake supports the following actions:\n\nSecurely access data files located in cloud storage.\n\nShare file access URLs with collaborators and partners.\n\nLoad file access URLs and other file metadata into Snowflake tables.\n\nProcess unstructured data.\n\nBoth external and internal stages support unstructured data.\n\nTypes of URLs available to access files\n\nScoped URL: Encoded URL that permits temporary access to a staged file without granting privileges to the stage. The URL expires when the persisted query result period ends (i.e. the results cache expires), which is currently 24 hours.\n\nFile URL: URL that identifies the database, schema, stage, and file path to a set of files. A role that has sufficient privileges on the stage can access the files. The URL is permanent.\n\nPre-signed URL: Simple HTTPS URL used to access a file via a web browser. A file is temporarily accessible to ALL users via this URL using a pre-signed access token.\n\nThe expiration time for the access token is configurable.\n"
    },
    "154": {
        "q": "Question 18\n\nWhen should a stored procedure be created with caller's rights",
        "opt": {
            "0": "When the caller needs to run a statement that could not execute outside the SP",
            "1": "When the SP needs to run with the privileges of the role that created the SP",
            "2": "When the SP needs to run with the privileges of the role that called the SP",
            "3": "When the caller should not view the source code of the SP"
        },
        "ans": [
            2
        ],
        "exp": "\n\nStored Procedures - Caller\u2019s Rights vs. Owner\u2019s Rights\n\nA stored procedure runs with either the caller\u2019s rights or the owner\u2019s rights. It cannot run with both at the same time. This topic describes the differences between a caller\u2019s rights stored procedure and an owner\u2019s rights stored procedure.\n\nA caller\u2019s rights stored procedure runs with the privileges of the caller. The primary advantage of a caller\u2019s rights stored procedure is that it can access information about that caller or about the caller\u2019s current session. For example, a caller\u2019s rights stored procedure can read the caller\u2019s session variables and use them in a query.\n\nAn owner\u2019s rights stored procedure runs mostly with the privileges of the stored procedure\u2019s owner. The primary advantage of an owner\u2019s rights stored procedure is that the owner can delegate specific administrative tasks, such as cleaning up old data, to another role without granting that role more general privileges, such as privileges to delete all data from a specific table.\n\nAt the time that the stored procedure is created, the creator specifies whether the procedure runs with owner\u2019s rights or caller\u2019s rights. The default is owner\u2019s rights.\n\n\n"
    },
    "155": {
        "q": "Question 19\n\nWhat is TRUE for JavaScript stored procedures and transactions",
        "opt": {
            "0": "A stored procedure can't be inside a transaction",
            "1": "Transactions can be started in one stored procedure and finished in another stored procedure",
            "2": "A stored procedure can be inside a transaction",
            "3": "A transaction can be inside a stored procedure",
            "4": "Only one transaction can be executed inside a stored procedure",
            "5": "Stored procedures do not support transactions"
        },
        "ans": [
            2,
            3
        ],
        "exp": "\n\nStored Procedures and Transactions\n\nA transaction can be inside a stored procedure, or a stored procedure can be inside a transaction; however, a transaction cannot be partly inside and partly outside a stored procedure, or started in one stored procedure and finished in a different stored procedure.\n\nFor example:\n\nYou cannot start a transaction before calling the stored procedure, then complete the transaction inside the stored procedure. If you try to do this, Snowflake reports an error similar to Modifying a transaction that has started at a different scope is not allowed.\n\nYou cannot start a transaction inside the stored procedure, then complete the transaction after returning from the procedure. If a transaction is started inside a stored procedure and is still active when the stored procedure finishes, then an error occurs and the transaction is rolled back.\n\nThese rules also apply to nested stored procedures. If procedure A calls procedure B, then B cannot complete a transaction started in A or vice versa. Each BEGIN TRANSACTION in A must have a corresponding COMMIT (or ROLLBACK) in A, and each BEGIN TRANSACTION in B must have a corresponding COMMIT (or ROLLBACK) in B.\n"
    },
    "156": {
        "q": "Question 20\n\nTaking the ITEM column as a example, how can a Snowflake user access a JSON object",
        "opt": {
            "0": "--------------------------------------------------------------------+\nITEM                                                                |\n--------------------------------------------------------------------+\n{                                                                   |\n\"firstName\": \"John\",                                                |\n\"lastName\": \"Smith\",                                                |\n\"address\": {                                                        |\n            \"streetAddress\": \"21 2nd Street\",                       | \n            \"city\": \"New York\",                                     |\n            \"state\": \"NY\",                                          |\n            \"postalCode\": \"10021-3100\"                              |\n}                                                                   |\n--------------------------------------------------------------------+\n ITEM:address.state",
            "1": "item:Address.state",
            "2": "item:address.state",
            "3": "item:address.State",
            "4": "ITEM:address.STATE"
        },
        "ans": [
            0,
            2
        ],
        "exp": "\n\nSemi-structured Data\n\nAccessing Individual Elements\n\nIndividual elements in a VARIANT column can be accessed using the \u2018.\u2019 (period) character as the path delimiter, i.e. table.column:pathelement1.pathelement2.pathelement3.\n\nNote: Column names are case-insensitive, but element names are case-sensitive.\n\n\n\nFor example, the following table that has a column of type VARIANT with the name \u201cITEM\u201d:\n\nDESC TABLE mytable;\n-----+----------+----------+-------+---------+-------------+------------+\nname | type     | kind     | null? | default | primary key | unique key | ....\n--------+---------+--------+-------+---------+-------------+------------+\nITEM | VARIANT  | COLUMN   | Y     | [NULL]  | N           | N          | ....\n--------+---------+--------+-------+---------+-------------+------------+\nWith the following JSON object stored in the first row:\n\nselect * FROM mytable LIMIT 1;\n-----------------------------------------------------+\n                    ITEM                             |\n-----------------------------------------------------+\n{                                                    |\n\"firstName\": \"John\",                                 |\n\"lastName\": \"Smith\",                                 |\n\"isAlive\": true,                                     |\n\"age\": 25,                                           |\n\"height_cm\": 167.64,                                 |\n\"address\": {                                         |\n        \"streetAddress\": \"21 2nd Street\",            |\n        \"city\": \"New York\",                          |\n        \"state\": \"NY\",                               |\n        \"postalCode\": \"10021-3100\"                   |\n},                                                   |\n\"phoneNumbers\": [                                    |\n    { \"type\": \"home\", \"number\": \"212 555-1234\" },    |\n    { \"type\": \"office\", \"number\": \"646 555-4567\" }   |\n]                                                    |\n}                                                    |\n-----------------------------------------------------+\n\n\nRetrieve firsts elements\n\nThe colon (:) is used between the column name and the first level element to retrieve it.\n\nWe can retrieve the \u201clastName\u201d element using the following query:\n\nselect item:lastName FROM mytable LIMIT 1;\n------------------------+\nmytable:lastName        |\n------------------------+\n\"Smith\"                 |\n------------------------+"
    },
    "157": {
        "q": "Question 21\n\nThe following table is created with some test data:\n\ncreate OR REPLACE TABLE stringdata (str string);\ninsert INTO stringdata VALUES (null), ('somedata');\n\n\nTo test how JavaScript handles NULL values, the following JavaScript UDF is created:\n\ncreate OR REPLACE FUNCTION nullcheck(s string)\nRETURNS string\nLANGUAGE JAVASCRIPT\nAS '\nif (S === undefined) {return \"string was undefined\";}\nelse if (S === null) {return \"string was null\";}\nelse {return \"string was not null\";}\n';\n\n\nWhat will be the output of this query statement",
        "opt": {
            "0": "select nullcheck(str) FROM stringdata ORDER BY 1;\n string was not null",
            "1": "string was undefined",
            "2": "reference error",
            "3": "string was undefined",
            "4": "string was not null",
            "5": "string was null",
            "6": "string was not null",
            "7": "string was not null"
        },
        "ans": [
            0
        ],
        "exp": "\n\nJavaScript UDFs NULL and Undefined Values\n\nWhen using JavaScript UDFs, pay close attention to rows and variables that might contain NULL values. Specifically, Snowflake contains two distinct NULL values (SQL NULL and variant\u2019s JSON null), while JavaScript contains the undefined value in addition to null.\n\n\nSQL NULL arguments to a JavaScript UDF will translate to the JavaScript undefined value. Likewise, returned JavaScript undefined values translate back to SQL NULL. This is true for all data types, including variant. For non-variant types, a returned JavaScript null will also result in a SQL NULL value.\n\n\nArguments and returned values of the variant type distinguish between JavaScript\u2019s undefined and null values.\n\nSQL NULL continues to translate to JavaScript undefined (and visceversa);\n\nVariant JSON null translates to JavaScript null (and visceversa).\n\nAn undefined value embedded in a JavaScript object (as the value) or array will cause the element to be omitted.\n\n\n\nThe following example shows how JavaScript UDFS handles NULL values:\n\n\n\n-- create a table with one string and one NULL value:\ncreate or replace table strings (s string);\ninsert into strings values (null), ('non-null string');\n \n-- create a function that converts a string to a NULL and vise-versa:\ncreate OR REPLACE FUNCTION string_reverse_nulls(s string)\nRETURNS string\nLANGUAGE JAVASCRIPT\nAS '\nif (S === undefined) {return \"string was null\";} \nelse {return undefined;}\n';\n \n-- Call the function:\nselect s, string_reverse_nulls(s)\nfrom strings order by 1;\n+-------------------------++-------------------------+\n| s                       || STRING_REVERSE_NULLS(S) |\n|-------------------------||-------------------------|\n| non-null string         || NULL                    |\n| NULL                    || string was null         |\n+-------------------------++-------------------------+"
    },
    "158": {
        "q": "Question 22\n\nHow can the OBJECT_CONSTRUCT function can be used when unloading data",
        "opt": {
            "0": "To insert values in to a table",
            "1": "To convert relational data to semi-structured data.",
            "2": "To convert semi-structured data to a relational representation"
        },
        "ans": [
            1
        ],
        "exp": "\n\nOBJECT_CONSTRUCT\n\nOBJECT_CONSTRUCT takes arguments from a table or explicitly defined as key-value pairs, and returns an OBJECT constructed from this arguments.\n\n\n\nselect OBJECT_CONSTRUCT\n( [<key1>, <value1> [, <keyN>, <valueN> ...]] | * )\n[ FROM <table> ]\nThe function accepts either:\n\nA sequence of zero or more key-value pairs.\n\n(where keys are strings, and values are of any type).\n\nAn asterisk. When invoked with an asterisk, the object is constructed using the attribute names as keys and the associated tuple values as values. See the examples below.\n\nIf the key or value is NULL (i.e. SQL NULL), the key-value pair is omitted from the resulting object. A key-value pair consisting of a not-null string as key and a JSON NULL as value (i.e. PARSE_JSON(\u2018NULL\u2019)) is not omitted.\n"
    },
    "159": {
        "q": "Question 23\n\nAssume the below table exists:\n\ncreate table foo (name STRING, entered_at TIMESTAMP);\n\n\n\nConsider the following stored procedures:\n\ncreate or replace procedure sp1()\nreturns varchar\nlanguage javascript\nAS\n$$\nsnowflake.execute ({sqlText: \"insert into foo values ('Bob', CURRENT_TIMESTAMP)\"} );\nsnowflake.execute ({sqlText: \"begin transaction\"} );\nsnowflake.execute ({sqlText: \"insert into foo values ('Jane', CURRENT_TIMESTAMP)\"} );\nsnowflake.execute ({sqlText: \"call sp2()\"} );\nsnowflake.execute ({sqlText: \"rollback\"} );\nsnowflake.execute ({sqlText: \"insert into foo values ('Frank', CURRENT_TIMESTAMP)\"} );\nreturn \"\";\n$$;\n\n\ncreate or replace procedure sp2()\nreturns varchar\nlanguage javascript\nAS\n$$\nsnowflake.execute ({sqlText: \"begin transaction\"} );\nsnowflake.execute ({sqlText: \"insert into foo values ('Zach', CURRENT_TIMESTAMP)\"} );\nsnowflake.execute ({sqlText: \"commit\"} );\nreturn \"\";\n$$;\n\n\nAssuming auto commit is set to TRUE, the below commands are issued, in order:\n\nTRUNCATE FOO;\ninsert INTO FOO VALUES ('Mary', CURRENT_TIMESTAMP);\nCALL SP1();\nselect name FROM FOO ORDER BY entered_at;\n\n\nWhat names will be returned by the final select statement",
        "opt": {
            "0": "Mary, Bob, Jane, Frank",
            "1": "Mary, Bob, Frank",
            "2": "Mary, Bob, Zach, Frank",
            "3": "Mary, Bob, Jane, Zach, Frank"
        },
        "ans": [
            2
        ],
        "exp": "\n\nEven if store procedure 1 has a ROLLBACK statement at the end, the transaction of the store procedure 2 (BEGIN WORK \u2026 COMMIT WORK) is a separate transaction. Snowflake does not treat the inner transaction as nested; instead, the inner transaction is a separate transaction. Snowflake calls these \u201cautonomous scoped transactions\u201d (or simply \u201cscoped transactions\u201d)\n\n\n\nIn this ## question, as the inner transaction in sp2() is a separate transaction (Snowflake does not treat the inner transaction as nested), the only insert statement that is rolled back is \u2018Jane\u2019.\n\n\n\n"
    },
    "160": {
        "q": "Question 24\n\nWhich of the following is NOT a data type supported in Snowflake for semi-structured data",
        "opt": {
            "0": "BLOB",
            "1": "VARIANT",
            "2": "ARRAY",
            "3": "OBJECT"
        },
        "ans": [
            0
        ],
        "exp": "\n\nSemi-structured Data Types\n\nThe following Snowflake data types can contain other data types:\n\nVARIANT (can contain any other data type).\n\nARRAY (can directly contain VARIANT, and thus indirectly contain any other data type, including itself).\n\nOBJECT (can directly contain VARIANT, and thus indirectly contain any other data type, including itself).\n\nWe often refer to these data types as semi-structured data types. Strictly speaking, OBJECT is the only one of these data types that, by itself, has all of the characteristics of a true semi-structured data type. However, combining these data types allows you to explicitly represent arbitrary hierarchical data structures, which can be used to load and operate on data in semi-structured formats (e.g. JSON, Avro, ORC, Parquet, or XML).\n"
    },
    "161": {
        "q": "Question 25\n\nWhat is the recommended Snowflake data type to store semi-structured data like JSON",
        "opt": {
            "0": "VARIANT data type",
            "1": "JSON data type",
            "2": "RAW data type",
            "3": "VARCHAR data type"
        },
        "ans": [
            0
        ],
        "exp": "\n\nVARIANT datatype\n\nSnowflake use a data type called VARIANT that allows semi-structured data to be loaded, as is, into a column in a relational table.\n\nA VARIANT can have a maximum size of up to 16 MB of uncompressed data. However, in practice, the maximum size is usually smaller due to internal overhead. The maximum size is also dependent on the object being stored.\n\nVARIANT is the recommended Snowflake data type to store semi-structured data like JSON.\n"
    },
    "162": {
        "q": "Question 26\n\nWhat columns are returned when performing a FLATTEN command on semi-structured data",
        "opt": {
            "0": "NODE",
            "1": "SEQ",
            "2": "VALUE",
            "3": "INDEX"
        },
        "ans": [
            1,
            2,
            3
        ],
        "exp": "\n\nSemi-structured and Structured Data Functions\n\nThese functions are used with:\n\nsemi-structured data formats (including JSON, Avro, and XML)\n\nsemi-structured data types (including VARIANT, OBJECT, and ARRAY).\n\nstructured data types (including structured OBJECTs, structured ARRAYs, and MAPs).\n\nThe functions are grouped by type of operation performed:\n\nParsing JSON and XML data.\n\nCreating and manipulating ARRAYs and OBJECTs.\n\nExtracting values from semi-structured and structured data (e.g. from an ARRAY, OBJECT, or MAP).\n\nConverting/casting semi-structured data types and structured data types to/from other data types.\n\nDetermining the data type for values in semi-structured data (i.e. type predicates).\n\n\n\nFLATTEN function\n\nFLATTEN is used to convert semi-structured data to a relational representation, returning a set of rows as output. It takes a VARIANT, OBJECT, or ARRAY column and produces a lateral view (i.e. an inline view that contains correlation referring to other tables that precede it in the FROM clause).\n\nThe returned rows consist of a fixed set of columns:\n\nSEQ. A unique sequence number associated with the input record; the sequence is not guaranteed to be gap-free or ordered in any particular way.\n\nKEY. For maps or objects, this column contains the key to the exploded value.\n\nVALUE. The value of the element of the flattened array/object.\n\nPATH. The path to the element within a data structure which needs to be flattened.\n\nINDEX. The index of the element, if it is an array; otherwise NULL.\n\nTHIS. The element being flattened (useful in recursive flattening).\n"
    },
    "163": {
        "q": "Question 27\n\nWhich programming languages are supported for Snowflake User-Defined Functions (UDFs)",
        "opt": {
            "0": "PHP",
            "1": "SQL",
            "2": "C#",
            "3": "Python",
            "4": "JavaScript"
        },
        "ans": [
            1,
            3,
            4
        ],
        "exp": "\n\nUser Defined Functions (UDF)\n\nA user-defined function (UDF) is a function you define so you can call it from SQL. A UDF\u2019s logic typically extends or enhances SQL to perform transformations or calculations with functionalities that SQL doesn\u2019t have or doesn\u2019t do well. In a UDF, you can use SQL to execute queries only (not DML or DDL statements).\n\n\n\nSupported Languages\n\nYou write a function\u2019s handler \u2013 its logic \u2013 in any of several programming languages. Each language allows you to manipulate data within the constraints of the language and its runtime environment. Regardless of the handler language, you create the procedure itself in the same way using SQL, specifying your handler and handler language.\n\nYou can write a handler in any of the following languages:\n\nJava\n\nJavaScript\n\nPython\n\nScala\n\nSQL\n\n\n"
    },
    "164": {
        "q": "Question 28\n\nWhat does the SECURE parameter indicate when creating an external function",
        "opt": {
            "0": "The context headers are hidden from all users who are not owners of the function",
            "1": "All options are correct",
            "2": "The HTTP headers are hidden from all users who are not owners of the function",
            "3": "The URL is hidden from all users who are not owners of the function"
        },
        "ans": [
            1
        ],
        "exp": "\n\ncreate External Function Syntax\n\ncreate [ OR REPLACE ] [ SECURE ] EXTERNAL FUNCTION <name> \n( [ <arg_name> <arg_data_type> ] [ , ... ] )\n  RETURNS <result_data_type>\n  [ [ NOT ] NULL ]\n  [ { CALLED ON NULL INPUT | { RETURNS NULL ON NULL INPUT | STRICT } } ]\n  API_INTEGRATION = <api_integration_name>\n  [ HEADERS = ( '<header_1>' = '<val_1>' [, '<header_2>' = '<val_2>' ... ])]\n  [ CONTEXT_HEADERS = ( <context_function_1> [, <context_function_2> ...])]\n  [ MAX_BATCH_ROWS = <integer> ]\n  [ COMPRESSION = <compression_type> ]\n  [ REQUEST_TRANSLATOR = <request_translator_udf_name> ]\n  [ RESPONSE_TRANSLATOR = <response_translator_udf_name> ]\n  AS '<url_of_proxy_and_resource>';\n\n\nWhere:\n\nSECURE. Specifies that the function is secure. If a function is secure, the URL, the HTTP headers, and the context headers are hidden from all users who are not owners of the function.\n\n[ [ NOT ] NULL ] Indicates whether the function can return NULL values or must return only NON-NULL values. If NOT NULL is specified, the function must return only non-NULL values. If NULL is specified, the function can return NULL values.\n\nDefault: The default is NULL (i.e. the function can return NULL values).\n"
    },
    "165": {
        "q": "Question 29\n\nWhat can you do as a Data Engineer to avoid exposing sequence-generated column values in UDFs",
        "opt": {
            "0": "Do not expose the sequence-generated column as part of the function",
            "1": "Use randomized identifiers (such as those generated by UUID_STRING) instead of sequence-generated values",
            "2": "Programmatically obfuscate the identifiers",
            "3": "All of the above"
        },
        "ans": [
            3
        ],
        "exp": "\n\nBest Practices for Protecting Access to Sensitive Data\n\nSecure UDFs prevent users from possibly being exposed to data from rows of tables that are filtered by the function. However, there are still ways that a data owner might inadvertently expose information about the underlying data if UDFs are not constructed carefully. This section describes some potential pitfalls to avoid.\n\nAvoid Exposing Sequence-Generated Column Values\n\nA common practice for generating surrogate keys is to use a sequence or auto-increment column. If these keys are exposed to users who do not have access to all of the underlying data, then a user might be able to guess details of the underlying data distribution.\n\nFor example, suppose that we have a function get_widgets_function() that exposes the ID column. If ID is generated from a sequence, then a user of get_widgets_function() could deduce the total number of widgets created between the creation timestamps of two widgets that the user has access to.\n\n\n\nIf information is too sensitive to expose to users of a function, you can use any of the following alternatives:\n\nDo not expose the sequence-generated column as part of the function.\n\nUse randomized identifiers (such as those generated by UUID_STRING) instead of sequence-generated values.\n\nProgrammatically obfuscate the identifiers.\n\nResources"
    },
    "166": {
        "q": "Question 30\n\nA JSON document is stored in the source_column of type VARIANT. The document has an array called elements. The array contains the name key that has a string value.\n\nHow can a Snowflake user extract the name from the first element",
        "opt": {
            "0": "source_column:element[1].name",
            "1": "source_column::element[0].name",
            "2": "source_column.element[0].name",
            "3": "source_column:element[0].name"
        },
        "ans": [
            3
        ],
        "exp": "\n\nSemi-structured Data\n\nArrays and Indexes\n\nElements of an array can be accessed using bracket notation. Array indexes are 0-based; the first element in an array is element 0.\n\nTo select the first element of the phoneNumbers array in the above example:\n\nselect item:phoneNumbers[0] FROM mytable;\n-----------------------------+\nitem:phoneNumbers[0]         |\n-----------------------------+\n{                            |\n\"number\": \"212 555-1234\",    |\n\"type\": \"home\"               |\n}                            |\n-----------------------------+\n\n\nTo query a specific field inside the element, you can select the field with a dot (.) operator after the element and index number.\n\nselect item:phoneNumbers[0].number FROM mytable;\n-----------------------------+\nitem:phoneNumbers[0]         |\n-----------------------------+\n{                            |\n\"number\": \"212 555-1234\"     |\n}                            |\n-----------------------------+"
    },
    "167": {
        "q": "Question 31\n\nWhile running an external function, the following error message is received:\n\nError: Function received the wrong number of rows\n\nWhat is causing this to occur",
        "opt": {
            "0": "Nested arrays are not supported in the JSON response",
            "1": "The return message did not produce the same number of rows that it received",
            "2": "The JSON returned by the remote service is not constructed correctly",
            "3": "External functions do not support multiple rows"
        },
        "ans": [
            1
        ],
        "exp": "\n\nTroubleshooting External Functions for AWS\n\n\n\nError: Function received the wrong number of rows\n\nPossible Causes: The remote service tried to return more or fewer rows than it received. Even though the function is nominally scalar, it might receive multiple rows in the body field of the event parameter, and should return exactly as many rows as it received.\n\nPossible Solution(s): Ensure that the remote service returns one row for each row that it receives.\n\nError: Row numbers out of order\n\nPossible Causes: The row numbers you return within each batch should be monotonically ascending integers starting at 0. The input row numbers must also follow this rule, and each output row should match the corresponding input row. For example, the output in output row 0 should correspond to the input in input row 0.\n\nPossible Solutions: Ensure that the row numbers you return are the same as the row numbers you received, and that each output value uses the row number of the corresponding input. If this doesn\u2019t work, then the input row numbers may not be correct or you did not return the rows in the correct order.\n\nNext, ensure that the output row numbers start from 0, increase by 1, and are in order.\n\n\n\nError: Format of the returned value is not JSON\n\nPossible Causes: Your return value includes double quotes inside the value.\n\nPossible Solutions: Although JSON strings are delimited by double quotes, the string itself should not start and end with a quotation mark in most cases. If the embedded double quotes are incorrect, remove them.\n\nResources"
    },
    "168": {
        "q": "Question 32\n\nWhich command will generate a JSON object using the data of table \"countries\" ",
        "opt": {
            "0": "select TO_JSON(*) AS obj FROM countries ;",
            "1": "select OBJECT_CONSTRUCT(*) AS obj FROM countries ;",
            "2": "select TO_OBJECT(*) AS obj FROM countries ;"
        },
        "ans": [
            1
        ],
        "exp": "\n\nOBJECT_CONSTRUCT\n\nOBJECT_CONSTRUCT takes arguments from a table or explicitly defined as key-value pairs, and returns an OBJECT constructed from this arguments.\n\n\n\nselect OBJECT_CONSTRUCT\n( [<key1>, <value1> [, <keyN>, <valueN> ...]] | * )\n[ FROM <table> ]\nThe function accepts either:\n\nA sequence of zero or more key-value pairs.\n\n(where keys are strings, and values are of any type).\n\nAn asterisk. When invoked with an asterisk, the object is constructed using the attribute names as keys and the associated tuple values as values. See the examples below.\n\nIf the key or value is NULL (i.e. SQL NULL), the key-value pair is omitted from the resulting object. A key-value pair consisting of a not-null string as key and a JSON NULL as value (i.e. PARSE_JSON(\u2018NULL\u2019)) is not omitted.\n"
    },
    "169": {
        "q": "Question 33\n\nHow can a Snowflake user access individual elements on semi-structured data",
        "opt": {
            "0": "insert a double colon (::) between the column name and any second level element",
            "1": "insert a double colon (::) between the column name and any first level element",
            "2": "insert a colon (:) between the column name and any first level element",
            "3": "insert a colon (:) between the column name and any second level element"
        },
        "ans": [
            2
        ],
        "exp": "\n\nSemi-structured Data\n\nAccessing Individual Elements\n\nIndividual elements in a VARIANT column can be accessed using the \u2018.\u2019 (period) character as the path delimiter, i.e. table.column:pathelement1.pathelement2.pathelement3.\n\nNote: Column names are case-insensitive, but element names are case-sensitive.\n\n\n\nFor example, the following table that has a column of type VARIANT with the name \u201cITEM\u201d:\n\nDESC TABLE mytable;\n-----+----------+----------+-------+---------+-------------+------------+\nname | type     | kind     | null? | default | primary key | unique key | ....\n--------+---------+--------+-------+---------+-------------+------------+\nITEM | VARIANT  | COLUMN   | Y     | [NULL]  | N           | N          | ....\n--------+---------+--------+-------+---------+-------------+------------+\nWith the following JSON object stored in the first row:\n\nselect * FROM mytable LIMIT 1;\n-----------------------------------------------------+\n                    ITEM                             |\n-----------------------------------------------------+\n{                                                    |\n\"firstName\": \"John\",                                 |\n\"lastName\": \"Smith\",                                 |\n\"isAlive\": true,                                     |\n\"age\": 25,                                           |\n\"height_cm\": 167.64,                                 |\n\"address\": {                                         |\n        \"streetAddress\": \"21 2nd Street\",            |\n        \"city\": \"New York\",                          |\n        \"state\": \"NY\",                               |\n        \"postalCode\": \"10021-3100\"                   |\n},                                                   |\n\"phoneNumbers\": [                                    |\n    { \"type\": \"home\", \"number\": \"212 555-1234\" },    |\n    { \"type\": \"office\", \"number\": \"646 555-4567\" }   |\n]                                                    |\n}                                                    |\n-----------------------------------------------------+\n\n\nRetrieve firsts elements\n\nThe colon (:) is used between the column name and the first level element to retrieve it.\n\nWe can retrieve the \u201clastName\u201d element using the following query:\n\nselect item:lastName FROM mytable LIMIT 1;\n------------------------+\nmytable:lastName        |\n------------------------+\n\"Smith\"                 |\n------------------------+"
    },
    "170": {
        "q": "Question 34\n\nWhich of the following data types are supported for semi-structured data",
        "opt": {
            "0": "ARRAY",
            "1": "BLOB",
            "2": "OBJECT",
            "3": "VARIANT",
            "4": "CSV"
        },
        "ans": [
            0,
            2,
            3
        ],
        "exp": "\n\nSemi-structured Data Types\n\nThe following Snowflake data types can contain other data types:\n\nVARIANT (can contain any other data type).\n\nARRAY (can directly contain VARIANT, and thus indirectly contain any other data type, including itself).\n\nOBJECT (can directly contain VARIANT, and thus indirectly contain any other data type, including itself).\n\nWe often refer to these data types as semi-structured data types. Strictly speaking, OBJECT is the only one of these data types that, by itself, has all of the characteristics of a true semi-structured data type. However, combining these data types allows you to explicitly represent arbitrary hierarchical data structures, which can be used to load and operate on data in semi-structured formats (e.g. JSON, Avro, ORC, Parquet, or XML).\n"
    },
    "171": {
        "q": "Question 35\n\nWhich function helps to convert semi-structured data to a relational representation",
        "opt": {
            "0": "TO_JSON",
            "1": "PARSE_JSON",
            "2": "FLATTEN"
        },
        "ans": [
            2
        ],
        "exp": "\n\nSemi-structured and Structured Data Functions\n\nThese functions are used with:\n\nsemi-structured data formats (including JSON, Avro, and XML)\n\nsemi-structured data types (including VARIANT, OBJECT, and ARRAY).\n\nstructured data types (including structured OBJECTs, structured ARRAYs, and MAPs).\n\nThe functions are grouped by type of operation performed:\n\nParsing JSON and XML data.\n\nCreating and manipulating ARRAYs and OBJECTs.\n\nExtracting values from semi-structured and structured data (e.g. from an ARRAY, OBJECT, or MAP).\n\nConverting/casting semi-structured data types and structured data types to/from other data types.\n\nDetermining the data type for values in semi-structured data (i.e. type predicates).\n\n\n\nFLATTEN function\n\nFLATTEN is used to convert semi-structured data to a relational representation, returning a set of rows as output. It takes a VARIANT, OBJECT, or ARRAY column and produces a lateral view (i.e. an inline view that contains correlation referring to other tables that precede it in the FROM clause).\n\nThe returned rows consist of a fixed set of columns:\n\nSEQ. A unique sequence number associated with the input record; the sequence is not guaranteed to be gap-free or ordered in any particular way.\n\nKEY. For maps or objects, this column contains the key to the exploded value.\n\nVALUE. The value of the element of the flattened array/object.\n\nPATH. The path to the element within a data structure which needs to be flattened.\n\nINDEX. The index of the element, if it is an array; otherwise NULL.\n\nTHIS. The element being flattened (useful in recursive flattening)."
    }
}